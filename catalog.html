<html>
<head>
    <title>Generative AI Design Patterns</title>
    <style>
        table {
            border-collapse: collapse; /* Make table borders touch */
            width: 80%;                /* Adjust table width as needed */
            margin: 20px auto;         /* Center the table */
        }

        th, td {
            border: 1px solid black;   /* Give cells a border */
            padding: 10px;             /* Add spacing within cells */
            text-align: left;        /* Center the text in cells */
        }

        th {
            background-color: #f0f0f0; /* Light gray background for headers */
        }

        li {
            margin-bottom: 10px;    /* Add space between list items */
        }
    </style>
</head>
<h2>Generative AI Design Patterns.</h2>

This is a catalog that is being updated routinely.
Please see: https://github.com/lakshmanok/generative-ai-design-patterns/
for the updated list.

<table>
    <tr>
        <th>Category</th>
        <th>Pattern Name</th>
        <th>Problem</th>
        <th>Design Pattern Solution</th>
        <th>Reference</th>
        <th>Tradeoffs and Alternatives</th>
        <th>Example demonstrating usage</th>
    </tr>
    <!-- MANIPULATING OUTPUT STYLE -->
    <tr>
        <th>Controlling Style</th>
        <td>Grammar</td>
        <td>LLM needs to generate text that follows a strict grammar. 
            This could be as simple as JSON, XML, etc. or as complex as a chess game notation.</td>
        <td>
            Express constraints in Backus-Naur Form (BNF) using GGML -- this is called GNBF.
            The model will use these constraints to change the token probabilities
            to ensure that only legal next-tokens receive non-zero probabilities.
        <td>
            <ul>
                <li><a href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">GNBF Guide in llama.cpp</a></li>
            </ul>
        </td>
        <td>
            Because this is just a constraint on how the next token is chosen,
            there is no guarantee that the generated JSON will be complete/closed.
            <ul>
                <li>Providing few-shot examples is simpler, but is more inconsistent.</li>
                <li>For simple tasks, it's possible to get guaranteed conformance and completeness with a template approach.</li>
                <li>Approaches like <a href="https://arxiv.org/abs/2305.19234">Grammar Prompting for Domain-Specific Language Generation with LLMs</a>
                    that incorporate the grammar into the prompt are error-prone.
                </li>
                <li>This is what the structured output generation in GPT-4, Gemini, etc. do for
                    supported formats. Using the GNBF pattern allows you to add much more specific constraints.
                </li>
            </ul>
        <td>
            <ul>
            <li><a href="https://til.simonwillison.net/llms/llama-cpp-python-grammars">Llama CPP Python Grammars</a></li>
            <li><a href="https://github.com/ejones/llama-journey">A game with Llama CPP Grammar</a></li>
            </ul>
        </td>
    </tr>
    <tr>
        <th></th>
        <td>Self Check</td>
        <td>Get an LLM to say &quot;I don&#39;t know&quot; rather than hallucinating facts or details.</td>
        <td>Get an LLM to generate multiple answers in parallel. 
            Do a self-consistency check, and if they diverge, it&#39;s probably hallucinating.</td>
        <td><a href="https://arxiv.org/abs/2303.08896">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection</a>
        </td>
        <td>Fine-tuning the LLM or using examples for which &quot;I don&#39;t know&quot; 
            is the right answer is less reliable.</td>
        <td>Use output rails in <a href="https://github.com/NVIDIA/NeMo-Guardrails">NeMo-Guardrails</a>
        </td>
    </tr>
    <tr>
        <th></th>
        <td>Reverse Neutralization</td>
        <td>LLM needs to match a specific tone and voice, such as your company style or personal style.</td>
        <td>Take content created in desired style, ask LLM to rephrase in neutral language at 10th grade level. 
            Reverse the inputs and outputs and use it to Adapter Tune a model that will take LLM outputs
            and rephrase them in your style.</td>
        <td>
            <ul>
            <li><a href="https://community.openai.com/t/train-a-gpt-model-in-my-tone/119175/31"> Train a GPT Model in My Tone </a></li>
            <li><a href="https://arxiv.org/abs/2308.07968">Teach LLMs to Personalize</a></li>
            <li><a href="https://technicalwriting.tools/posts/style-guide-fine-tuning/">Fine-tuning an LLM into a style guide editor</a></li>
            </ul>

        </td>
        <td>
            <ul>
                <li>It&#39;s hard to create a dataset that covers all possible topics, so using only a fine-tuned LLM tends to lose of creativity.</li>
                <li>If all you need is a style checker, you can use an off-the-shelf LLM to enforce a style guide. </li>
                <li>If your content does cover all possible topics, you can create a RAG and fine-tune every stage of it (see: https://arxiv.org/abs/2308.07968) </li>
                <li>If the matching is in terms of instructions, use the Style Transfer pattern.</li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Style Transfer</td>
        <td>Get an LLM to follow instructions such as &quot;more positive&quot; but get the right level of positivity.</td>
        <td>Use augmented zero-shot prompting, which augments what&#39;s desired with variations that are non-exemplars.</td>
        <td><a href="https://aclanthology.org/2022.acl-short.94.pdf">A Recipe For Arbitrary Text Style Transfer with Large Language Models</a></td>
        <td>See Reverse Neutralization</td>
        <td></td>
    </tr>

    <!-- ADDRESSING CAPABILITY LIMITATIONS of LLMs -->
    <tr>
        <th>Adding Capability</th>
        <td>Adapter Tuning</td>
        <td>LLM has to do a task that the foundational model was not trained to do.</td>
        <td>Create instruction dataset of input-output pairs that demonstrate task; use LoRA</td>
        <td><a href="https://arxiv.org/abs/2106.09685">Low Rank adapation of LLMs</a></td>
        <td>
            <ul>
                <li>For tasks that are similar to the foundational model, consider Few Shot.</li>
                <li>More efficient than fine-tuning of all the weights (instruction tuning).</li>
            </ul></u>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Few Shot</td>
        <td>LLM has to do a task that the foundational model was not trained to do, but lies within its frontier capability.</td>
        <td>Add a few examples of task input and output to the prompt.</td>
        <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></td>
        <td>If beyond frontier capability, may need to do Adapter Tuning.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Chain of Thought</td>
        <td>LLMs are poor at reasoning.</td>
        <td>Teach the LLM the steps it has to follow to get to an answer.</td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li>
            </ul>
        </td>
        <td>
            <ul>
                <li>Limited to linear flows only. For complex flows, consider Tree of Thought.</li>
                <li>In some cases, the LLM can self-reflect and get the list of steps to follow.</li>
                <li>Can also generate steps using LLMs and self-consistency checks.</li>
                <li>If unable to express logic, see Iterative Self-Refine</li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Tree of Thought</td>
        <td>LLMs are poor at planning, and reasoning.</td>
        <td>Teach the LLM the steps it has to follow to get to an answer.</td>
        <td><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with LLMs</a></td>
        <td>
            <ul>
                <li>For simple linear flows, can use Chain of Thought. Trees provide the ability to look ahead strategically.</li>
                <li>See also Agent frameworks for orchestrating a flow with multiple LLM prompts.</li>
            </ul>

        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Evol Instruct</td>
        <td>Teach the LLM new capabilities efficiently</td>
        <td>Organize the training data starting from tutorials and then evolve the examples to increase complexity.</td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2306.11644">Textbooks are all you need</a></li>
                <li><a href="https://arxiv.org/abs/2304.12244">WizardLM: Empowering LLMs to follow complex instructions</a></li>
            </ul>
        </td>
        <td>
            <ul>
                <li>Applies to all the other patterns in this category.</li>
                <li>Could also increase model complexity along with instruction complexity, but more common to use Adapter Tuning where complexity is fixed. </li>
            </ul>
        </td>
        <td></td>
    </tr>

    <!-- ADDRESSING UNRELIABILITY -->
    <tr>
        <th>Unreliable Primitives</th>
        <td>Dependency Injection</td>
        <td>LLMs return different answers to same input</td>
        <td>During development and testing, inject proxies for all internal and external dependencies.</td>
        <td>
            <ul>
                <li><a href="https://medium.com/towards-data-science/evaluation-driven-development-for-agentic-applications-using-pydanticai-d9293ac81d91">Evaluation-driven development</a></li>
            </ul>
        </td>
        <td>
            <ul>
                <li>Should proxies be stochastic?</li>
            </ul>
        </td>
        <td></td>
    </tr>
     <tr>
        <th></th>
        <td>Multiple Responses</td>
        <td>LLMs responses are highly variable</td>
        <td>Generate multiple responses and choose the best</td>
         <td></td>
        <td>How to do this efficiently instead of invoking LLM multiple times.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Automatic Prompt Tuning</td>
        <td>Ensure that as model versions change, your prompts continue to perform well. </td>
        <td>Evaluate multiple candidate prompts against desired outputs as part of both development and deployment. Incorporate into CI/CD ML pipelines.</td>
        <td></td>
        <td>Can also be used to counteract drift.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Iterative Self-Refine (or Reflection)</td>
        <td>Get an LLM to improve its own performance and meet some evaluation criterion.</td>
        <td>Use the LLM to critique generated text and invoke again.</td>
        <td><a href="https://arxiv.org/abs/2303.17651">Self-Refine: Iterative Refinement with Self-Feedback</a></td>
        <td>
            Use Chain of Thought for a less costly alternative if generation can be expressed in terms of logical steps.
        </td>
        <td></td>
    </tr>

    <!-- AGENTIC ARCHITECTURES -->
    <tr>
        <th>Extending LLMs</th>
        <td>Retrieval Augmented Generation (RAG)</td>
        <td>Use knowledge the LLM was not trained on, such as confidential information or new information in generation.</td>
        <td>Identify document chunks relevant to a question, add into the context and ask LLM to generate answer.</td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
                <li><a href="https://github.com/jxzhangjhu/Awesome-LLM-RAG">GitHub Repo with links to RAG papers</a></li>
                <li><a href="https://docs.llamaindex.ai/en/stable/">LlamaIndex: abstraction framework for RAG</a></li>
            </ul>
        </td>
        <td>
            <ul>
                <li>Considerations include appropriate chunking, embedding, matching, query rewriting, and reranking strategies.</li>
                <li>Recent research <a href="https://arxiv.org/html/2402.09906v1">Generative Representational Instruction Tuning (GRIT)</a> combining the search and generation into a single model.</li>
                <li>Fully managed services like Glean, Vertex AI Search exist.</li>
                <li>Parts of the workflow can be outsourced, e.g. embedding to Unstructured.io and search to pinecone </li>
            </ul>
        </td>
        <td>
            RAG in:
            <ul>
                <li><a href="https://python.langchain.com/docs/use_cases/question_answering/">langchain</a></li>
                <li><a href="https://docs.llamaindex.ai/en/stable/">LlamaIndex</a></li>
            </ul>
        </td>
    </tr>
    <tr>
        <th></th>
        <td>Tool Functions</td>
        <td>Incorporate non-language tasks whose results are needed for generation</td>
        <td>Teach LLM to emit text of function name + parameters to call backend functions </td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
            </ul>

        </td>
        <td>See Grammar for how to ensure compliance to API specs.  See also Domain Languages. See also Agents for the orchestration.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Domain Languages</td>
        <td>Incorporate non-language tasks whose results are needed for generation</td>
        <td>Teach LLM to emit a domain specific language (SQL, Matplotlib, etc.) that will be processed by a sandbox or backend service.</td>
        <td></td>
        <td>See Grammar for how to ensure compliance to API specs.  See also Tool Functions.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Agentic workflows</td>
        <td>Orchestrate a complex workflow consisting of information retrieval, function calling, and conversation maintenance.</td>
        <td></td>
        <td></td>
        <td>
            <ul>
                <li>Whenever a single LLM call can not do the job.</li>
                <li>Degree of autonomy to give the LLM vs. prewiring the workplan</li>
            </ul>
        </td>
        <td>Multiagent, ReAct: different philosophies/ways of doing the orchestration</td>
    </tr>

    <!-- CONTROLLING CONTEXT -->
    <tr>
        <th>Controlling Context</th>
        <td>Example Selection</td>
        <td>Guard against information leakage.</td>
        <td>Restrict world knowledge.</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Conversation State</td>
        <td>How can the LLM remember previous turns of the conversation?</td>
        <td>Summarize the previous conversation and add it to context.</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Guardrails</td>
        <td>Guard against toxicity, bias, information leakage, etc.</td>
        <td>Pre- and post- processing of LLM inputs and outputs to ensure compliance to goals</td>
        <td></td>
        <td>See also Iterative Self-Refine to decrease number of refusals.</td>
        <td></td>
    </tr>
    <tr>
        <th>Meeting Constraints</th>
        <td>Distillation</td>
        <td>Reduce cost of inference within acceptable quality bounds.</td>
        <td>Teach student model on output of larger model.</td>
        <td></td>
        <td>See also Quantization</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Templates</td>
        <td>Reduce need for human oversight.</td>
        <td>Have the LLM generate a limited number of templated responses. The template will be replaced at runtime.</td>
        <td><a href="https://medium.com/towards-data-science/how-to-choose-the-architecture-for-your-genai-application-6053e862c457">Balancing creativity and risk</a></td>
        <td>
            <ul>
                <li>Assembled reformat: use LLMs to extract the data that goes into the template.</li>
                <li>ML selection of template: for personalization</li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Prompt Caching</td>
        <td>Ensure same answer to repeat query and/or reduce costs.</td>
        <td>Cache previous queries and responses to provide deterministic behavior.
            You can cache parts of a prompt (system instructions, context) and not just whole prompts.
            If the context includes a large PDF, for example, context caching can improve speed.
        </td>
        <td>
            <ul>
                <li> <a href="https://www.anthropic.com/news/prompt-caching">Anthropic prompt-caching</a></li>
                <li> <a href="https://ai.google.dev/gemini-api/docs/caching?lang=python">Gemini context caching</a></li>
            </ul>
        </td>
        <td>
            Should you use semantic search or exact match to see if this is a repeat query?
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Context Window</td>
        <td>How to fit examples, conversation state, etc. into limited context window</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Quantization</td>
        <td>Reduce cost of inference within acceptable quality bounds.</td>
        <td></td>
        <td></td>
        <td>Would use collection of techniques (quantization, distillation, caching, etc.)</td>
        <td></td>
    </tr>
</table>
</html>
