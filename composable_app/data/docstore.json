{"docstore/data": {"51134260-f449-4ed2-97de-69e94473ecd8": {"__data__": {"id_": "51134260-f449-4ed2-97de-69e94473ecd8", "embedding": null, "metadata": {"bbox": [{"page": 3, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 230.76, "x1": 432.0, "y1": 360.26}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6d485752-8035-43b4-a657-b64648debe54", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 661.5, "page_width": 504.0, "x0": 0.0724, "y0": 0.25, "x1": 503.92762500000003, "y1": 661.5}]}, "hash": "c0e595d73fbccdb5b1f7df2b6aeb1d2f6dc2dc7ae6fe8cf6b0c70a359e073cf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4684e8ad-e933-467c-a7c4-241d7aeb6107", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.23, "x1": 432.0, "y1": 586.28}]}, "hash": "d675ae7d0005ca977ba18f50d2685aa12e6c04d2bfddb5cba217c0b5f6dabc2b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See http://oreilly.com/catalog/errata.csp?isbn=9798341622661 for release details. <br><br>The O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Generative AI Design Patterns, the \ncover image, and related trade dress are trademarks of O\u2019Reilly Media, Inc. <br><br>The views expressed in this work are those of the authors and do not represent the publisher\u2019s views. \nWhile the publisher and the authors have used good faith efforts to ensure that the information and \ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility \nfor errors or omissions, including without limitation responsibility for damages resulting from the use \nof or reliance on this work. Use of the information and instructions contained in this work is at your \nown risk. If any code samples or other technology this work contains or describes is subject to open \nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use \nthereof complies with such licenses and/or rights. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4684e8ad-e933-467c-a7c4-241d7aeb6107": {"__data__": {"id_": "4684e8ad-e933-467c-a7c4-241d7aeb6107", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.23, "x1": 432.0, "y1": 586.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "51134260-f449-4ed2-97de-69e94473ecd8", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 230.76, "x1": 432.0, "y1": 360.26}]}, "hash": "c01cfa9cbbeae3a030aa5280332601c69ba9233b67f6f03ae85aa5dffb439635", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adb2b9a7-3a78-4cea-9b52-e04c0ac56deb", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 661.5, "page_width": 504.0, "x0": 72.91, "y0": 76.64, "x1": 432.0, "y1": 127.5}]}, "hash": "3884f651012da37637139ae74e1a03f5430a40f004bedbdafc95ec27d8f7fa66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Table of Contents**<br><br>**Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi**<br><br>**1.**<br><br>**Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1**\nGenAI Design Patterns 1 \nBuilding on Foundational Models 3 \nPrompt and Context 3 \nUsing the Model Provider\u2019s API 5 \nUsing an LLM-Agnostic Framework 6 \nRunning Your Model Locally 6 \nHow Foundational Models Are Created 7 \nThe Landscape of Foundational Models 10 \nAgentic AI 12 \nAutonomy 13 \nCharacteristics of Agents 13 \nFine-Grained Control 15 \nLogits 15 \nTemperature 16 \nTop-K Sampling 18 \nNucleus Sampling 18 \nBeam Search 19 \nIn-Context Learning 20 \nZero-Shot Learning 20 \nFew-Shot Learning 21 \nPost-Training 22 \nPost-Training methods 22 \nFine-Tuning a Frontier Model 24 \nFine-Tuning an Open-Weight Model 25 \nConsiderations 26 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "adb2b9a7-3a78-4cea-9b52-e04c0ac56deb": {"__data__": {"id_": "adb2b9a7-3a78-4cea-9b52-e04c0ac56deb", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 661.5, "page_width": 504.0, "x0": 72.91, "y0": 76.64, "x1": 432.0, "y1": 127.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4684e8ad-e933-467c-a7c4-241d7aeb6107", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.23, "x1": 432.0, "y1": 586.28}]}, "hash": "d675ae7d0005ca977ba18f50d2685aa12e6c04d2bfddb5cba217c0b5f6dabc2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84811740-609e-4e67-a8cf-f9a0b8cf222f", "node_type": "1", "metadata": {"bbox": [{"page": 9, "page_height": 661.5, "page_width": 504.0, "x0": 72.92, "y0": 78.44, "x1": 432.01, "y1": 104.1}]}, "hash": "aa9e07ac5d24d4d519233c8287d7ad9a33a08c61900ad91ad0c71d2dedcda6d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**6.**<br><br>**Improving Reliability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275**\nPattern 17: LLM-as-Judge 275 \nProblem 276 \nSolution 277 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84811740-609e-4e67-a8cf-f9a0b8cf222f": {"__data__": {"id_": "84811740-609e-4e67-a8cf-f9a0b8cf222f", "embedding": null, "metadata": {"bbox": [{"page": 9, "page_height": 661.5, "page_width": 504.0, "x0": 72.92, "y0": 78.44, "x1": 432.01, "y1": 104.1}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "adb2b9a7-3a78-4cea-9b52-e04c0ac56deb", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 661.5, "page_width": 504.0, "x0": 72.91, "y0": 76.64, "x1": 432.0, "y1": 127.5}]}, "hash": "3884f651012da37637139ae74e1a03f5430a40f004bedbdafc95ec27d8f7fa66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d00f936d-0787-452b-97a8-033ca4b18aac", "node_type": "1", "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 195.23, "x1": 432.01, "y1": 586.28}]}, "hash": "5a524eb00500a5d6eecf0d6ec180b2eb0d60e02ace2f3c2df6cd6cae279af5c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**9. Setting Safeguards. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417**\nPattern 29: Template Generation 417 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d00f936d-0787-452b-97a8-033ca4b18aac": {"__data__": {"id_": "d00f936d-0787-452b-97a8-033ca4b18aac", "embedding": null, "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 195.23, "x1": 432.01, "y1": 586.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "84811740-609e-4e67-a8cf-f9a0b8cf222f", "node_type": "1", "metadata": {"bbox": [{"page": 9, "page_height": 661.5, "page_width": 504.0, "x0": 72.92, "y0": 78.44, "x1": 432.01, "y1": 104.1}]}, "hash": "aa9e07ac5d24d4d519233c8287d7ad9a33a08c61900ad91ad0c71d2dedcda6d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74d24e36-9bff-4138-a41c-6c152df3b6c7", "node_type": "1", "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.83, "x1": 432.01, "y1": 187.13}]}, "hash": "99da2dc473ad1e4111e588796511cba8941ded8674e89a0f3fa09bf1cb48b599", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Preface**<br><br>If you\u2019re an AI engineer building generative AI (GenAI) applications, you\u2019ve likely \nexperienced the frustrating gap between the ease of creating impressive prototypes \nand the complexity of deploying them reliably in production. While foundational \nmodels make it easy to build compelling demos, production systems demand solu\u2010 \ntions to fundamental challenges: hallucinations that compromise accuracy, inconsis\u2010 \ntent outputs that break downstream processes, knowledge gaps that limit enterprise \napplicability, and reliability issues that make systems unsuitable for critical applica\u2010 \ntions. <br><br>This book bridges that gap by providing 32 battle-tested design patterns that address \nthe recurring problems you\u2019ll encounter when building production-grade GenAI \napplications. These patterns aren\u2019t theoretical constructs\u2014they codify proven solu\u2010 \ntions that are often derived from cutting-edge research and refined by practitioners \nwho have successfully deployed GenAI systems at scale. <br><br>Supervised machine learning (ML) involves training a problem-specific model on a \nlarge training dataset of example inputs and outputs\u2014but GenAI applications rarely \ninclude a training phase. Instead, they commonly use general-purpose foundational \nmodels. This book is focused on design patterns for AI applications that are built \non top of foundational models, such as Open AI\u2019s GPT, Anthropic\u2019s Claude, Google\u2019s \nGemini, and Meta\u2019s Llama. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74d24e36-9bff-4138-a41c-6c152df3b6c7": {"__data__": {"id_": "74d24e36-9bff-4138-a41c-6c152df3b6c7", "embedding": null, "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.83, "x1": 432.01, "y1": 187.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d00f936d-0787-452b-97a8-033ca4b18aac", "node_type": "1", "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 195.23, "x1": 432.01, "y1": 586.28}]}, "hash": "5a524eb00500a5d6eecf0d6ec180b2eb0d60e02ace2f3c2df6cd6cae279af5c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e016ee1f-2079-49df-95c3-fcc27459bf64", "node_type": "1", "metadata": {"bbox": [{"page": 13, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 432.01, "y1": 253.74}]}, "hash": "c083c193aa3bcf3437fee2d29dfa16c5ef2810ef95707f018133ebb7b2dedc5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this book, we cover the entire AI engineering workflow. After an introduction \nin Chapter 1, Chapter 2 provides practical patterns for controlling content style and \nformat (including Logits Masking [Pattern 1] and Grammar [Pattern 2]). Chapter 3 \nand Chapter 4 cover integrating external knowledge through sophisticated retrieval- \naugmented generation (RAG) implementations, from Basic RAG (Pattern 6) to Deep \nSearch (Pattern 12). Chapter 5 is about enhancing your model\u2019s reasoning capabilities \nwith patterns like Chain of Thought (Pattern 13), Tree of Thoughts (Pattern 14), \nand Adapter Tuning (Pattern 15). Chapter 6 emphasizes building reliable systems \nwith LLM-as-Judge (Pattern 17), Reflection (Pattern 18), and Prompt Optimization ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e016ee1f-2079-49df-95c3-fcc27459bf64": {"__data__": {"id_": "e016ee1f-2079-49df-95c3-fcc27459bf64", "embedding": null, "metadata": {"bbox": [{"page": 13, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 432.01, "y1": 253.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "74d24e36-9bff-4138-a41c-6c152df3b6c7", "node_type": "1", "metadata": {"bbox": [{"page": 12, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.83, "x1": 432.01, "y1": 187.13}]}, "hash": "99da2dc473ad1e4111e588796511cba8941ded8674e89a0f3fa09bf1cb48b599", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "585f4c7f-7b87-4b8a-96a5-fc66c3faa5df", "node_type": "1", "metadata": {"bbox": [{"page": 15, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249998}]}, "hash": "e8c812c6a8fc4fa51911b1e6a80eedc808b71d65ac56c2594e4579b7319c7b40", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AI engineering overlaps heavily with software engineering, data engineering, and \nML\u2014but in this book, we\u2019ve limited our focus to core AI engineering. We encourage \nyou to think of this book as a companion to the literature on patterns in related \nareas. Specifically, the book Machine Learning Design Patterns (O\u2019Reilly), also co- \nauthored by Valliappa Lakshmanan, covers proven solutions to recurring issues you\u2019ll \nencounter when training a bespoke machine-learning model for a specific problem. <br><br>You\u2019ll also likely find yourself working with both bespoke ML models and general- \npurpose foundational models, depending on the use case. In some situations, you \nmight start with a foundational model but then find that edge cases require you \nto customize (or fine-tune) it for your problem. This book and Machine Learning \nDesign Patterns are complementary and will help you work with both models, so we \nrecommend that you read both. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "585f4c7f-7b87-4b8a-96a5-fc66c3faa5df": {"__data__": {"id_": "585f4c7f-7b87-4b8a-96a5-fc66c3faa5df", "embedding": null, "metadata": {"bbox": [{"page": 15, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249998}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e016ee1f-2079-49df-95c3-fcc27459bf64", "node_type": "1", "metadata": {"bbox": [{"page": 13, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 432.01, "y1": 253.74}]}, "hash": "c083c193aa3bcf3437fee2d29dfa16c5ef2810ef95707f018133ebb7b2dedc5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e24c3fa-1b1b-41b1-9180-cbe6daacfda1", "node_type": "1", "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.86, "x1": 432.0, "y1": 607.97}]}, "hash": "172f51753c3b9ab8a9b6856a2d0812ff0a3d83294e1d93161d41f0be4e90b9bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure P-1. Color scheme used in diagrams in this book <br><br>**Using Code Examples**\nSupplemental material (code examples, exercises, etc.) is available for download at \nhttps://github.com/oreillymedia/title_title. <br><br>If you have a technical question or a problem using the code examples, please send \nemail to support@oreilly.com. <br><br>This book is here to help you get your job done. In general, if example code is \noffered with this book, you may use it in your programs and documentation. You \ndo not need to contact us for permission unless you\u2019re reproducing a significant \nportion of the code. For example, writing a program that uses several chunks of code \nfrom this book does not require permission. Selling or distributing examples from \nO\u2019Reilly books does require permission. Answering a question by citing this book \nand quoting example code does not require permission. Incorporating a significant \namount of example code from this book into your product\u2019s documentation does \nrequire permission. <br><br>We appreciate, but generally do not require, attribution. An attribution usually \nincludes the title, author, publisher, and ISBN. For example: \u201cBook Title by Some \nAuthor (O\u2019Reilly). Copyright 2012 Some Copyright Holder, 978-0-596-xxxx-x.\u201d <br><br>If you feel your use of code examples falls outside fair use or the permission given \nabove, feel free to contact us at permissions@oreilly.com. <br><br>**xiv** **| Preface**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e24c3fa-1b1b-41b1-9180-cbe6daacfda1": {"__data__": {"id_": "2e24c3fa-1b1b-41b1-9180-cbe6daacfda1", "embedding": null, "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.86, "x1": 432.0, "y1": 607.97}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "585f4c7f-7b87-4b8a-96a5-fc66c3faa5df", "node_type": "1", "metadata": {"bbox": [{"page": 15, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249998}]}, "hash": "e8c812c6a8fc4fa51911b1e6a80eedc808b71d65ac56c2594e4579b7319c7b40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ce67e5e-7055-4ce5-ad9c-62c60ac6c8db", "node_type": "1", "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.0, "y1": 254.76}]}, "hash": "a0510f2d2952edb2f42e0fcb73f691764a4af0c7cc07c9fbb61e090eac0b9ab4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**O\u2019Reilly Online Learning**<br><br>For more than 40 years, O\u2019Reilly Media has provided technol\u2010 \nogy and business training, knowledge, and insight to help \ncompanies succeed. <br><br>Our unique network of experts and innovators share their knowledge and expertise \nthrough books, articles, and our online learning platform. O\u2019Reilly\u2019s online learning \nplatform gives you on-demand access to live training courses, in-depth learning \npaths, interactive coding environments, and a vast collection of text and video from \nO\u2019Reilly and 200+ other publishers. For more information, visit https://oreilly.com. <br><br>**How to Contact Us**\nPlease address comments and questions concerning this book to the publisher: <br><br>O\u2019Reilly Media, Inc. \n141 Stony Circle, Suite 195 \nSanta Rosa, CA 95401 \n800-889-8969 (in the United States or Canada) \n707-827-7019 (international or local) \n707-829-0104 (fax) \nsupport@oreilly.com \nhttps://oreilly.com/about/contact.html <br><br>We have a web page for this book, where we list errata and any additional informa\u2010 \ntion. You can access this page at https://www.oreilly.com/catalog/<catalog page>. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ce67e5e-7055-4ce5-ad9c-62c60ac6c8db": {"__data__": {"id_": "7ce67e5e-7055-4ce5-ad9c-62c60ac6c8db", "embedding": null, "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.0, "y1": 254.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2e24c3fa-1b1b-41b1-9180-cbe6daacfda1", "node_type": "1", "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.86, "x1": 432.0, "y1": 607.97}]}, "hash": "172f51753c3b9ab8a9b6856a2d0812ff0a3d83294e1d93161d41f0be4e90b9bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e29c259c-42c7-41e7-a3d9-2920996831c0", "node_type": "1", "metadata": {"bbox": [{"page": 18, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.07, "x1": 432.01, "y1": 582.33}]}, "hash": "9d214566da0c8a2f82ea30a0807133d19276c247a8fa2610c10e164015e33a67", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For news and information about our books and courses, visit https://oreilly.com. <br><br>Find us on LinkedIn: https://linkedin.com/company/oreilly-media <br><br>Watch us on YouTube: https://youtube.com/oreillymedia <br><br>**Acknowledgments**\nLak is thankful to his family for their forbearance as he (once again) vanished deep \ninto writing and to collaborators and colleagues who gave him the opportunity to \nwork far and wide with exciting new technology in practical ways. He\u2019s also deeply \nappreciative of Hannes for the partnership while writing this book. <br><br>Hannes would like to thank Lak for his insightful mentorship and guidance through\u2010 \nout the writing process. Lak\u2019s ability to explain complex topics in simple terms is \ntruly exceptional, and Hannes is deeply grateful for being taken on this writing ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e29c259c-42c7-41e7-a3d9-2920996831c0": {"__data__": {"id_": "e29c259c-42c7-41e7-a3d9-2920996831c0", "embedding": null, "metadata": {"bbox": [{"page": 18, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.07, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7ce67e5e-7055-4ce5-ad9c-62c60ac6c8db", "node_type": "1", "metadata": {"bbox": [{"page": 16, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.0, "y1": 254.76}]}, "hash": "a0510f2d2952edb2f42e0fcb73f691764a4af0c7cc07c9fbb61e090eac0b9ab4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "048107fe-3278-42dd-b341-46c1f5898f80", "node_type": "1", "metadata": {"bbox": [{"page": 19, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 117.44, "x1": 432.01, "y1": 253.94}]}, "hash": "26b732cf6b063bff64cb18b0724926bc0bfd7a9a2dca5aeed8876ac5bdc799a8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 1**\n**Introduction**<br><br>GenAI is so powerful and easy-to-use that even nontechnical users can easily proto\u2010 \ntype very compelling applications on top of GenAI. However, taking such GenAI \nprototypes to production is hard because GenAI models are unreliable\u2014they can \nhallucinate, return different answers to the same input, and can have surprising \nlimitations because of how they are trained. The design patterns in this book capture \nbest practices and solutions to these and other recurring problems you\u2019re likely to \nencounter when building production applications on top of GenAI models. <br><br>**GenAI Design Patterns**\nDesign patterns, in software engineering, are proven solutions to common problems \nthat occur during software design and development. They represent standardized \nbest practices that have evolved over time through the collective experience of soft\u2010 \nware developers. Design patterns are important because they establish a common \nvocabulary developers can use to communicate efficiently and because they help \nimprove software quality, maintainability, and scalability. <br><br>The concept of design patterns was heavily influenced by the work of architect \nChristopher Alexander, who introduced patterns in architecture in his book A Pat\u2010 \ntern Language (Oxford University Press, 1977). Design patterns gained significant \nprominence in software engineering with the publication of the book Design Patterns: \nElements of Reusable Object-Oriented Software by Erich Gamma, Richard Helm, \nRalph Johnson, and John Vlissides (Addison-Wesley), which is often called \u201cthe Gang \nof Four book.\u201d Since then, design patterns have been catalogued for other software \nengineering domains, such as for Java Enterprise applications and ML. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "048107fe-3278-42dd-b341-46c1f5898f80": {"__data__": {"id_": "048107fe-3278-42dd-b341-46c1f5898f80", "embedding": null, "metadata": {"bbox": [{"page": 19, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 117.44, "x1": 432.01, "y1": 253.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e29c259c-42c7-41e7-a3d9-2920996831c0", "node_type": "1", "metadata": {"bbox": [{"page": 18, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.07, "x1": 432.01, "y1": 582.33}]}, "hash": "9d214566da0c8a2f82ea30a0807133d19276c247a8fa2610c10e164015e33a67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dce5e87f-9ea9-42a0-8ad9-1ca57b733ac1", "node_type": "1", "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 268.61, "x1": 423.75, "y1": 462.1}]}, "hash": "47c7a11b5e96db6f433e72e2c016fdd17f3f36a3f2b386d7a10199ab45be9aeb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AI engineers often encounter tasks that are too complex for a foundational model \nto perform all at once, so a common tactic is to break the complex task into smaller \ncomponents that can be accomplished by foundational models. Such small software \ncomponents that provide capabilities with the help of foundational models are called \nagents. Agents become increasingly autonomous as they use GenAI models to plan \nout a sequence of operations, identify the backend tools that they can invoke for \neach operation, determine how to recover from errors, and/or evaluate whether the \ntask is complete. Applications that are built by orchestrating agents are called agentic. \nBy showing you how to handle the inevitable challenges that arise when building \napplications on foundational models, the patterns in this book will help you build \nbetter agents and agentic applications. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dce5e87f-9ea9-42a0-8ad9-1ca57b733ac1": {"__data__": {"id_": "dce5e87f-9ea9-42a0-8ad9-1ca57b733ac1", "embedding": null, "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 268.61, "x1": 423.75, "y1": 462.1}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "048107fe-3278-42dd-b341-46c1f5898f80", "node_type": "1", "metadata": {"bbox": [{"page": 19, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 117.44, "x1": 432.01, "y1": 253.94}]}, "hash": "26b732cf6b063bff64cb18b0724926bc0bfd7a9a2dca5aeed8876ac5bdc799a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4fd7240-62f0-476f-9ef8-bc862c402793", "node_type": "1", "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 100.52, "x1": 432.0, "y1": 245.79}]}, "hash": "df6a7f2e8b4fdb5bf7d9bc7c8100f673c827e9c7e769b6d832c75af0f943bd0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**A Note on Models and Frameworks**\nIn Machine Learning Design Patterns, we used just two frameworks (scikit-learn \nand TensorFlow) and a single hyperscaler (Google Cloud Platform [GCP]) for consis\u2010 \ntency, but many readers felt that the resulting examples were too TensorFlow- and \nGCP-heavy. Therefore, in this book, we endeavor to be agnostic to model, framework, \nand hyperscaler. <br><br>Our code examples employ a wide range of technologies from a number of different \nvendors: large language models (LLMs) from OpenAI, Anthropic, Google, Alibaba, \nand Meta; and GenAI frameworks like LangChain, Pydantic AI, Hugging Face, and \nDSPy. Our examples are also agnostic to hyperscalers such as Amazon Web Services \n(AWS), Azure, GCP, and Oracle Cloud Infrastructure. Since you\u2019re likely to be using \na different model in a different framework to address a different scenario, the code \nexamples are not meant to serve as starting points for your implementation\u2014we \nfully intend that you will have to adapt the code examples to your preferred LLM, \nframework, and hyperscaler. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4fd7240-62f0-476f-9ef8-bc862c402793": {"__data__": {"id_": "b4fd7240-62f0-476f-9ef8-bc862c402793", "embedding": null, "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 100.52, "x1": 432.0, "y1": 245.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dce5e87f-9ea9-42a0-8ad9-1ca57b733ac1", "node_type": "1", "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 268.61, "x1": 423.75, "y1": 462.1}]}, "hash": "47c7a11b5e96db6f433e72e2c016fdd17f3f36a3f2b386d7a10199ab45be9aeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3813dab6-431b-4d37-b880-8f3fa0e2e3d4", "node_type": "1", "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 264.72, "x1": 432.0, "y1": 605.54}]}, "hash": "e4cb32901b8d8b378dc7481063357c384b582d87e1feee7ace81ccebd9c8a701", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Prompt and Context**\nWhen you build AI applications, you typically invoke hosted foundational models \nthrough an API. This might be the API provided by the vendor of the foundational \nmodel, or it might be a framework that allows you to easily switch between providers. <br><br>You invoke a foundational model by sending it a prompt and getting back a response. \nYou are, doubtless, familiar with doing this by using the web user interface of a \nfoundational model. For example, on ChatGPT, you might type a prompt like this \none into the text box: <br><br>Create a pencil sketch in the style of Degas depicting a family of four playing a board \ngame ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3813dab6-431b-4d37-b880-8f3fa0e2e3d4": {"__data__": {"id_": "3813dab6-431b-4d37-b880-8f3fa0e2e3d4", "embedding": null, "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 264.72, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b4fd7240-62f0-476f-9ef8-bc862c402793", "node_type": "1", "metadata": {"bbox": [{"page": 20, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 100.52, "x1": 432.0, "y1": 245.79}]}, "hash": "df6a7f2e8b4fdb5bf7d9bc7c8100f673c827e9c7e769b6d832c75af0f943bd0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44eaabf6-eb25-4eea-b477-c0ea9c1d2ad3", "node_type": "1", "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.04, "x1": 432.01, "y1": 253.17}]}, "hash": "1e9b9c1fa913f4b632218d3c88f69ac9408a50c7c4451b9a65be40009756d1bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A more complex prompt might consist not just of an instruction but also context. \nThis might be information that the model should use in the generation, or it might \ninvolve asking the model to take on a specific role. For example, we could tell Claude \n(on Claude.ai) to take on a personality and use some information when generating \ncontent: <br><br>You are an expert marketer who is very familiar with the book market in university \ntowns in Germany. <br><br>Covenant of Water is a novel that tells the story of three generations of an Orthodox \nSaint Thomas Christian family in Kerala. <br><br>Write a one-paragraph blurb introducing the book to readers at a bookstore in G\u00f6ttin\u2010 \ngen, drawing local connections. <br><br>The response from Claude Sonnet takes into account the role, context, and instruc\u2010 \ntion: <br><br>**Covenant of Water - A Literary Journey to Kerala**<br><br>Nestled between the historic walls of G\u00f6ttingen, where academic tradition meets \nliterary curiosity, we present Abraham Verghese\u2019s masterful epic, \u201cThe Covenant of \nWater.\u201d This sweeping saga follows three generations of an Orthodox Saint Thomas \nChristian family in Kerala, India\u2014a region whose complex religious heritage might \nresonate with students of our university\u2019s renowned theological studies program. As \nthe story unfolds across decades amid Kerala\u2019s lush backwaters, readers familiar with \nG\u00f6ttingen\u2019s own relationship with water\u2014from the sacred G\u00e4nseliesel fountain to \nthe meandering Leine River\u2014will find surprising parallels in how geography shapes \ndestiny. Verghese weaves a tale of family secrets, medical mysteries, and generational \nbonds that transcends cultural boundaries, offering our community of international \nscholars and lifelong learners a profound exploration of how water\u2014whether in Kera\u2010 \nla\u2019s monsoons or Lower Saxony\u2019s gentle rains\u2014becomes both metaphor and covenant, \nconnecting us across time and space. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44eaabf6-eb25-4eea-b477-c0ea9c1d2ad3": {"__data__": {"id_": "44eaabf6-eb25-4eea-b477-c0ea9c1d2ad3", "embedding": null, "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.04, "x1": 432.01, "y1": 253.17}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3813dab6-431b-4d37-b880-8f3fa0e2e3d4", "node_type": "1", "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 264.72, "x1": 432.0, "y1": 605.54}]}, "hash": "e4cb32901b8d8b378dc7481063357c384b582d87e1feee7ace81ccebd9c8a701", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ac0cfa6-fe14-4c5f-a1fa-790e87292704", "node_type": "1", "metadata": {"bbox": [{"page": 23, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.35, "x1": 450.25, "y1": 267.24}]}, "hash": "393b4691224c91dab38d54a0f4d1f6f9f0335dd76ed0c11d4cf396e4ccee6fcc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Using the Model Provider\u2019s API**\nAPIs that are designed to invoke foundational models are available in a wide variety \nof languages\u2014Python, Go, Java, and TypeScript are all commonly employed. For \nsimplicity, we focus on Python APIs in this book. <br><br>To invoke Claude Sonnet 3 through the Anthropic API in Python, you could use the \nanthropic Python package as follows (the full code is on GitHub): <br><br>**import anthropic**\nclient = anthropic.Anthropic( \n*# defaults to os.environ.get(\"ANTHROPIC_API_KEY\")*\n api_key=\"YOUR_ANTHROPIC_API_KEY\", \n) <br><br>completion = client.messages.create( \n model=\"claude-3-7-sonnet-latest\", ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ac0cfa6-fe14-4c5f-a1fa-790e87292704": {"__data__": {"id_": "0ac0cfa6-fe14-4c5f-a1fa-790e87292704", "embedding": null, "metadata": {"bbox": [{"page": 23, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.35, "x1": 450.25, "y1": 267.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "44eaabf6-eb25-4eea-b477-c0ea9c1d2ad3", "node_type": "1", "metadata": {"bbox": [{"page": 22, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.04, "x1": 432.01, "y1": 253.17}]}, "hash": "1e9b9c1fa913f4b632218d3c88f69ac9408a50c7c4451b9a65be40009756d1bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33ccd756-67eb-436c-a486-c08b1b2bc91c", "node_type": "1", "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "hash": "6a3489e715e4728bad97f12d246da7143eb189a1da3f3cfc2a4dcf0f29893af5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "result = agent.run_sync(\"Write code to find the median value of a list of integers.\") \nprint(result.data) <br><br>The advantage here \nfoundational \nmodel providers by switching the model string to openai:gpt-4o-mini, google- \nvertex:gemini-2.0-flash, groq:llama3-70b-8192, and so on (see Pydantic\u2019s docu\u2010 \nmentation for the full list of models supported). <br><br>is  that you can easily switch between <br><br>The class in the Pydantic API that invokes the Claude model is called Agent. We\u2019ll \ndiscuss what agents are in the next section, but before that, let\u2019s conclude our discus\u2010 \nsion of ways to invoke foundational models. <br><br>**Running Your Model Locally**\nTo run a model such as Llama 3 on your local hardware, you could use the Ollama \nclient to download and run the model that you want to use: <br><br>ollama run llama3.2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33ccd756-67eb-436c-a486-c08b1b2bc91c": {"__data__": {"id_": "33ccd756-67eb-436c-a486-c08b1b2bc91c", "embedding": null, "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0ac0cfa6-fe14-4c5f-a1fa-790e87292704", "node_type": "1", "metadata": {"bbox": [{"page": 23, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.35, "x1": 450.25, "y1": 267.24}]}, "hash": "393b4691224c91dab38d54a0f4d1f6f9f0335dd76ed0c11d4cf396e4ccee6fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7236fbbf-1e02-478e-8d6c-9adced49a440", "node_type": "1", "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.84, "x1": 432.0, "y1": 297.74}]}, "hash": "70a6c68711f3c4ab068dfe9b53b293f3c9fdcb84b9a89356c7d263cfb19ac8f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "comparison, amount to about 1.2 million tokens\u2014so the DeepSeek training dataset \nis equivalent to 12 million Shakespeares!) Unlike early LLMs, which were trained on \nwords, modern LLMs are trained on tokens, which are short sequences of characters. \nThis allows such LLMs to learn things that are not in the vocabulary of the language, \nlike proper names. It isn\u2019t just size that helps\u2014the DeepSeek team attributes the high \nquality of their models to careful data curation, including rigorous deduplication \nprocesses. <br><br>The pretraining stage (Step 1 in Figure 1-2) involves training on this massive dataset \nof tokens to develop the model\u2019s general language-understanding capabilities. The \nkey goal at this stage is to train the model to predict the next token, given a context \nconsisting of the previous tokens in the training input. This is why you\u2019ll often hear \npeople call LLMs next-token predictors\u2014but next-token prediction is only the first \nstage of the training regimen. <br><br>Following pretraining, the model undergoes supervised fine-tuning (SFT, Step 2 in \nFigure 1-2) to improve its ability to follow instructions and generate high-quality \nresponses. This stage uses carefully curated datasets of human-written examples. \nCohere, for example, has said it uses licensed physicians, financial analysts, and \naccountants to improve its models. Presumably, these practitioners write ideal \nanswers given a prompt. The result, DeepSeek-V3, is a mixture of experts (MoE) \nmodel: an optimization that allows models to have a large number of parameters \nwhile activating only a fraction of them for each token. DeepSeek-V3 has 671 billion \nparameters in total, but only 37 B are activated per token. This allows the model to \nuse different pathways for different types of instructions. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7236fbbf-1e02-478e-8d6c-9adced49a440": {"__data__": {"id_": "7236fbbf-1e02-478e-8d6c-9adced49a440", "embedding": null, "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.84, "x1": 432.0, "y1": 297.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "33ccd756-67eb-436c-a486-c08b1b2bc91c", "node_type": "1", "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "hash": "6a3489e715e4728bad97f12d246da7143eb189a1da3f3cfc2a4dcf0f29893af5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72617f1b-140a-4fbd-8212-c0e203e0b435", "node_type": "1", "metadata": {"bbox": [{"page": 27, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 246.43, "x1": 432.0, "y1": 349.27}]}, "hash": "474848feb5aa7b77d4988753a77275c2f6b68c0628a53631a318039d9eb6a4fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reinforcement learning stage (Step 3 in Figure 1-2) is where the models are further \nrefined based on human preferences. This step involves reinforcement learning with \nhuman feedback (RLHF), which means showing human raters a pair of generated \noutputs and asking them which one they prefer. Such preference tuning helps align the \nmodels\u2019 outputs with human expectations and values. This stage is also sometimes \ncalled preference optimization. <br><br>Once DeepSeek-V3 was created with a small number of human-written examples \nand human preferences, the full DeepSeek-R1 model was developed through the \nfollowing multistage process: <br><br>1. The cold start involved fine-tuning a base model (DeepSeek-V3-Base) with thou\u2010 \n1. <br><br>sands of cold-start data points to lay a foundation. <br><br>2. \n2. Pure reinforcement learning (RL) involved applying a pure reinforcement learning <br><br>approach to enhance reasoning skills. <br><br>3. Rejection sampling involved using rejection sampling to create synthetic data by \n3. <br><br>selecting the best examples from the last successful RL run. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72617f1b-140a-4fbd-8212-c0e203e0b435": {"__data__": {"id_": "72617f1b-140a-4fbd-8212-c0e203e0b435", "embedding": null, "metadata": {"bbox": [{"page": 27, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 246.43, "x1": 432.0, "y1": 349.27}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7236fbbf-1e02-478e-8d6c-9adced49a440", "node_type": "1", "metadata": {"bbox": [{"page": 26, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.84, "x1": 432.0, "y1": 297.74}]}, "hash": "70a6c68711f3c4ab068dfe9b53b293f3c9fdcb84b9a89356c7d263cfb19ac8f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ee49f38-4964-4da1-a730-d68a265ff03c", "node_type": "1", "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.04, "x1": 432.01, "y1": 605.54}]}, "hash": "54105454adc25a38e69f6ab3ec79cd17e369208920f192a54d9ca362fbea79ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The Landscape of Foundational Models**\nThe GenAI foundational model ecosystem has evolved significantly, with distinct \ncategories emerging to serve different needs. <br><br>Because academic benchmarks are saturated and can be gamed, the currently most \naccepted way to rate GenAI models is to compare them pair-wise in blind tests. \nLMArena carries out a large-scale comparison, and Figure 1-3 shows the April 2025 \nleaderboard. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ee49f38-4964-4da1-a730-d68a265ff03c": {"__data__": {"id_": "4ee49f38-4964-4da1-a730-d68a265ff03c", "embedding": null, "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.04, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "72617f1b-140a-4fbd-8212-c0e203e0b435", "node_type": "1", "metadata": {"bbox": [{"page": 27, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 246.43, "x1": 432.0, "y1": 349.27}]}, "hash": "474848feb5aa7b77d4988753a77275c2f6b68c0628a53631a318039d9eb6a4fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f481bfa8-75d7-43f0-b415-67765d675983", "node_type": "1", "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.64, "x1": 432.0, "y1": 249.7}]}, "hash": "e9b3589a913bf0b2732c199381150f69a4b25b8c31b559743d21a90980278848", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "capabilities, enhanced reasoning, and extended-content windows (with up to two \nmillion tokens in some models). <br><br>Distilled versions of frontier models balance performance with efficiency, offering \nreasonable capabilities at lower costs and with faster response times. Leading exam\u2010 \nples include Gemini Flash, Claude Sonnet, and GPT-4o-mini. These models tend \nto offer good performance on common tasks like content generation and summariza\u2010 \ntion. They also offer fast response times, and they\u2019re cost-effective for high-volume \napplications. The cost difference between running Gemini Pro and Gemini Flash \nbecomes quite apparent once you note that the x-axis in Figure 1-3 is logarithmic\u2014 \nFlash is 20 times less expensive. <br><br>Open-weight models have their parameters publicly available, allowing for trans\u2010 \nparency, community improvement, and customization. Examples include LLama, \nMistral, DeepSeek, Qwen, and Falcon. These models offer strong performance but \ngenerally lag behind frontier models, and they can be fine-tuned on proprietary data \nbut require more expertise to host. However, there are hosted API services such as \nTogether.ai, as well as fully managed API endpoints on the hyperscalers, that address \nthis issue. <br><br>Locally hostable models are designed to run on consumer or enterprise hardware \nwithout requiring cloud connectivity. Examples include LLama 8B and Gemma 2B, \nhardware-optimized versions of which are available through NVIDIA NIM. This \nallows for complete privacy, with no data leaving your local devices and no ongoing \nAPI costs. However, these models have significantly reduced capabilities compared \nto cloud models. The demand for frontier models in \u201cair-gapped\u201d systems that are \ndisconnected from the internet has led some proprietary model vendors to offer this \nservice as well\u2014for example, Gemini can be run on-premises in Google Distributed \nCloud and OpenAI can be run on Azure\u2019s on-premises offerings. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f481bfa8-75d7-43f0-b415-67765d675983": {"__data__": {"id_": "f481bfa8-75d7-43f0-b415-67765d675983", "embedding": null, "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.64, "x1": 432.0, "y1": 249.7}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4ee49f38-4964-4da1-a730-d68a265ff03c", "node_type": "1", "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.04, "x1": 432.01, "y1": 605.54}]}, "hash": "54105454adc25a38e69f6ab3ec79cd17e369208920f192a54d9ca362fbea79ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "234330d9-b6f1-478a-8352-e91c23b2ea02", "node_type": "1", "metadata": {"bbox": [{"page": 30, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.82, "x1": 432.0, "y1": 338.73}]}, "hash": "df36170e776ba46b6003a3793948af046a7eac88cf9e953e81196d1efee98e23", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Agentic AI**\nThe class in the Pydantic API that invoked the Claude model was called Agent. <br><br>**from pydantic_ai import**Agent \nagent =**Agent**('anthropic:claude-3-7-sonnet-latest', \n ... <br><br>What is an agent? In computer science, the term agent has long been used to describe \nsoftware entities that act on behalf of users or other programs. When you invoke \na foundational model, you specify a role, provide some context, and ask it to carry \nout some instruction. In the computer science sense, then, the LLM is acting as your \nagent. <br><br>For example, here\u2019s an example of creating an agent to manage inventory levels in a \nstore (the full code is on GitHub): ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "234330d9-b6f1-478a-8352-e91c23b2ea02": {"__data__": {"id_": "234330d9-b6f1-478a-8352-e91c23b2ea02", "embedding": null, "metadata": {"bbox": [{"page": 30, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.82, "x1": 432.0, "y1": 338.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f481bfa8-75d7-43f0-b415-67765d675983", "node_type": "1", "metadata": {"bbox": [{"page": 29, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.64, "x1": 432.0, "y1": 249.7}]}, "hash": "e9b3589a913bf0b2732c199381150f69a4b25b8c31b559743d21a90980278848", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86444a78-bbd3-46f8-a11b-e134d6a991aa", "node_type": "1", "metadata": {"bbox": [{"page": 31, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.04, "x1": 432.01, "y1": 332.54}]}, "hash": "41a1bad4938bb415dcc949f01b7ced060acfd7d6926b714d2192bd0310fe7e6e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Provide the list of items to the agent and it will figure out which ones to reorder: <br><br>result = agent.run_sync(f\"\"\" \nIdentify which of these items need to be reordered this week. <br><br>**Items**\n{items} \n\"\"\") <br><br>The result will include this, in part: <br><br>itemB <br><br>**quantity_to_order=300**reason_to_reorder='Current stock (100) is insufficient to \ncover projected demand over delivery time. Based on recent weekly sales (70-90 units), \nwe need to**order enough to cover the 2-week delivery period**plus maintain safety \nstock.\u2019 <br><br>Compare this to traditional programming, where you\u2019d have to write code to explic\u2010 \nitly manage inventory. Such autonomy\u2014which means the ability to operate inde\u2010 \npendently without constant human guidance or being explicitly programmed to do \nso\u2014is the key differentiator between traditional software and AI agents. <br><br>**Characteristics of Agents**\nBesides autonomy, agents are usually expected to have the following characteristics: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86444a78-bbd3-46f8-a11b-e134d6a991aa": {"__data__": {"id_": "86444a78-bbd3-46f8-a11b-e134d6a991aa", "embedding": null, "metadata": {"bbox": [{"page": 31, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.04, "x1": 432.01, "y1": 332.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "234330d9-b6f1-478a-8352-e91c23b2ea02", "node_type": "1", "metadata": {"bbox": [{"page": 30, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.82, "x1": 432.0, "y1": 338.73}]}, "hash": "df36170e776ba46b6003a3793948af046a7eac88cf9e953e81196d1efee98e23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be108ad-c1c6-4d76-be06-63e7d08d4a38", "node_type": "1", "metadata": {"bbox": [{"page": 33, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "a0a821eb0360a795b2d82581a0686cd4155a474b9a5d56a214784a699dd78846", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Adaptability and learning <br><br>How do you know that ordering 300 items is correct? A human inventory \nmanager would plug in assumptions of weekly sales for the next two weeks and \nvalidate that the store will not run out\u2014and an agent can do the same thing. \nIn Chapter 6 and Chapter 9 (respectively), you\u2019ll see patterns such as Reflection \n(Pattern 18) and Self-Check (Pattern 31) that allow an agent to evaluate its output \nand self-correct. <br><br>At the time of writing, agentic behavior remains an aspirational goal for applications \nbuilt on foundational models\u2014nondeterminism, hallucinations, and various other \nfailure modes pose challenges to building fully autonomous AI applications. Take \nnondeterminism, for example\u2014each time, you might get a different list and quantity \nof items to reorder. (Try it!) Planning works in simple cases, but not in hard ones. \nIn Chapter 5, you\u2019ll see patterns such as Chain of Thought (Pattern 13) that improve \nthe ability of an agent to do planning and reasoning. Many of the design patterns in \nthis book are ways to make your AI applications more agentic, or at least to push the \nboundaries of what you can build. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3be108ad-c1c6-4d76-be06-63e7d08d4a38": {"__data__": {"id_": "3be108ad-c1c6-4d76-be06-63e7d08d4a38", "embedding": null, "metadata": {"bbox": [{"page": 33, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "86444a78-bbd3-46f8-a11b-e134d6a991aa", "node_type": "1", "metadata": {"bbox": [{"page": 31, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.04, "x1": 432.01, "y1": 332.54}]}, "hash": "41a1bad4938bb415dcc949f01b7ced060acfd7d6926b714d2192bd0310fe7e6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c80b39b-d1f7-4605-a25f-4696b8db75b4", "node_type": "1", "metadata": {"bbox": [{"page": 34, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249997}]}, "hash": "65bf1b25f48f968dcb6f42c60a39d1a3aea09f0b65cc1363b661db586442fadb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of being selected. The transformation from logits to probabilities occurs through the \nsoftmax function: <br><br>P tokeni = elogiti/\u03a3 jelogitj <br><br>Here, P(token_i) is the probability of selecting token_i. The softmax function \naccentuates the peaks and dampens the tails\u2014for example, compare the length of the \nbars for the and a before and after the softmax function is applied in Figure 1-4. <br><br>If the distribution of potential continuations is less peaked, as shown in Figure 1-5, \nthen the impact of the softmax is less pronounced. <br><br><br><br>Figure 1-5. The impact of the*softmax*of the logits is less pronounced if the distribution \nis not peaked <br><br>We\u2019ll explore the use of logits to control style in Chapter 2. <br><br>**Temperature**\nTemperature (T) is a hyperparameter that controls the randomness of token selection \nby scaling the logits before they\u2019re passed through the softmax function. The modi\u2010 \nfied softmax equation with temperature is as follows: <br><br>P tokeni = elogiti/T  /\u03a3 jelogitj/T <br><br>The effect of scaling the same graphs with different values of T is shown in Fig\u2010 \nure 1-6. <br><br>**16** **|** **Chapter 1: Introduction**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c80b39b-d1f7-4605-a25f-4696b8db75b4": {"__data__": {"id_": "9c80b39b-d1f7-4605-a25f-4696b8db75b4", "embedding": null, "metadata": {"bbox": [{"page": 34, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249997}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3be108ad-c1c6-4d76-be06-63e7d08d4a38", "node_type": "1", "metadata": {"bbox": [{"page": 33, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "a0a821eb0360a795b2d82581a0686cd4155a474b9a5d56a214784a699dd78846", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aa28892-7477-467f-8930-e60f8010b834", "node_type": "1", "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 326.49, "x1": 432.04, "y1": 605.76}]}, "hash": "3c05553cb09728524636aab39c79046302afdc20c665f0098e108043e0f5efc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 1-6. Effects of scaling the logits by temperature <br><br>As you can see, setting the temperature to zero turns on greedy sampling. As the \ntemperature increases, the likelihood that less likely \u201ctail\u201d words will be chosen also \nincreases. The effect of temperature is less pronounced when the distribution was less \npeaked to begin with. <br><br>Here\u2019s how to vary the temperature when using PydanticAI: <br><br>agent = Agent('anthropic:claude-3-7-sonnet-latest', \n model_settings={ \n \"temperature\": 0.5 \n }, \n system_prompt=\"Complete the sentence.\") <br><br>Here\u2019s how to do it when using the Anthropic API directly: <br><br>completion = client.messages.create( \n model=\"claude-3-7-sonnet-latest\", \n system=\"Complete the sentence.\", \n temperature=0.5, \n messages=[ \n ... \n ] \n) <br><br>Here\u2019s an example, showing various continuations of the phrase The trade war caused \nproduced at three different temperature settings: <br><br>0.0 : The trade war caused significant disruptions to global supply chains, leading \nto increased prices for consumers and economic uncertainty for businesses across \nmultiple industries. <br><br>**Fine-Grained Control** **|** **17**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3aa28892-7477-467f-8930-e60f8010b834": {"__data__": {"id_": "3aa28892-7477-467f-8930-e60f8010b834", "embedding": null, "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 326.49, "x1": 432.04, "y1": 605.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9c80b39b-d1f7-4605-a25f-4696b8db75b4", "node_type": "1", "metadata": {"bbox": [{"page": 34, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249997}]}, "hash": "65bf1b25f48f968dcb6f42c60a39d1a3aea09f0b65cc1363b661db586442fadb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f2e94b8-23e9-4719-bf82-ef47b9f432f9", "node_type": "1", "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.6, "x1": 432.0, "y1": 318.39}]}, "hash": "cca1b28836beccc8b03557ae57c9f206151fc0587e3ef02a2e1db8c69a1f1b87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0.5 : The trade war caused significant disruptions to global supply chains, leading \nto increased prices for consumers and economic uncertainty for businesses across \nmultiple industries. Many manufacturers were forced to reconsider their production \nstrategies, while farmers faced reduced export opportunities as retaliatory tariffs limi\u2010 \nted access to international markets. The long-term effects included accelerated efforts \nto diversify supply chains away from affected regions and renewed debates about the \neffectiveness of protectionist trade policies. <br><br>0.8 : The trade war caused significant disruptions to global supply chains, forcing many \ncompanies to reconsider their manufacturing strategies and sourcing policies. It led \nto increased tariffs on imported goods, higher prices for consumers, and economic \nuncertainty in affected industries. Several businesses reported decreased profits as they \nabsorbed additional costs or lost market share in foreign markets. The prolonged \ntension also contributed to volatility in financial markets and complicated diplomatic \nrelations between the involved nations. <br><br>As you can see, increasing the temperature tends to lead to more creative output. <br><br>In Chapter 3 and Chapter 6, you will see situations, like RAG and LLM-as-Judge, \nwhere it may be necessary to use low or even zero temperatures. <br><br>**Top-K Sampling**\nTop-K sampling restricts token selection to only the k most likely tokens from the \nvocabulary, effectively truncating the long tail of the probability distribution. This can \nhelp you avoid off-the-wall continuations at high temperatures. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f2e94b8-23e9-4719-bf82-ef47b9f432f9": {"__data__": {"id_": "2f2e94b8-23e9-4719-bf82-ef47b9f432f9", "embedding": null, "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.6, "x1": 432.0, "y1": 318.39}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3aa28892-7477-467f-8930-e60f8010b834", "node_type": "1", "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 326.49, "x1": 432.04, "y1": 605.76}]}, "hash": "3c05553cb09728524636aab39c79046302afdc20c665f0098e108043e0f5efc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "978b6792-24fd-4711-9b4a-44522485f0a0", "node_type": "1", "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 228.84, "x1": 432.01, "y1": 605.54}]}, "hash": "d3de4c7d88aadb6a46abdac97d0d51dc666816293910a545e078ee167ef42e86", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here\u2019s the impact of setting different top-K values to continue the phrase The space\u2010 \nship: <br><br>1 : The spaceship zoomed through the vast expanse of space, its powerful engines \nglowing blue against the darkness as it carried its crew toward distant stars and \nunknown adventures. <br><br>10 : The spaceship glided silently through the vast emptiness of space, its powerful \nengines propelling it toward the distant galaxy where no human had ventured before. <br><br>100 : The spaceship soared through the starry expanse, its gleaming hull reflecting the \ndistant light of alien suns as it carried its crew toward unexplored worlds beyond the \nedge of known space. <br><br>As you can see, when the top-K value is low, the generated text closely follows phrases \nthat you can find in existing science fiction. <br><br>**Nucleus Sampling**\nNucleus sampling dynamically selects the smallest set of tokens whose cumulative \nprobability exceeds a threshold p. Hence, it\u2019s also called top-P sampling. This creates a \n\u201cnucleus\u201d of tokens that represent the bulk of the probability mass. Figure 1-7 shows \nthe impact of applying varying top-P values to our illustrative distributions. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "978b6792-24fd-4711-9b4a-44522485f0a0": {"__data__": {"id_": "978b6792-24fd-4711-9b4a-44522485f0a0", "embedding": null, "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 228.84, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2f2e94b8-23e9-4719-bf82-ef47b9f432f9", "node_type": "1", "metadata": {"bbox": [{"page": 35, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.6, "x1": 432.0, "y1": 318.39}]}, "hash": "cca1b28836beccc8b03557ae57c9f206151fc0587e3ef02a2e1db8c69a1f1b87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d91315d-f452-41c1-b1c6-005c26c2cfea", "node_type": "1", "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.11, "x1": 432.0, "y1": 217.27}]}, "hash": "5005e6cb245c3dab09c45ea4e983b46115e8609941eda901d509b982ed556c21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Length penalties adjust the scoring of sequences based on their length. These penalties \ninclude the following: <br><br>Minimum length penalty <br><br>This forces the model to generate at least a certain number of tokens. <br><br>Maximum length penalty <br><br>This caps generation at a specified length, <br><br>Length normalization penalty <br><br>This divides sequence scores by a function of their length to prevent bias toward \nshorter sequences. <br><br>Beam search width is the number of parallel sequences considered during generation. \nYou will see beam search being used in patterns in Chapter 2 and Chapter 6. <br><br>At the time of writing, OpenAI and Gemini support repetition penalties but \nAnthropic does not. The Transformers library in Hugging Face supports length \npenalties and beam search width, but none of the hosted models do. <br><br>**In-Context Learning**\nIn \u201ctraditional\u201d ML, where models are predominantly trained for single problems like \nclassifications or regressions, models require extensive training to adapt their weights \nor parameters to a new task. But you can use LLMs for a wide variety of tasks by \nchanging just the prompt without changing the weights. <br><br>Foundational models\u2019 capability to adapt to new tasks based solely on examples or \ninstructions they\u2019re given in a prompt is called in-context learning. It emerges from \ntraining on diverse datasets that include various task formats and instructions. When \ngiven examples of a task within a prompt, the model identifies the underlying pattern \nand applies it to new instances. This process doesn\u2019t modify the model\u2019s weights, but \nrather, it leverages the model\u2019s existing knowledge to perform new tasks. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d91315d-f452-41c1-b1c6-005c26c2cfea": {"__data__": {"id_": "3d91315d-f452-41c1-b1c6-005c26c2cfea", "embedding": null, "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.11, "x1": 432.0, "y1": 217.27}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "978b6792-24fd-4711-9b4a-44522485f0a0", "node_type": "1", "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 228.84, "x1": 432.01, "y1": 605.54}]}, "hash": "d3de4c7d88aadb6a46abdac97d0d51dc666816293910a545e078ee167ef42e86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "311831c2-bb1b-4753-8106-55eab744b1d7", "node_type": "1", "metadata": {"bbox": [{"page": 38, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.91, "x1": 675.5, "y1": 294.48}]}, "hash": "f4b78ad94ec7fb604d408cccaa535a53fdfdedd4deac293458f541c6fc2fe86a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Zero-Shot Learning**\nZero-shot learning involves instructing a model to perform a task without providing \nany examples. This technique relies on the model\u2019s pretrained knowledge and its \nability to understand natural language instructions. <br><br>For example, you could ask Gemini-Flash to do the following: <br><br>Analyze the use of light in Claude Monet\u2019s*Impression, Sunrise*and explain how it \nexemplifies impressionist techniques. <br><br>It would respond with something like this (though your results may vary, since model \noutputs are nondeterministic): ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "311831c2-bb1b-4753-8106-55eab744b1d7": {"__data__": {"id_": "311831c2-bb1b-4753-8106-55eab744b1d7", "embedding": null, "metadata": {"bbox": [{"page": 38, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.91, "x1": 675.5, "y1": 294.48}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3d91315d-f452-41c1-b1c6-005c26c2cfea", "node_type": "1", "metadata": {"bbox": [{"page": 37, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.11, "x1": 432.0, "y1": 217.27}]}, "hash": "5005e6cb245c3dab09c45ea4e983b46115e8609941eda901d509b982ed556c21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "add545b1-01b7-4bc6-9536-a56a69a454ba", "node_type": "1", "metadata": {"bbox": [{"page": 39, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.49, "x1": 432.0, "y1": 384.2}]}, "hash": "d1c8ed9dfc294bd7b7bc18d7d8f8013bc0f4fa6fc8b22c8a7dca5fc9240e608f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "result = agent.run_sync(\"\"\" \nExample: \n``` \nDescription: shows two small rowboats in the foreground and a red Sun. \nAnswer: \n Painting: Impression, Sunrise \n Artist: Claude Monet \n Year: 1872 \n Significance: Gave the Impressionist movement its name; captured the fleeting effects of light and atmosphere, with loose brushstrokes. \n``` <br><br>Description: The painting shows a group of people eating at a table under an outside tent. The men are wearing boating hats. <br><br>\"\"\") <br><br>The result correctly identifies a Renoir painting that matches the description: <br><br>Answer: <br><br>Painting: Luncheon of the Boating Party <br><br>Artist: Pierre-Auguste Renoir <br><br>Year: 1881 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "add545b1-01b7-4bc6-9536-a56a69a454ba": {"__data__": {"id_": "add545b1-01b7-4bc6-9536-a56a69a454ba", "embedding": null, "metadata": {"bbox": [{"page": 39, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.49, "x1": 432.0, "y1": 384.2}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "311831c2-bb1b-4753-8106-55eab744b1d7", "node_type": "1", "metadata": {"bbox": [{"page": 38, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.91, "x1": 675.5, "y1": 294.48}]}, "hash": "f4b78ad94ec7fb604d408cccaa535a53fdfdedd4deac293458f541c6fc2fe86a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c7adbd5-f8e9-44e4-8088-a5327b3beb42", "node_type": "1", "metadata": {"bbox": [{"page": 40, "page_height": 661.5, "page_width": 504.0, "x0": 72.01, "y0": 70.04, "x1": 432.01, "y1": 256.94}]}, "hash": "2212255af97d9e252d5fc9a9393635f314305446b782aa15661b4d1d60c477fe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in-context learning has a few limitations: <br><br>\u2022 \n\u2022 It only works when the foundational model already has the necessary knowledge <br><br>and capability. <br><br>\u2022 \n\u2022 Adding many examples consumes valuable tokens of your model\u2019s context win\u2010 <br><br>dow and will slow your inference time. <br><br>\u2022 \n\u2022 LLMs sometimes struggle to generalize more complex problems based on a few <br><br>examples. <br><br>In these scenarios, post-training might offer a better approach. <br><br>**Post-Training**\nPost-training methods involve modifying the model weights of pretrained models \nto customize them to new tasks or domains. A post-trained model will have to be \ndeployed and utilized from a different end-point than the foundational model that \nwas its starting point. <br><br>**Post-Training methods**\nRecall from Figure 1-2 that training a foundational model involves multiple stages. \nThe first stage involves training a base LLM on predicting the next word, the second \nstage involves training the base LLM to perform tasks using SFT, and the third stage \ninvolves RLHF You can post-train the model in any or all of these ways. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c7adbd5-f8e9-44e4-8088-a5327b3beb42": {"__data__": {"id_": "6c7adbd5-f8e9-44e4-8088-a5327b3beb42", "embedding": null, "metadata": {"bbox": [{"page": 40, "page_height": 661.5, "page_width": 504.0, "x0": 72.01, "y0": 70.04, "x1": 432.01, "y1": 256.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "add545b1-01b7-4bc6-9536-a56a69a454ba", "node_type": "1", "metadata": {"bbox": [{"page": 39, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.49, "x1": 432.0, "y1": 384.2}]}, "hash": "d1c8ed9dfc294bd7b7bc18d7d8f8013bc0f4fa6fc8b22c8a7dca5fc9240e608f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8fe4d35-5ad2-4091-bfe3-c02e50a93ae4", "node_type": "1", "metadata": {"bbox": [{"page": 41, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.34, "x1": 432.01, "y1": 379.34}]}, "hash": "30220c220a2ff5f347f8062642cce2f8200c98e9463a2bf65cee4dae2709a662", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Parameter-efficient fine-tuning (PeFT) <br><br>Because foundational models are so large, it\u2019s unwieldy to train them further. \nSo, parameter-efficient fine-tuning (PeFT) approaches have emerged to make the \ntraining process more practical for large models. Low-rank adaptation (LoRA) \nrepresents weight updates using smaller matrices through low-rank decomposi\u2010 \ntion. Instead of fine-tuning all model parameters, LoRA freezes the original pre\u2010 \ntrained weights and adds small, trainable \u201cadapter\u201d matrices that are decomposed \ninto low-rank representations. LoRA drastically reduces the number of trainable \nparameters (making them up to 10,000 times fewer) and reduces GPU memory \nrequirements (making them up to 3 times fewer). It also does not add inference \nlatency, and it often performs on par with full fine-tuning. Quantization-aware \nlow-rank adaptation (QLoRA) is an extension of LoRA that quantizes all the \nweights of the model. Its training process is more memory efficient, albeit slower. \nOn the other hand, the quantized fine-tuned model takes up less space and is \ntherefore faster than LoRA. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8fe4d35-5ad2-4091-bfe3-c02e50a93ae4": {"__data__": {"id_": "e8fe4d35-5ad2-4091-bfe3-c02e50a93ae4", "embedding": null, "metadata": {"bbox": [{"page": 41, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.34, "x1": 432.01, "y1": 379.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6c7adbd5-f8e9-44e4-8088-a5327b3beb42", "node_type": "1", "metadata": {"bbox": [{"page": 40, "page_height": 661.5, "page_width": 504.0, "x0": 72.01, "y0": 70.04, "x1": 432.01, "y1": 256.94}]}, "hash": "2212255af97d9e252d5fc9a9393635f314305446b782aa15661b4d1d60c477fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6de5837-0630-40af-94f4-36e3dbf36a92", "node_type": "1", "metadata": {"bbox": [{"page": 42, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 505.5, "y1": 333.27}]}, "hash": "26e59c6f072c67550c270475840df24d642095fe46e16966f5ef7411631e20a8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You will encounter post-training in several patterns in this book, including Content \nOptimization (Pattern 5 in Chapter 2), Adapter Tuning (Pattern 15 in Chapter 5), and \nPrompt Optimization (Pattern 20 in Chapter 6). At the time of writing (June 2025), \nopen weights models are the only ones that support all the forms of post-training \nabove. If you are using a hosted model, please consult up-to-date documentation \nfrom your model provider to determine whether a pattern that requires post-training \nis possible (or change to a model that provides the needed capability). <br><br>**Fine-Tuning a Frontier Model**\nCompanies like OpenAI and Anthropic, as well as hyperscalers such as AWS and \nGoogle Cloud, have streamlined the process of post-training frontier models using \nSFT. It\u2019s possible to upload a training dataset of input-output pairs and launch the \nfine-tuning process, and the result will be an endpoint of an adapter-tuned model \nthat can be used just like the foundational model. <br><br>We\u2019ll illustrate this by using OpenAI\u2019s GPT series of models, but fine-tuning \nAnthropic Claude on Amazon Bedrock or Google\u2019s Gemini on Vertex AI is quite \nsimilar. Once you generate your training pairs (you\u2019ll need at least a hundred pairs, \nbut a couple of thousand pairs are even better), you\u2019ll need to store them in a JSON \nline-formatted file. You can then load the training dataset as follows: <br><br>training_file = client.files.create( \n file=open(\"training_data.jsonl\", \"rb\"), \n purpose=\"fine-tune\" \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6de5837-0630-40af-94f4-36e3dbf36a92": {"__data__": {"id_": "c6de5837-0630-40af-94f4-36e3dbf36a92", "embedding": null, "metadata": {"bbox": [{"page": 42, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 505.5, "y1": 333.27}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e8fe4d35-5ad2-4091-bfe3-c02e50a93ae4", "node_type": "1", "metadata": {"bbox": [{"page": 41, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.34, "x1": 432.01, "y1": 379.34}]}, "hash": "30220c220a2ff5f347f8062642cce2f8200c98e9463a2bf65cee4dae2709a662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a9c9a0d-4d95-4521-b061-31f090f6928b", "node_type": "1", "metadata": {"bbox": [{"page": 43, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.01, "y1": 298.94}]}, "hash": "9972fd7cbdad35b08b8956ae88f27b08a0612012728c7badce9273b0487c9d6c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fine-Tuning an Open-Weight Model**\nUnsloth.ai provides you with the capability to fine-tune and train open-weight LLMs \nlike Gemma and Llama. You can run Unsloth on your local hardware or use its \nmanaged fine-tuning services. <br><br>To fine-tune the 4-bit quantized version of Llama 3, start by loading in the model and \nits tokenizer (the class that will break an input text sequence into tokens of the sort \nthe model expects): <br><br>**from unsloth import**FastLanguageModel \nmax_seq_length = 2048 \nmodel, tokenizer = FastLanguageModel.from_pretrained( \n model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\", \n max_seq_length=max_seq_length, \n load_in_4bit=**True**, \n dtype=**None**, \n) <br><br>Then, attach a set of adapter weights to the base Llama model: <br><br>model = FastLanguageModel.get_peft_model( \n model, \n r=16, \n target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a9c9a0d-4d95-4521-b061-31f090f6928b": {"__data__": {"id_": "8a9c9a0d-4d95-4521-b061-31f090f6928b", "embedding": null, "metadata": {"bbox": [{"page": 43, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.01, "y1": 298.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c6de5837-0630-40af-94f4-36e3dbf36a92", "node_type": "1", "metadata": {"bbox": [{"page": 42, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 505.5, "y1": 333.27}]}, "hash": "26e59c6f072c67550c270475840df24d642095fe46e16966f5ef7411631e20a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f48875e-c3ce-4479-9994-9b31e199b07b", "node_type": "1", "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 228.24, "x1": 432.01, "y1": 364.74}]}, "hash": "035852dc3ed2637c04d81bb1794e3ed2d72460facb86f7be002be5bff2fa3fc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, you can use this model just like the base model. <br><br>**Considerations**\nFine-tuning models gives you the unique ability to customize LLMs to your domain- \nspecific use cases, but fine-tuned models come with additional complexities\u2014so \nmake sure that the benefits of fine-tuning are worth these additional headaches: <br><br>Data requirements <br><br>Instead of providing a few in-context examples, you must use a more significant \nnumber of samples (more than a hundred) to customize the range of tasks the \nLLM can handle. This will require you to collect the samples ahead of time before \nattempting to fine-tune your LLMs. If you don\u2019t have the time or resources right \nnow to do that, consider starting with in-context learning, collecting the data, \nand then fine-tuning your model later to boost its performance and provide \nmore consistent, reliable outputs. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f48875e-c3ce-4479-9994-9b31e199b07b": {"__data__": {"id_": "0f48875e-c3ce-4479-9994-9b31e199b07b", "embedding": null, "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 228.24, "x1": 432.01, "y1": 364.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8a9c9a0d-4d95-4521-b061-31f090f6928b", "node_type": "1", "metadata": {"bbox": [{"page": 43, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.01, "y1": 298.94}]}, "hash": "9972fd7cbdad35b08b8956ae88f27b08a0612012728c7badce9273b0487c9d6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be20e68-b0c0-4d6d-aa23-eeeff38d022d", "node_type": "1", "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 203.3}]}, "hash": "304bcb5a377bae95ee37a287cd672c5fe4ba4192a175e1994e6b6f3bdfdc48cd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additional costs <br><br>Fine-tuning models isn\u2019t free. Providers like OpenAI charge for the model train\u2010 \ning, often at a higher per-token rate than for requests to their standard models. \nThey charge a higher price because prompts to fine-tune models can be much \nshorter and can elicit the same or higher-quality output. Since providers calculate \nprices on input and output tokens, they increase token pricing to recover the \noverhead costs of hosting your fine-tuned model. \nOn the other hand, when you fine-tune open models, the models are free\u2014but \nyou must pay for the GPU costs while performing the fine-tuning procedures. \nDepending on the base LLM, this can start at a few dollars but quickly spiral to \nhundreds of dollars per model version. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3be20e68-b0c0-4d6d-aa23-eeeff38d022d": {"__data__": {"id_": "3be20e68-b0c0-4d6d-aa23-eeeff38d022d", "embedding": null, "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 203.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0f48875e-c3ce-4479-9994-9b31e199b07b", "node_type": "1", "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 228.24, "x1": 432.01, "y1": 364.74}]}, "hash": "035852dc3ed2637c04d81bb1794e3ed2d72460facb86f7be002be5bff2fa3fc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7b89987-82bd-46b3-9969-d6834f351ede", "node_type": "1", "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.84, "x1": 432.01, "y1": 448.34}]}, "hash": "831189668a7f4e08a60000e809bd14f8f166dfb307ca4e9c5e802a712754135a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The Organization of the Rest of the Book**\nThe rest of this book covers 32 design patterns, organized into eight chapters. You\u2019ll \nlearn how to control model outputs, enhance knowledge retrieval, improve reasoning \ncapabilities, increase reliability, enable action, optimize performance, and implement \nsafeguards. The section on each pattern includes a clear problem statement, a solu\u2010 \ntion approach, practical usage scenarios, and code examples. We hope that this book \nwill help you learn how to build robust and effective GenAI applications. <br><br>In Chapter 2, we show you how to control the style and format of AI-generated \ncontent\u2014which is a critical skill for ensuring brand consistency, accuracy, and com\u2010 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7b89987-82bd-46b3-9969-d6834f351ede": {"__data__": {"id_": "b7b89987-82bd-46b3-9969-d6834f351ede", "embedding": null, "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.84, "x1": 432.01, "y1": 448.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3be20e68-b0c0-4d6d-aa23-eeeff38d022d", "node_type": "1", "metadata": {"bbox": [{"page": 44, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 203.3}]}, "hash": "304bcb5a377bae95ee37a287cd672c5fe4ba4192a175e1994e6b6f3bdfdc48cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44b8cf19-76a2-45b1-90cb-c1c166e744fd", "node_type": "1", "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.64, "x1": 432.01, "y1": 240.74}]}, "hash": "2a82ba68416190e1234101aa457ce545a84d0204c39040c86d445e8a8ed1c8a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Chapter 3 and Chapter 4, you\u2019ll learn patterns that can help you build AI systems \nthat leverage external knowledge sources to address fundamental limitations like \nknowledge cutoffs, confidential data access, and hallucinations. You\u2019ll begin with \nBasic RAG (Pattern 6) and learn to ground AI responses in relevant information \nfrom knowledge bases. The section on Semantic Indexing (Pattern 7) will teach \nyou to capture meaning across different media types by using embeddings, thus \nmoving beyond simple keyword matching. With Indexing at Scale (Pattern 8), you\u2019ll \nmaster techniques for managing outdated or contradictory information through \nmetadata, filtering, and reranking. Index-Aware Retrieval (Pattern 9) will equip you \nwith advanced methods like hypothetical answers, query expansion, and GraphRAG \nto improve retrieval quality. Node Postprocessing (Pattern 10) will show you how to \nhandle irrelevant content and ambiguous entities through reranking and contextual \ncompression. You\u2019ll learn to build Trustworthy Generation (Pattern 11) systems that \nmaintain user trust despite inevitable errors, and finally, the section on Deep Search \n(Pattern 12) will teach you iterative processes for complex information retrieval that \novercome context window constraints and enable multihop reasoning. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44b8cf19-76a2-45b1-90cb-c1c166e744fd": {"__data__": {"id_": "44b8cf19-76a2-45b1-90cb-c1c166e744fd", "embedding": null, "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.64, "x1": 432.01, "y1": 240.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b7b89987-82bd-46b3-9969-d6834f351ede", "node_type": "1", "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.84, "x1": 432.01, "y1": 448.34}]}, "hash": "831189668a7f4e08a60000e809bd14f8f166dfb307ca4e9c5e802a712754135a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c143b398-b016-446d-99f6-79257491e67d", "node_type": "1", "metadata": {"bbox": [{"page": 46, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.04, "x1": 432.01, "y1": 247.34}]}, "hash": "ef373df9516fe717baf53402193b0896842f0bdeecf2234897080b83d2678858", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Chapter 5, we discuss powerful techniques to enhance the reasoning and special\u2010 \nized capabilities of language models. You\u2019ll learn Chain of Thought (CoT) (Pattern \n13), which enables models to break down complex problems into intermediate rea\u2010 \nsoning steps and dramatically improve their performance on mathematical problems \nand logical deductions. The section on Tree of Thoughts (ToT) (Pattern 14) will \nteach you to implement tree search approaches for problems requiring exploration \nof multiple solution paths\u2014which are ideal for strategic thinking and planning tasks. \nWith Adapter Tuning (Pattern 15), you\u2019ll discover how to efficiently specialize large \nmodels by training small add-on neural network layers while keeping original model \nweights frozen, thus making specialized adaptation practical with limited data (from \n100 to 10,000 examples). Finally, the section on Evol-Instruct (Pattern 16) will show \nyou how to efficiently generate high-quality instruction-tuning datasets by evolving ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c143b398-b016-446d-99f6-79257491e67d": {"__data__": {"id_": "c143b398-b016-446d-99f6-79257491e67d", "embedding": null, "metadata": {"bbox": [{"page": 46, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.04, "x1": 432.01, "y1": 247.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "44b8cf19-76a2-45b1-90cb-c1c166e744fd", "node_type": "1", "metadata": {"bbox": [{"page": 45, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.64, "x1": 432.01, "y1": 240.74}]}, "hash": "2a82ba68416190e1234101aa457ce545a84d0204c39040c86d445e8a8ed1c8a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afafda04-73d3-479a-9eb0-fe492c698ecb", "node_type": "1", "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 210.51, "x1": 432.01, "y1": 582.33}]}, "hash": "1b7ca3d92867e95a59bf63d8b5822e06fc4fe806be348c18803df826ebf2d2f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Chapter 8, you\u2019ll learn essential patterns for deploying generative AI within \nreal-world constraints of cost, latency, and computational resources. The section \non Small Language Model (SLM) (Pattern 24) will demonstrate how to leverage \nsmaller, more efficient models that can run on edge devices or with limited resources \nwhile still delivering acceptable performance for specific tasks. With Prompt Caching \n(Pattern 25), you\u2019ll discover techniques to reduce redundant computations and API \ncalls and thus significantly lower costs for frequently requested content. Inference \nOptimization (Pattern 26) will equip you with methods to maximize throughput and \nminimize latency through techniques like speculative decoding, continuous batching, \nand prompt compression. The section on Degradation Testing (Pattern 27) will \nshow you how to systematically evaluate model performance across different deploy\u2010 \nment scenarios and thus ensure consistent quality over time. Finally, the Long-Term \nMemory (Pattern 28) section will demonstrate how to maintain user history and \ndynamically apply personalization. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afafda04-73d3-479a-9eb0-fe492c698ecb": {"__data__": {"id_": "afafda04-73d3-479a-9eb0-fe492c698ecb", "embedding": null, "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 210.51, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c143b398-b016-446d-99f6-79257491e67d", "node_type": "1", "metadata": {"bbox": [{"page": 46, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.04, "x1": 432.01, "y1": 247.34}]}, "hash": "ef373df9516fe717baf53402193b0896842f0bdeecf2234897080b83d2678858", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaa31981-0425-4c35-8808-e7c78fb7e156", "node_type": "1", "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.67, "x1": 422.35, "y1": 196.0}]}, "hash": "ee96bd4bba9da56447b207a3ae664f217f88fc6335144d5a3809c8d07b22fe98", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 2**\n**Controlling Content Style**<br><br>The patterns in this chapter all have to do with controlling the style of the content \n(such as text, images, and video) generated by foundational models, which are models \nthat have been trained on large datasets and are capable of generating a wide variety \nof content. However, the style of that content will, by default, be based on the training \nprocess that the model provider has used. Even if you stick to a single model version, \nmodel responses are stochastic\u2014which means you may get different responses even \nif you repeat a question exactly (assuming that there is no caching going on). This \nmeans that downstream applications and end users that use responses from a GenAI \nmodel will have to deal with quite a wide variety of possible styles. <br><br>For example, we asked a number of foundational models the same question: <br><br>What\u2019s a good side dish for pierogi? Answer in a single sentence. <br><br>We then recorded the answers (see Table 2-1). As you can see, even when we restrict \nthe answer to a single sentence, different models can answer the same question \nvery differently. Imagine how much more diverse the results would be had we not \nrestricted the style of the response in any way! ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aaa31981-0425-4c35-8808-e7c78fb7e156": {"__data__": {"id_": "aaa31981-0425-4c35-8808-e7c78fb7e156", "embedding": null, "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.67, "x1": 422.35, "y1": 196.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "afafda04-73d3-479a-9eb0-fe492c698ecb", "node_type": "1", "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 210.51, "x1": 432.01, "y1": 582.33}]}, "hash": "1b7ca3d92867e95a59bf63d8b5822e06fc4fe806be348c18803df826ebf2d2f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a19b8fd-0cd0-4704-bba0-f33ff9aa03d2", "node_type": "1", "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.79, "x1": 432.0, "y1": 604.6}]}, "hash": "fea8df80b084d2826f946a06fda870f338486bc4fb1e403913f9db70d16617e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2-1. Answers to the same question in different styles (answers retrieved from the \nmodels in February 2025) <br><br>**Model**<br><br>**Model**\n**provider**\nOpenAI <br><br>**Answer**<br><br>GPT-4 <br><br>A great side dish for pierogi is saut\u00e9ed onions with butter and a sprinkle of crispy bacon \nbits. \nA tangy sauerkraut or caramelized onions complement pierogies perfectly by adding \ncontrasting acidity or sweetness to the dumplings. \nSaut\u00e9ed onions and mushrooms are a classic and delicious side dish for pierogi. <br><br>Claude Sonnet 3.5 Anthropic <br><br>Gemini 2.0 Flash  Google ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a19b8fd-0cd0-4704-bba0-f33ff9aa03d2": {"__data__": {"id_": "4a19b8fd-0cd0-4704-bba0-f33ff9aa03d2", "embedding": null, "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.79, "x1": 432.0, "y1": 604.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "aaa31981-0425-4c35-8808-e7c78fb7e156", "node_type": "1", "metadata": {"bbox": [{"page": 48, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.67, "x1": 422.35, "y1": 196.0}]}, "hash": "ee96bd4bba9da56447b207a3ae664f217f88fc6335144d5a3809c8d07b22fe98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "133e24ca-61da-4334-acf9-2563bbcbc4f8", "node_type": "1", "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.18, "x1": 432.0, "y1": 266.45}]}, "hash": "96ca8a1ed351c46885850d839f99556b60f463835941b95dec6bb5b9d8faf69a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Model**<br><br>**Model**\n**provider**\nMeta <br><br>**Answer**<br><br>Llama 3.2 70B <br><br>A traditional and delicious side dish for pierogi is fried onions and sour cream, but other \npopular options include saut\u00e9ed spinach, braised red cabbage, or a simple green salad \nwith a light vinaigrette. \nA tangy sauerkraut salad or caramelized roasted carrots with dill make excellent, flavorful \nsides for pierogi. \nA classic side dish for pierogi is coleslaw, especially when garlic and herbs are added. <br><br>DeepSeek R1  DeepSeek <br><br>Mistral Small 24B Mistral AI <br><br>How can you control (or restrict) the style of the response? Naturally, the answer \ndepends on your goals\u2014do you want to control the tone, the vocabulary, the reading \nlevel, or the formatting? You can try to control any of these aspects of style by \nusing prompt engineering, but such an approach is extremely brittle\u2014the results will \nvary from model to model and from one attempt to another. The patterns in this \nchapter provide a variety of more sophisticated and robust solutions to the problem \nof controlling style, so either choose the one that best meets your needs or combine \nthe patterns. <br><br>Logits Masking (Pattern 1) ensures that generated text conforms to a set of rules. \nGrammar (Pattern 2) ensures that generated text conforms to a user-specified schema \nor standard data format\u2014which is like Logits Masking, but it\u2019s carried out server-side \nby the model provider. Style Transfer (Pattern 3) uses example translations to ensure \nthat text or generated images have the desired characteristics of some reference \ncontent. Reverse Neutralization (Pattern 4) provides a way to perform style transfer \nwhen only the reference content is available. Finally, Content Optimization (Pattern \n5) is a way to choose whichever style performs best, without having to identify what \nthe factors of that style are. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "133e24ca-61da-4334-acf9-2563bbcbc4f8": {"__data__": {"id_": "133e24ca-61da-4334-acf9-2563bbcbc4f8", "embedding": null, "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.18, "x1": 432.0, "y1": 266.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4a19b8fd-0cd0-4704-bba0-f33ff9aa03d2", "node_type": "1", "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.79, "x1": 432.0, "y1": 604.6}]}, "hash": "fea8df80b084d2826f946a06fda870f338486bc4fb1e403913f9db70d16617e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "514e4efb-b681-4a00-95fb-db3fe73e3280", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 452.13, "x1": 423.75, "y1": 597.62}]}, "hash": "58143293526db25a235320d6bf47e3b4bc0e94a693882e11c611854ac6099d31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Pattern 1: Logits Masking**\nLogits Masking provides a way for application clients of a foundational model to \nensure that the text the model generates conforms to a set of rules. These rules \nare often static, but in some cases, they can change based on the content already \ngenerated. <br><br>**Problem**\nSometimes, when you\u2019re generating text using LLMs, you want it to conform to \nspecific style rules. These rules might be in place for branding, accuracy, compliance, \nor other reasons. <br><br>Here are some illustrative rules to help you understand the kinds of problems that \nthis pattern tries to solve: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "514e4efb-b681-4a00-95fb-db3fe73e3280": {"__data__": {"id_": "514e4efb-b681-4a00-95fb-db3fe73e3280", "embedding": null, "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 452.13, "x1": 423.75, "y1": 597.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "133e24ca-61da-4334-acf9-2563bbcbc4f8", "node_type": "1", "metadata": {"bbox": [{"page": 49, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.18, "x1": 432.0, "y1": 266.45}]}, "hash": "96ca8a1ed351c46885850d839f99556b60f463835941b95dec6bb5b9d8faf69a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a96e250b-11ab-46e3-9504-25433a7b2078", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 89.35, "y0": 282.13, "x1": 423.75, "y1": 440.13}]}, "hash": "c3b6073787489bb40502ad3d01670aef44b163ce7f234a3d477de66b951ad244", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**When Is the Try-and-Try-Again Approach Acceptable?**\nIf content has to be generated 3.2 times on average, then the typical latency will be \n3.2 times that of a single call. The tail latency, which is the latency of the slowest \nrequests, will be much greater because there is no guarantee that the approach will \never converge. Also, reducing the number of attempts allowed will increase the refusal \nrate of your application. <br><br>Whether or not an attempt is successful is a binary outcome, and you need to try \nonly until the first success. Because the number of attempts needed in such a process \nfollows a geometric distribution, you can calculate the average and tail latency. If only \np% of your generations succeed, then you can estimate the average and tail latency of \nthe try-and-try-again approach as follows: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a96e250b-11ab-46e3-9504-25433a7b2078": {"__data__": {"id_": "a96e250b-11ab-46e3-9504-25433a7b2078", "embedding": null, "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 89.35, "y0": 282.13, "x1": 423.75, "y1": 440.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "514e4efb-b681-4a00-95fb-db3fe73e3280", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 452.13, "x1": 423.75, "y1": 597.62}]}, "hash": "58143293526db25a235320d6bf47e3b4bc0e94a693882e11c611854ac6099d31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "739d6db3-2418-440c-9bcb-292c80dd6b8c", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.28, "x1": 432.0, "y1": 224.79}]}, "hash": "2ccf80a9ba88f272bb22e26607ed49b20116a9b648f88481121c891746e29086", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The average number of generations required before you get a successful one is \n\u2022 \ngiven by this formula: 100/p. For example, if 10% of your generations fail, then \n90% succeed\u2014so you will need 100/90, or 1.1, attempts on average, and the \naverage latency goes up by 10%. On the other hand, if 70% of your generations \nfail, only 30% succeed\u2014so you will need 100/30, or 3.3, attempts on average, and \nthe average latency goes up by 230%. If you use an exponential backoff method \nthat increases the time between attempts to avoid overloading the server, then the \nlatency increase will be even higher. <br><br>\u2022 There\u2019s no closed formula for the 99th percentile of the latency. Instead, you have \n\u2022 \nto estimate it numerically from the geometrical distribution. You can use the \nonline calculator at eStat to look up the correct value. For a 90% success rate, \nplug in 0.9 for the value of p and look for 0.99 in P(X <= x). The 99th percentile \nof the number of attempts required is 2 if p = 90 and 13 if p = 30! ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "739d6db3-2418-440c-9bcb-292c80dd6b8c": {"__data__": {"id_": "739d6db3-2418-440c-9bcb-292c80dd6b8c", "embedding": null, "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.28, "x1": 432.0, "y1": 224.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a96e250b-11ab-46e3-9504-25433a7b2078", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 89.35, "y0": 282.13, "x1": 423.75, "y1": 440.13}]}, "hash": "c3b6073787489bb40502ad3d01670aef44b163ce7f234a3d477de66b951ad244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4c8b5c8-3a56-459f-b797-e5999a6101e6", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 569.84, "x1": 432.0, "y1": 605.54}]}, "hash": "9d9c3512b4da5f426d9904157b1ec92263bc6609d3f9ec2b60d5af6bea9787dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A better approach to making generated text conform to a set of rules is to use Logits \nMasking. <br><br>**Solution**\nAs discussed in the section on beam search in Chapter 1, foundational models \ngenerate text by sampling from a sequence of possible continuations. The idea behind \nLogits Masking is to intercept the generation at this sampling stage. <br><br>Logits Masking works as follows: <br><br>\u2022 \n\u2022 Rather than wait until the full content is generated, you obtain the set of possible <br><br>continuations at each intermediate point. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4c8b5c8-3a56-459f-b797-e5999a6101e6": {"__data__": {"id_": "f4c8b5c8-3a56-459f-b797-e5999a6101e6", "embedding": null, "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 569.84, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "739d6db3-2418-440c-9bcb-292c80dd6b8c", "node_type": "1", "metadata": {"bbox": [{"page": 51, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.28, "x1": 432.0, "y1": 224.79}]}, "hash": "2ccf80a9ba88f272bb22e26607ed49b20116a9b648f88481121c891746e29086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e948882-d6f9-4b11-b39b-5a7783b720ad", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 314.98, "x1": 559.75, "y1": 549.82}]}, "hash": "47938c87601151ee2b78f1eee7d877bf6ed44bc80b1723c9b868062ece94a76f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To demonstrate the previously listed steps, we\u2019ll use the Transformers library. We\u2019ll \nshow only the relevant code snippets; the full code for this pattern is in the 01_log\u2010 \nits_masking folder of the GitHub repository for this book. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e948882-d6f9-4b11-b39b-5a7783b720ad": {"__data__": {"id_": "3e948882-d6f9-4b11-b39b-5a7783b720ad", "embedding": null, "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 314.98, "x1": 559.75, "y1": 549.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f4c8b5c8-3a56-459f-b797-e5999a6101e6", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 569.84, "x1": 432.0, "y1": 605.54}]}, "hash": "9d9c3512b4da5f426d9904157b1ec92263bc6609d3f9ec2b60d5af6bea9787dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52bdd0ee-8bad-4ffc-8280-2c3189fdf2e1", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 88.18, "x1": 432.0, "y1": 290.93}]}, "hash": "a4e9f2b41a0de2e792edc3954b38f1843a0a5c154d26f7f77a31284a1981e2a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The Transformers Library**\nThe Transformers library from Hugging Face simplifies working with pretrained \ntransformer-based foundational models, regardless of the ML framework (Tensor\u2010 \nflow, PyTorch, or JAX) that was used to train the foundational model. It offers \nsupport for a wide range of tasks such as text classification, question answering, sum\u2010 \nmarization, translation, and text generation. For example, to do sentiment analysis \nusing Llama, you could use the following: <br><br>**from transformers import**pipeline \nMODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\" \nclassifier = pipeline('sentiment-analysis', MODEL_ID) \nresult = classifier('I am learning a lot of neat concepts   from the GenAI design patterns book.  ') <br><br>Through Hugging Face, the library also provides access to thousands of pretrained \nmodels and simplifies the process of fine-tuning these models for specific down\u2010 \nstream tasks and deploying them. <br><br>We are using Transformers to illustrate Logits Masking only so that we have a \nconcrete implementation with which to illustrate the steps needed and discuss the \ncontrolling parameters. You can carry out these steps by using other LLM frameworks \nor even the model providers\u2019 client libraries, although the details will vary. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52bdd0ee-8bad-4ffc-8280-2c3189fdf2e1": {"__data__": {"id_": "52bdd0ee-8bad-4ffc-8280-2c3189fdf2e1", "embedding": null, "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 88.18, "x1": 432.0, "y1": 290.93}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3e948882-d6f9-4b11-b39b-5a7783b720ad", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 314.98, "x1": 559.75, "y1": 549.82}]}, "hash": "47938c87601151ee2b78f1eee7d877bf6ed44bc80b1723c9b868062ece94a76f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d159a17-b922-4b01-bd3c-02a4bc759508", "node_type": "1", "metadata": {"bbox": [{"page": 55, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.09, "x1": 432.0, "y1": 251.87}]}, "hash": "d6773fec007587e79bdc02c7cb33ba51934973584107c7d03baab7520ac660b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 1: Intercepting sampling**<br><br>To intercept sampling and get access to the probability of each continuation \nsequence, you need to create a LogitsProcessor subclass, which is part of Transfor\u2010 \nmers\u2019 generation library. In the LogitsProcessor subclass, you initialize the token\u2010 \nizer and any parameters you need to apply your rules as follows: <br><br>**class MyRulesLogitsProcessor**(LogitsProcessor): \n**def** __init__(self, tokenizer, rules): \n self.tokenizer = tokenizer \n self.rules = rules <br><br>Once you\u2019ve created the logits processor, you have to pass it in when you invoke the \nTransformer pipeline. To do so, you create a text generation pipeline as normal: <br><br>**from transformers import**pipeline \npipe = pipeline( \n task=\"text-generation\", \n model=MODEL_ID, \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d159a17-b922-4b01-bd3c-02a4bc759508": {"__data__": {"id_": "3d159a17-b922-4b01-bd3c-02a4bc759508", "embedding": null, "metadata": {"bbox": [{"page": 55, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.09, "x1": 432.0, "y1": 251.87}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "52bdd0ee-8bad-4ffc-8280-2c3189fdf2e1", "node_type": "1", "metadata": {"bbox": [{"page": 54, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 88.18, "x1": 432.0, "y1": 290.93}]}, "hash": "a4e9f2b41a0de2e792edc3954b38f1843a0a5c154d26f7f77a31284a1981e2a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80c9ae6a-1e57-43bc-a3de-6243aeb0d70b", "node_type": "1", "metadata": {"bbox": [{"page": 56, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.99, "x1": 432.01, "y1": 313.07}]}, "hash": "af1f2caa48bb6f01f111788bfbc40de220d2c02f406a232c83ead6b525a35739", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, because logits are the log of the probability, zeroing out the probability of a \nsequence means setting the logits to negative infinity. Do this by using the following: <br><br>output_logits[idx] = -np.inf <br><br>Putting this together, the implementation of the__call__method will be as follows: <br><br>@add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) \n**def** __call__( \n self, input_ids: torch.LongTensor, input_logits: torch.FloatTensor \n ) -> torch.FloatTensor: \n output_logits = input_logits.clone() \n**for**idx, input_id**in**enumerate(input_ids): \n seq = self.tokenizer.**decode**(input_id) \n**if not**self.apply_rules(seq, self.rules): \n output_logits[idx] =**-np.inf***# zero out*\n**return**output_logits ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80c9ae6a-1e57-43bc-a3de-6243aeb0d70b": {"__data__": {"id_": "80c9ae6a-1e57-43bc-a3de-6243aeb0d70b", "embedding": null, "metadata": {"bbox": [{"page": 56, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.99, "x1": 432.01, "y1": 313.07}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3d159a17-b922-4b01-bd3c-02a4bc759508", "node_type": "1", "metadata": {"bbox": [{"page": 55, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.09, "x1": 432.0, "y1": 251.87}]}, "hash": "d6773fec007587e79bdc02c7cb33ba51934973584107c7d03baab7520ac660b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d367847e-0d01-46c8-bd7d-954a58449ab1", "node_type": "1", "metadata": {"bbox": [{"page": 57, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 432.0, "y1": 361.99}]}, "hash": "9e30e41a6cadfbeb30248cdca89255c32a8d31f65ad96d3b67551c66d227ee6a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The code above appends previously generated text to the original prompt, tokenizes it \ninto IDs, and sends the sequence of IDs as a PyTorch tensor (return_tensors=\"pt\") \nthat\u2019s ready for GPU computations (to(\"cuda\")) by the model. <br><br>A typical loop around generate() will involve logic to (re)initialize the generation, \napply the rules, backtrack and remove previously generated sequences, or stop the \ngeneration. You\u2019ll want to maintain the necessary state variables\u2014and the reasons for \nthis will become clearer when we examine concrete examples in the next section. <br><br>You\u2019ll also need some logic to stop the generation. A common approach is to stop \nthe generation when the model outputs a stop string. The input prompt could include \nan instruction to output the stop string, or the stop string could be part of examples \nprovided in the context. This stop string can also be passed into the generate() call \nto do an early stop while generating a sequence (for example, before 16 new tokens \nare generated). <br><br>**Examples**\nTo illustrate Logits Masking, we\u2019ll use a couple of examples. The first is a simple \nproblem in which you can simply select a continuation sequence (Steps 1\u20133 in the ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d367847e-0d01-46c8-bd7d-954a58449ab1": {"__data__": {"id_": "d367847e-0d01-46c8-bd7d-954a58449ab1", "embedding": null, "metadata": {"bbox": [{"page": 57, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 432.0, "y1": 361.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "80c9ae6a-1e57-43bc-a3de-6243aeb0d70b", "node_type": "1", "metadata": {"bbox": [{"page": 56, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.99, "x1": 432.01, "y1": 313.07}]}, "hash": "af1f2caa48bb6f01f111788bfbc40de220d2c02f406a232c83ead6b525a35739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12b62324-cb46-4bf8-9114-93ac641e9516", "node_type": "1", "metadata": {"bbox": [{"page": 58, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.06, "x1": 432.0, "y1": 330.06}]}, "hash": "2332ff3e2cbf5a2b0446b8abaa0bab055dd6c6df0d85f9140e1b4edf8ffc1e44", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The result, though, does not meet our requirements: <br><br>Introducing PowerBoost, a delicious and convenient protein drink that helps you \nfuel your active lifestyle. With 20 grams of protein and 0 g of sugar, this refreshing \nbeverage supports muscle growth and recovery after your toughest workouts. Made \nwith high-quality whey protein and essential vitamins, PowerBoost is the perfect way \nto recharge and refuel on the go. <br><br>It does include two good SEO terms\u2014whey and whey protein\u2014but unfortunately, it \nalso includes three words that increase the odds of being banned: quality, growth, \nand perfect. Sure, high-quality and muscle growth seem innocuous, but why take the \nchance? <br><br>**Using Logits Masking.**Now, try using Logits Masking to choose continuations that \nhave the maximum number of positive words and the least number of negative \nwords. First, write an evaluation function that counts the occurrences: <br><br>**def**evaluate(descr: str, positives, negatives) -> int: \n descr = descr.lower() \n num_positive = np.sum([1**for**phrase**in**positives**if**phrase**in**descr]) \n num_negative = np.sum([1**for**phrase**in**negatives**if**phrase**in**descr]) \n**return**int(num_positive - num_negative) <br><br>To do this, write a subclass of LogitsProcessor and select the continuation sequence \nthat has the characteristics you desire: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12b62324-cb46-4bf8-9114-93ac641e9516": {"__data__": {"id_": "12b62324-cb46-4bf8-9114-93ac641e9516", "embedding": null, "metadata": {"bbox": [{"page": 58, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.06, "x1": 432.0, "y1": 330.06}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d367847e-0d01-46c8-bd7d-954a58449ab1", "node_type": "1", "metadata": {"bbox": [{"page": 57, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 432.0, "y1": 361.99}]}, "hash": "9e30e41a6cadfbeb30248cdca89255c32a8d31f65ad96d3b67551c66d227ee6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41b9054a-a4a1-4801-93a3-7141934393be", "node_type": "1", "metadata": {"bbox": [{"page": 59, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.39, "x1": 1189.75, "y1": 280.14}]}, "hash": "a6650b60839a760806788897fc6a60baf0ea0c385d3aadbcce37b9a4fd8e1df8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fuel your active lifestyle with our premium protein drink, packed with 20 grams \nof whey protein, 10 grams of branched-chain amino acids (BCAAs), and essential \nvitamins and minerals to support muscle recovery and overall well-being. Our unique \nblend of whey protein isolate and micellar casein provides a sustained release of \nnutrients, helping to build and repair muscle tissue. With no artificial flavors or \nsweeteners, our protein drink is a guilt-free way to support your fitness goals. Enjoy \nthe taste of a refreshing beverage while nourishing your body with the nutrients it \nneeds to thrive. <br><br>Note the use of premium to get around the problem with quality. We\u2019ll always get a \nproduct description, and the description that we get is the best possible one we could \nhave gotten, given the set of potential continuation sequences. Step 3 is automatically \nvalid the way we are doing this because we will always have some sequence\u2014even if it \nhas no positive words. <br><br>**Sequence regeneration**<br><br>Now, let\u2019s look at a more complex problem where zero/few-shot learning simply \ndoesn\u2019t work and is unlikely to work in the absence of a powerful reasoning model \nthat\u2019s capable of backtracking and correcting its work. The full code for this section is \nin the sequence_regeneration notebook on GitHub. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41b9054a-a4a1-4801-93a3-7141934393be": {"__data__": {"id_": "41b9054a-a4a1-4801-93a3-7141934393be", "embedding": null, "metadata": {"bbox": [{"page": 59, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.39, "x1": 1189.75, "y1": 280.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "12b62324-cb46-4bf8-9114-93ac641e9516", "node_type": "1", "metadata": {"bbox": [{"page": 58, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.06, "x1": 432.0, "y1": 330.06}]}, "hash": "2332ff3e2cbf5a2b0446b8abaa0bab055dd6c6df0d85f9140e1b4edf8ffc1e44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23f6f294-2aaa-4a36-81a8-242e492a10fd", "node_type": "1", "metadata": {"bbox": [{"page": 60, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 509.75, "y1": 299.39}]}, "hash": "08b43c7c460b61bac5adc93e7902f0b60de58a62d5f91526b88ede5b8c5ea338", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model probably tried to generate POWER as the starting letters for the lines of \nthe poem but failed and instead came up with PORE. There are several ways to fix \nthis. Reflection (Pattern 18 in Chapter 6) might work, for example. So would using \na more powerful reasoning model and giving it \u201cthinking tokens.\u201d Here, though, let\u2019s \nsee how to use Logits Masking to solve this with a smaller model. <br><br>**Initializing the poem.**To initialize the acrostic, we\u2019ll constrain the starts of lines explic\u2010 \nitly. To do so, we\u2019ll generate a list of adjectives for the animal by using the following \nprompt: <br><br>system_prompt=f\"\"\" You are an expert on words who has access to a thesaurus. Respond with a list of adjectives that could complete the phrase \"As___as a {animal}\" For example, for a rabbit, you could respond with these: quick, fast, gentle, playful \nRespond with just a list of words without any introduction or preamble. \n\"\"\" \nuser_prompt=f\"Give me the best {num_words*3} adjectives that would complete the phrase 'As___as a {animal}'\" <br><br>The resulting list of adjectives for tiger is this: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23f6f294-2aaa-4a36-81a8-242e492a10fd": {"__data__": {"id_": "23f6f294-2aaa-4a36-81a8-242e492a10fd", "embedding": null, "metadata": {"bbox": [{"page": 60, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 509.75, "y1": 299.39}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "41b9054a-a4a1-4801-93a3-7141934393be", "node_type": "1", "metadata": {"bbox": [{"page": 59, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.39, "x1": 1189.75, "y1": 280.14}]}, "hash": "a6650b60839a760806788897fc6a60baf0ea0c385d3aadbcce37b9a4fd8e1df8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a70cf9c-66b4-4ed5-b2e9-33c6d38eee1c", "node_type": "1", "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.7, "x1": 548.0, "y1": 508.79}]}, "hash": "cd16d54eeb303114dca82824c34f1debb718b6e4416c150d33d72338d1cb70f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " start_word = random.choices(population=allowed_start_words, weights=weights, k=1)[0].lower() \n start_letter = start_word[0].upper() \n**return**start_word, [get_phrase_that_starts_with(animal, start_letter)] <br><br>This method returns the acrostic word and the starting point for a poem that starts \nwith that word. So, for tiger, we might get the following: <br><br>('wild', ['Wrapped in warm,']) <br><br>**Generating a poem.**We start by initializing the poem and creating state variables to \nstore the poem we\u2019ve generated so far in this iteration: <br><br>**def**write_acrostic(animal: str, max_iter=10, num_sequences_per_iter=10): \n allowed_start_words = get_potential_starts(animal, 10) \n start_word, poem_so_far, prev_start_poem =**None**,**None**,**None**\n**for**iter**in**range(max_iter): \n*# reinitialize if we are stuck at a starting point*\n**if**poem_so_far**is None or**poem_so_far == prev_start_poem: \n start_word, poem_so_far = initialize_poem(animal, allowed_start_words)*# lines of poem*\n prev_start_poem = poem_so_far*# for next iter*", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a70cf9c-66b4-4ed5-b2e9-33c6d38eee1c": {"__data__": {"id_": "0a70cf9c-66b4-4ed5-b2e9-33c6d38eee1c", "embedding": null, "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.7, "x1": 548.0, "y1": 508.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "23f6f294-2aaa-4a36-81a8-242e492a10fd", "node_type": "1", "metadata": {"bbox": [{"page": 60, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.64, "x1": 509.75, "y1": 299.39}]}, "hash": "08b43c7c460b61bac5adc93e7902f0b60de58a62d5f91526b88ede5b8c5ea338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3ab1db0-d66c-40d8-850c-b532bfddfb24", "node_type": "1", "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.8, "x1": 432.0, "y1": 308.89}]}, "hash": "4cc00aa882b65f58e569f3fe70ff9b0d6e5fca6194ec40d51e4e3679bfa97090", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Regeneration logic.**If, on the other hand, all the continuation sequences have been \nzeroed out, we backtrack by removing generated lines one at a time (starting at the \nend) until there is a sequence that meets the acrostic requirements. We can use the \npoem fragment that starts with \u201cBrightening up the forest floor\u201d as the starting point \nfor the next call to generate(). <br><br>*# remove the lines that don't fit and try again*\n**while True**: \n*# remove a line, and see if it matches the start word*\n best_bad_poem_in_iter = best_bad_poem_in_iter[:-1] \n**if**len(best_bad_poem_in_iter) == 0: \n*# reinitialize, potentially to different start word*\n start_word, poem_so_far = initialize_poem(animal, allowed_start_words) \n**break**\n candidate_starts = ''.join([line[0]**for**line**in**best_bad_poem_in_iter]).lower() \n**if**len(start_word) >= len(candidate_starts)**and**start_word[:len(candidate_starts)] == candidate_starts: \n poem_so_far = best_bad_poem_in_iter*# start from here, for same start_word*\n**break**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3ab1db0-d66c-40d8-850c-b532bfddfb24": {"__data__": {"id_": "c3ab1db0-d66c-40d8-850c-b532bfddfb24", "embedding": null, "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.8, "x1": 432.0, "y1": 308.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0a70cf9c-66b4-4ed5-b2e9-33c6d38eee1c", "node_type": "1", "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.7, "x1": 548.0, "y1": 508.79}]}, "hash": "cd16d54eeb303114dca82824c34f1debb718b6e4416c150d33d72338d1cb70f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc461b7f-401a-4503-9046-075f1b13c800", "node_type": "1", "metadata": {"bbox": [{"page": 64, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.34, "x1": 432.0, "y1": 320.44}]}, "hash": "016ad03d31d1963a1bd5f4920e030fb26c0dc497c4054d64ee48753205d1bfa9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you end up stuck on a starting point, then reinitialize the poem (potentially with a \nnew adjective for the animal) and continue the process. <br><br>**Example output.**For tiger, the process generates acrostic poems whose lines start with \nletters spelling out the words BOLD and SWIFT. Here\u2019s the first poem we get: <br><br>Boldly, the brave tiger stalks its prey <br><br>Owning the forest with its might, <br><br>Lurking in the shadows, waiting to pounce, <br><br>Daring to be the king of the night <br><br>And here\u2019s the second: <br><br>Smiling softly, the <br><br>Wild eyes gleam <br><br>In the <br><br>Forest depths, <br><br>Tigers stalk <br><br>For an owl, we get a poem such as this: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc461b7f-401a-4503-9046-075f1b13c800": {"__data__": {"id_": "cc461b7f-401a-4503-9046-075f1b13c800", "embedding": null, "metadata": {"bbox": [{"page": 64, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.34, "x1": 432.0, "y1": 320.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c3ab1db0-d66c-40d8-850c-b532bfddfb24", "node_type": "1", "metadata": {"bbox": [{"page": 62, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.8, "x1": 432.0, "y1": 308.89}]}, "hash": "4cc00aa882b65f58e569f3fe70ff9b0d6e5fca6194ec40d51e4e3679bfa97090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f03c0911-4af6-4302-8e10-415c9c08717e", "node_type": "1", "metadata": {"bbox": [{"page": 65, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.3, "x1": 432.01, "y1": 418.41}]}, "hash": "fa341b71ec359351d6d3165ffe0908b051e95cfb4f50c5821782a4b561fff554", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In many situations, therefore, you might want to implement autocomplete solely \nbased on word patterns. How can you do this? While you could build an index out \nof your documents and keep it up to date as your documents change, a simpler way \nmight be to use Logits Masking, with the document held in the context. The approach \ngoes like this: <br><br>1. \n1. Ask an LLM to complete the query that the user is typing in by adding a single <br><br>phrase or sentence. <br><br>2. \n2. In the LogitsProcessor, obtain the list of top completions, show the user the \ntop completions, and have them either select one or type a different phrase \naltogether. <br><br>3. If they select one of the completions, apply sequence selection and zero out \n3. \nthe logits of the other possibilities. If they type a different phrase, use sequence \nregeneration, using that as the new starting point. <br><br>For a simple implementation that grounds the autocomplete in a document (rather \nthan on the web), see the autocomplete notebook in the GitHub repository. In a real \nimplementation, you would cache the entire document in the context to give yourself \naccess to the full knowledge base without paying for input tokens on each query. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f03c0911-4af6-4302-8e10-415c9c08717e": {"__data__": {"id_": "f03c0911-4af6-4302-8e10-415c9c08717e", "embedding": null, "metadata": {"bbox": [{"page": 65, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.3, "x1": 432.01, "y1": 418.41}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cc461b7f-401a-4503-9046-075f1b13c800", "node_type": "1", "metadata": {"bbox": [{"page": 64, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.34, "x1": 432.0, "y1": 320.44}]}, "hash": "016ad03d31d1963a1bd5f4920e030fb26c0dc497c4054d64ee48753205d1bfa9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6a64285-62be-463e-92ab-006a16a5415f", "node_type": "1", "metadata": {"bbox": [{"page": 66, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.69, "x1": 432.01, "y1": 271.4}]}, "hash": "46999ed822f4fa8cc86f299cd9d7e7e919edabeb8c37e164f07e1e5505d6cd9e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A second key consideration is that intercepting the sampling means that each \nsequence being generated requires communication between the model and the client \ncode. Unless the model is locally hosted or deployed in such a way that you can run \nclient code on a colocated processor (talk to your model provider about that), such \ncommunication requirements might add unacceptable latency. This means Logits \nMasking is often applicable only to locally hosted models. However, it provides a way \nfor these smaller models to match the performance of larger, more costly models on \ncertain types of complex problems. <br><br>A third consideration is that Logits Masking works by censoring certain generations. \nIf there\u2019s no candidate token sequence that meets the rules, you won\u2019t be able to \ngenerate valid content. In the pattern discussion, we suggest regenerating from a \ndifferent starting point in this situation, but a simpler way is to raise an error or \nrefuse the request. It\u2019s important for AI engineers using Logits Masking to provide \nenough information in the prompt to make this a rare occurrence. <br><br>Self-Check (Pattern 31 in Chapter 9) is another use of logits. <br><br>**References**\nIn reinforcement learning, the idea of zeroing out logits is called invalid action \nmasking. Its first recorded use was by Vinyals et al. (2019) in the game StarCraft \nII. Theoretical justification for this practice was provided by Huang and Onta\u00f1\u03ccn \n(2020). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6a64285-62be-463e-92ab-006a16a5415f": {"__data__": {"id_": "c6a64285-62be-463e-92ab-006a16a5415f", "embedding": null, "metadata": {"bbox": [{"page": 66, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.69, "x1": 432.01, "y1": 271.4}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f03c0911-4af6-4302-8e10-415c9c08717e", "node_type": "1", "metadata": {"bbox": [{"page": 65, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.3, "x1": 432.01, "y1": 418.41}]}, "hash": "fa341b71ec359351d6d3165ffe0908b051e95cfb4f50c5821782a4b561fff554", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dec98454-2c5f-4b53-9ea8-e43cc7451dfc", "node_type": "1", "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 327.7, "x1": 432.0, "y1": 606.5}]}, "hash": "065bb45ed064067d4c017a22713118e3028a81929751c3da45e8bf6c727c74ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A better approach is to represent the rules you want in a generalizable way, which is \ncalled a grammar. Then, the model framework will apply your grammar specification \nto constrain the set of tokens it generates, so that the generated text will conform \nexactly to the grammar. <br><br>**Solution**\nWhen a foundational model generates text, it does so token by token. At each point, \nit generates a set of candidate tokens that could follow and then chooses among them. \nSome model providers and frameworks allow you to specify a grammar to apply \nto these candidate tokens. The model framework can restrict the next token to the \nones legally allowed by the grammar, and it does so by zeroing out the probability of \ndisallowed tokens. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dec98454-2c5f-4b53-9ea8-e43cc7451dfc": {"__data__": {"id_": "dec98454-2c5f-4b53-9ea8-e43cc7451dfc", "embedding": null, "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 327.7, "x1": 432.0, "y1": 606.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c6a64285-62be-463e-92ab-006a16a5415f", "node_type": "1", "metadata": {"bbox": [{"page": 66, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.69, "x1": 432.01, "y1": 271.4}]}, "hash": "46999ed822f4fa8cc86f299cd9d7e7e919edabeb8c37e164f07e1e5505d6cd9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fff960e-e4e0-4df8-b333-6dc80d74dcbb", "node_type": "1", "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.8, "x1": 432.01, "y1": 318.89}]}, "hash": "2a39e08a94a11a14f7e08dfb50698dc6d98d851ac6695d7ac683bd4dae318615", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Create a LogitsProcessor that will apply this grammar. \n2. <br><br>3. \n3. Pass in the logits processor to the pipeline. <br><br>**Step 1: Create a formal grammar.**The Transformer framework supports grammar spec\u2010 \nifications in Backus-Naur form (BNF). This is great because almost all formal formats \nand programming languages have readily available BNF descriptions. For example, \nyou can find the BNFs for regular expressions, for a SQL TIMESTAMP or CREATE \nINDEX statement, or for a line of a CSV file by using a quick Internet search for \n\u201cBNF for [insert topic].\u201d <br><br>Thus, if you want the LLM to generate valid SQL timestamps, you\u2019d specify the \ngrammar as a string: <br><br>grammar_str = \"\"\" \ntimestamp_literal ::= \n{ t 'yyyy-mm-dd hh:mi:ss' } |'date_literal time_literal' <br><br>date-literal ::= \n{ d'yyyy-mm-dd'} \n |mm-dd-yyyy| mm/dd/yyyy| mm-dd-yy| mm/dd/yy| yyyy-mm-dd \n | yyyy/mm/dd| dd-mon-yyyy| dd/mon/yyyy| dd-mon-yy| dd/mon/yy <br><br>time_literal ::= \n{ t 'hh:mi:ss'}|hh:mi:ss[:mls] \n\"\"\" ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fff960e-e4e0-4df8-b333-6dc80d74dcbb": {"__data__": {"id_": "1fff960e-e4e0-4df8-b333-6dc80d74dcbb", "embedding": null, "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.8, "x1": 432.01, "y1": 318.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dec98454-2c5f-4b53-9ea8-e43cc7451dfc", "node_type": "1", "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 327.7, "x1": 432.0, "y1": 606.5}]}, "hash": "065bb45ed064067d4c017a22713118e3028a81929751c3da45e8bf6c727c74ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07bee01c-df9b-45c2-bd79-367b56b9b590", "node_type": "1", "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 180.57, "x1": 432.0, "y1": 606.65}]}, "hash": "79d1a0b7b17332092272f34d27a19845f3c98db26743fb5d60cb35460c170d1b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because allowed timestamps could contain just the date or just the time, this gram\u2010 \nmar includes those two types as well. <br><br>**Step 2: Create a logits processor that applies grammar.**Second, use the grammar string to \ncreate a grammar-constrained logits processor, as follows: <br><br>grammar = IncrementalGrammarConstraint(grammar_str, \n**\"timestamp_literal\"**, \n pipe.tokenizer) \ngrammar_processor = GrammarConstrainedLogitsProcessor(grammar) <br><br>You have to provide the root element of the grammar, which is timestamp_literal, \nwhen creating the constraint. In this way, you can pass in the grammar of the entire \nSQL spec and still select the specific data type or instruction that you want. <br><br>**Step 3: Apply logits processing.**Finally, pass the logits processor when invoking the \npipeline: <br><br>results = pipe(input_message, \n max_new_tokens=256, \n do_sample=**False**, \n logits_processor=[grammar_processor]) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07bee01c-df9b-45c2-bd79-367b56b9b590": {"__data__": {"id_": "07bee01c-df9b-45c2-bd79-367b56b9b590", "embedding": null, "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 180.57, "x1": 432.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1fff960e-e4e0-4df8-b333-6dc80d74dcbb", "node_type": "1", "metadata": {"bbox": [{"page": 68, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.8, "x1": 432.01, "y1": 318.89}]}, "hash": "2a39e08a94a11a14f7e08dfb50698dc6d98d851ac6695d7ac683bd4dae318615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9637787-b857-4f52-95d5-1e2b43ab3a2b", "node_type": "1", "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.86, "x1": 432.0, "y1": 171.76}]}, "hash": "c1051f1f0917c9e72b5e0492b84305ce7553727fc1d4dc6788ff8a8bc41b0bae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " }, \n \"additionalProperties\": false, \n \"required\": [ \n \"quantity\", \"name\" \n ] \n } <br><br>OpenAI also supports the Python dataclass, as in the following Gemini example. <br><br>If you want Gemini to generate a receipt consisting of line items, you can create a \nPython dataclass: <br><br>@dataclass \n**class LineItem**: \n description: str \n quantity: int \n amount: float <br><br>@dataclass \n**class Receipt**: \n items: LineItem[] \n total_amount: float <br><br>Then, when invoking the model to generate content, you specify the schema: <br><br>response = client.models.generate_content( \n model='gemini-2.0-flash', \n contents=[f\"Parse the receipt contained in the image\", image], \n config={ \n 'response_mime_type': 'application/json', \n 'response_schema': Receipt, \n }, \n) <br><br>The response itself is still a string and will be in JSON format. The Pydantic object \nis used solely to specify the structure of the JSON, but you can use the JSON parsing \nlibrary in Python to parse the JSON text into an object belonging to the dataclass: <br><br>**import json**\ndata_obj = json.loads( \n response.text, \n object_hook=**lambda**args: Receipt(**args) \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9637787-b857-4f52-95d5-1e2b43ab3a2b": {"__data__": {"id_": "d9637787-b857-4f52-95d5-1e2b43ab3a2b", "embedding": null, "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.86, "x1": 432.0, "y1": 171.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "07bee01c-df9b-45c2-bd79-367b56b9b590", "node_type": "1", "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 180.57, "x1": 432.0, "y1": 606.65}]}, "hash": "79d1a0b7b17332092272f34d27a19845f3c98db26743fb5d60cb35460c170d1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a347c8f4-7e61-4efc-86b5-80bdbf67cc83", "node_type": "1", "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.39, "x1": 432.01, "y1": 605.54}]}, "hash": "44a662a45cd9e4b025e762867acb251f7e8be522feaab04487b1d1edeeb1a8fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Python data classes provide a flexible and powerful way to constrain the style of \na GenAI model\u2019s output. To implement the structured outputs feature, the model \nprovider takes on the responsibility of performing Logits Masking (Pattern 1). It \ndoes this server-side by converting the schema or data class into rules or logic (see \nFigure 2-4). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a347c8f4-7e61-4efc-86b5-80bdbf67cc83": {"__data__": {"id_": "a347c8f4-7e61-4efc-86b5-80bdbf67cc83", "embedding": null, "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.39, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d9637787-b857-4f52-95d5-1e2b43ab3a2b", "node_type": "1", "metadata": {"bbox": [{"page": 70, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.86, "x1": 432.0, "y1": 171.76}]}, "hash": "c1051f1f0917c9e72b5e0492b84305ce7553727fc1d4dc6788ff8a8bc41b0bae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b45106d1-5c8c-4ead-b551-c2d5c4497e3a", "node_type": "1", "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.48, "x1": 432.0, "y1": 260.57}]}, "hash": "9b3917af9aae3776911028e8fe289238e2b624ddb70afc3bcdabb9124828c5fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This, however, is not enough. The model\u2019s instruction-following capability is not \nperfect, and there is no guarantee that it will output mathematical expressions. It \nmight simply provide the answer, or it might provide the reasoning required to \ngenerate the answer. <br><br>To constrain the model, write a grammar for simple arithmetic expressions: <br><br>grammar_str = \"\"\" \nroot ::= (expr \"=\" ws term \"**\\n**\")+ \nexpr ::= term ([-+*/] term)*\nterm ::= ident | num | \"(\" ws expr \")\" ws \nident ::= [a-z] [a-z0-9_]*ws \nnum ::= [0-9]+ ws \nws ::= [**\\t\\n**]*\n \"\"\" <br><br>This grammar allows expressions that contain an equal sign that connects terms and \nexpressions. The expression itself (expr) is one or more terms connected by -, +, \n*, or /. A term could be an identity (num_dozen), a number (12), or an expression \nsurrounded by parentheses. The previous grammar also defines an identity (ident), a \nnumber (num), and a whitespace (ws). <br><br>Next, use this grammar to constrain the tokens allowed in the response: <br><br> grammar = IncrementalGrammarConstraint(grammar_str, \"root\", pipe.tokenizer) \n grammar_processor = GrammarConstrainedLogitsProcessor(grammar) <br><br>Then, pass the grammar processor to pipe: <br><br>results = pipe(input_message, \n max_new_tokens=256, \n do_sample=**False**, \n logits_processor=[grammar_processor]) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b45106d1-5c8c-4ead-b551-c2d5c4497e3a": {"__data__": {"id_": "b45106d1-5c8c-4ead-b551-c2d5c4497e3a", "embedding": null, "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.48, "x1": 432.0, "y1": 260.57}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a347c8f4-7e61-4efc-86b5-80bdbf67cc83", "node_type": "1", "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.39, "x1": 432.01, "y1": 605.54}]}, "hash": "44a662a45cd9e4b025e762867acb251f7e8be522feaab04487b1d1edeeb1a8fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edf2803a-29e2-4306-bf4d-d35035267def", "node_type": "1", "metadata": {"bbox": [{"page": 73, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.27, "x1": 432.0, "y1": 292.88}]}, "hash": "77a458fe145be676d5b4c826c9b6ccb795ded8e6cd00d4ea9e5cb867b99da9af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, ask the model this question: <br><br>Bill has 3 apples and 2 oranges. <br><br>Mae has 2 apples and 4 oranges. <br><br>How many apples do Bill and Mae have in total? <br><br>This obtains the following response: <br><br>bill_apples + mae_apples = total_apples <br><br>3 + 2 = 5 <br><br>You can check that the grammar truly does constrain the output by asking a question \nfor which the right answer is not allowed by the grammar: <br><br>Bill has 3 apples and 2 oranges. <br><br>Mae has 2 apples and 4 oranges. <br><br>Do Bill and Mae have more apples than oranges? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edf2803a-29e2-4306-bf4d-d35035267def": {"__data__": {"id_": "edf2803a-29e2-4306-bf4d-d35035267def", "embedding": null, "metadata": {"bbox": [{"page": 73, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.27, "x1": 432.0, "y1": 292.88}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b45106d1-5c8c-4ead-b551-c2d5c4497e3a", "node_type": "1", "metadata": {"bbox": [{"page": 72, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.48, "x1": 432.0, "y1": 260.57}]}, "hash": "9b3917af9aae3776911028e8fe289238e2b624ddb70afc3bcdabb9124828c5fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5930cb2-2006-4071-85a7-8ec95f710822", "node_type": "1", "metadata": {"bbox": [{"page": 74, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.8, "x1": 432.0, "y1": 304.81}]}, "hash": "f30b578c674b8643ecef111371121bc04d3e431d7f872224ae0434d8d02f14a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This grammar allows names and titles to consist of lowercase and uppercase letters \nand spaces, but not non-English characters such as those with accent or diacritical \nmarks. This can be important if your downstream processing code is a legacy system \nthat expects 7-bit ASCII characters. Thus, say you pass in this paragraph: <br><br>Love in the Time of Cholera (Spanish: El amor en los tiempos del c\u03cclera) is a novel \nwritten in Spanish by Colombian Nobel Prize\u2013winning author Gabriel Garc\u00eda M\u00e1rquez \nand published in 1985. <br><br>Then, the model will output the author\u2019s name with the accents removed: <br><br>Gabriel Garcia Marquez | Love in the Time of Cholera |1985 <br><br>Next, say you pass in this paragraph: <br><br>The Tirukkural (Tamil: \u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bcd, lit. \u2019sacred verses') is a classic Tamil language \ntext whose authorship is traditionally attributed to Valluvar, also known in full as \nThiruvalluvar. The text has been dated variously from 300 BCE to the 5th century CE. \nThe traditional accounts describe it as the last work of the third Sangam, but linguistic \nanalysis suggests a later date of 450 to 500 CE and that it was composed after the \nSangam period. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5930cb2-2006-4071-85a7-8ec95f710822": {"__data__": {"id_": "b5930cb2-2006-4071-85a7-8ec95f710822", "embedding": null, "metadata": {"bbox": [{"page": 74, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.8, "x1": 432.0, "y1": 304.81}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "edf2803a-29e2-4306-bf4d-d35035267def", "node_type": "1", "metadata": {"bbox": [{"page": 73, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.27, "x1": 432.0, "y1": 292.88}]}, "hash": "77a458fe145be676d5b4c826c9b6ccb795ded8e6cd00d4ea9e5cb867b99da9af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0690aad-fa54-48a4-9203-2e63d25f3f56", "node_type": "1", "metadata": {"bbox": [{"page": 75, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.26, "x1": 432.01, "y1": 276.96}]}, "hash": "62aea580be49a72056c785b434468d3d74454d803ed3bfe19161d6630f994fed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " input_message = [ \n {\"role\": \"developer\", \"content\": system_prompt}, \n {\"role\": \"user\", \"content\": paragraph} \n ] <br><br> response = client.chat.completions.create( \n model=MODEL_ID, \n messages=input_message, \n response_format={\"type\": \"json_object\"} \n ) \n**return**response.choices[0].message.content <br><br>When you pass in the paragraph about Love in the Time of Cholera, the output JSON \nis this: <br><br>{ \n \"author\": \"Gabriel Garc\u00eda M\u00e1rquez\", \n \"title\": \"Love in the Time of Cholera\", \n \"year\": 1985 \n} <br><br>When you pass in the paragraph about the Tamil classic text, the output JSON is this: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0690aad-fa54-48a4-9203-2e63d25f3f56": {"__data__": {"id_": "a0690aad-fa54-48a4-9203-2e63d25f3f56", "embedding": null, "metadata": {"bbox": [{"page": 75, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.26, "x1": 432.01, "y1": 276.96}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b5930cb2-2006-4071-85a7-8ec95f710822", "node_type": "1", "metadata": {"bbox": [{"page": 74, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.8, "x1": 432.0, "y1": 304.81}]}, "hash": "f30b578c674b8643ecef111371121bc04d3e431d7f872224ae0434d8d02f14a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf195232-2791-4107-8aaa-eb909e0029f1", "node_type": "1", "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 334.97, "x1": 432.0, "y1": 605.54}]}, "hash": "1e20bf5932e11d007a6e0fcaa8d9f169d21781aeee7ed1649c5d42678b77bf7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "@dataclass \n**class Invoice**: \n purpose: str \n amount: float \n currency: CurrencyEnum = CurrencyEnum.USD <br><br>Then, to parse invoice information from a paragraph of text, you can use the Pydanti\u2010 \ncAI framework to get structured outputs in an LLM-agnostic way: <br><br>**from pydantic_ai import**Agent <br><br>**def**parse_invoice_info(paragraph: str) -> str: \n system_prompt = \"\"\" \n You will be given a short snippet from an email that represents an invoice. \n Extract the purpose and amount of the invoice. \n \"\"\" <br><br> agent = Agent(model, \n result_type=Invoice, \n system_prompt=system_prompt) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf195232-2791-4107-8aaa-eb909e0029f1": {"__data__": {"id_": "bf195232-2791-4107-8aaa-eb909e0029f1", "embedding": null, "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 334.97, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a0690aad-fa54-48a4-9203-2e63d25f3f56", "node_type": "1", "metadata": {"bbox": [{"page": 75, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.26, "x1": 432.01, "y1": 276.96}]}, "hash": "62aea580be49a72056c785b434468d3d74454d803ed3bfe19161d6630f994fed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58efc4c5-6782-4760-966f-83eb59a7549c", "node_type": "1", "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.78, "x1": 432.0, "y1": 326.16}, {"page": 78, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.27, "x1": 432.0, "y1": 346.62}]}, "hash": "ce77e06545cc765bfe38c97aa8205d9c8394f78366a9db7b64a639a0f51f051f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Latency <br><br>BNF constraints are applied via Logits Masking by the model framework, and \nthey\u2019re therefore applied client-side. On the other hand, Pydantic constraints are \napplied by the model provider and are therefore applied server-side in the case \nof APIs such as GPT-4, Gemini, and Claude. Therefore, the Pydantic approach \nreduces the number of network calls. <br><br>Model support <br><br>Using BNF requires access to logprobs, and as mentioned in the \u201cCaveats\u201d \nsection of \u201cLogits Masking (Pattern 1),\u201d support for this is not universal. On \nthe other hand, every modern model supports Grammar constraints via Python \ndataclass. <br><br>The BNF approach is nevertheless more flexible than the Pydantic approach, and you \nneed to use it if your style rules involve more than just data formats. For example, \nit\u2019s possible to express validation rules as BNF, whereas any validation beyond Enum is \nhard to do in Pydantic. To illustrate, here\u2019s a potential BNF grammar that a company \nmight employ if it accepts three types of United States (US) credit cards: <br><br><credit_card_number> ::= <visa_number> | <mc_number> | <amex_number> \n<visa_number> ::= \"4\" <digit>{12,15} \n<mc_number> ::= (\"51\"..\"55\" <digit>{14}) | (\"2221\"..\"2720\" <digit>{12}) \n<amex_number> ::= \"34\" <digit>{13} | \"37\" <digit>{13} \n<digit> ::= \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58efc4c5-6782-4760-966f-83eb59a7549c": {"__data__": {"id_": "58efc4c5-6782-4760-966f-83eb59a7549c", "embedding": null, "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.78, "x1": 432.0, "y1": 326.16}, {"page": 78, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.27, "x1": 432.0, "y1": 346.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bf195232-2791-4107-8aaa-eb909e0029f1", "node_type": "1", "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 334.97, "x1": 432.0, "y1": 605.54}]}, "hash": "1e20bf5932e11d007a6e0fcaa8d9f169d21781aeee7ed1649c5d42678b77bf7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4aa330a-1566-4f33-bca6-0c2fc4492327", "node_type": "1", "metadata": {"bbox": [{"page": 79, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.64, "x1": 432.01, "y1": 244.27}]}, "hash": "68e6caa92e2b11e9cfcda5549655edce2621e9ee433e34bed5ed24b12cdd531b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\u2019s considerably harder to do this in Pydantic since you would have to write validator \nlogic \u2014 no longer would this be just a simple dataclass: <br><br>**class CreditCard**(BaseModel): \n number: str <br><br> @field_validator('number') \n @classmethod \n**def**validate_number(cls, value: str) -> str: \n value = value.replace(\" \", \"\").replace(\"-\", \"\") \n**if not**value.isdigit(): \n**raise ValueError**(\"Credit card number must contain only digits.\") <br><br> length = len(value) \n first_digit = value[0] \n first_two_digits = value[:2] \n first_four_digits = value[:4] \n first_six_digits = value[:6] <br><br>*# Visa*\n**if**first_digit == '4'**and**(length == 13**or**length == 16): \n**return**value <br><br>*# Mastercard*\n**if**(first_two_digits**in**('51', '52', '53', '54', '55')**or**<br><br>\u2022 The rules you want to apply can\u2019t be represented as a dataclass. Any rule that \n\u2022 \nconsists of logic rather than mere representation fits this profile. For example, \nyou may want to mask out names of competitor products, but only in specific \nsituations. <br><br>\u2022 \n\u2022 The BNF grammar is very complex and hard to debug. <br><br>\u2022 The rules are dynamic and depend on the content. For example, you may want to \n\u2022 \nmask out the name of a competitor product when the content is talking about a \nnew launch, but not if the content relates to an in-market product. <br><br>\u2022 \n\u2022 The rules need to be fetched from a database or rules engine. For example, rules <br><br>on what to mask may vary by client. <br><br>\u2022 \n\u2022 Masking depends on user input, such as in the Autocomplete use case that was <br><br>discussed in \u201cLogits Masking (Pattern 1).\u201d <br><br>\u2022 \n\u2022 The rules involve invoking an external tool or API. <br><br>In these situations, consider implementing a logits processor and writing code to \ndetermine whether to mask an input token sequence. <br><br>**Caveats**<br><br>If the model does not output any tokens that meet the grammar constraints, genera\u2010 \ntion will fail. This failure can manifest itself in the following ways: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4aa330a-1566-4f33-bca6-0c2fc4492327": {"__data__": {"id_": "c4aa330a-1566-4f33-bca6-0c2fc4492327", "embedding": null, "metadata": {"bbox": [{"page": 79, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.64, "x1": 432.01, "y1": 244.27}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "58efc4c5-6782-4760-966f-83eb59a7549c", "node_type": "1", "metadata": {"bbox": [{"page": 77, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.78, "x1": 432.0, "y1": 326.16}, {"page": 78, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.27, "x1": 432.0, "y1": 346.62}]}, "hash": "ce77e06545cc765bfe38c97aa8205d9c8394f78366a9db7b64a639a0f51f051f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e1acde8-e0d2-4522-b122-f6a11f2bb184", "node_type": "1", "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 139.43, "x1": 432.0, "y1": 363.67}]}, "hash": "df7b838f798a77dad81f10384929ff956f82bd272006f47bb82244c30670359c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nEarly approaches that incorporated the grammar into the prompt, like the one in \nWang et al.\u2019s 2024 paper \u201cGrammar Prompting for Domain-Specific Language Gener\u2010 \nation with Large Language Models,\u201d have turned out to be error-prone. Using gram\u2010 \nmar masking and constrained decoding was first detailed in a 2024 paper by Netz, \nReimar, and Rumpe, but the idea was implemented earlier, in 2023, by Rickard for \nregular expressions and Jones for BNF grammars. Grammar need not be strict\u2014in \n2025, a group of MIT researchers assigned weights to structured data continuations \nusing Monte Carlo simulations and demonstrated that this makes AI-generated code \nmore accurate. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e1acde8-e0d2-4522-b122-f6a11f2bb184": {"__data__": {"id_": "2e1acde8-e0d2-4522-b122-f6a11f2bb184", "embedding": null, "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 139.43, "x1": 432.0, "y1": 363.67}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c4aa330a-1566-4f33-bca6-0c2fc4492327", "node_type": "1", "metadata": {"bbox": [{"page": 79, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.64, "x1": 432.01, "y1": 244.27}]}, "hash": "68e6caa92e2b11e9cfcda5549655edce2621e9ee433e34bed5ed24b12cdd531b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d29be8-0163-4119-8a93-ad750f09a4b2", "node_type": "1", "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.94, "y1": 96.95}]}, "hash": "5007d7a9ddfed18f41106fc8145dd98bbeb4ee6a8f3ddc689a9d191823d43595", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Problem**\nSuppose you want a GenAI model to generate content that mimics a specific tone and \nstyle of texts. Let\u2019s assume that your situation satisfies these three criteria (also see \nFigure 2-5): <br><br>Available content <br><br>The content that you want is readily available, but it\u2019s just not in the tone or style \nyou want to use. Perhaps the content is available in academic research papers, \nbut you want to use parts of the content (perhaps the methods and results) in \nmarketing brochures targeted at nontechnical executives. <br><br>Nuanced style <br><br>It\u2019s difficult to express the nuances of what you want in a few rules. The charac\u2010 \nteristics of the desired style might be very subtle, and humans may find it hard \nto express what these characteristics are. However, humans will often recognize \nthe right style. (\u201cI know it when I see it\u201d) For example, it\u2019s hard to express what \nvocabulary is allowed in marketing brochures. Can we use the term fine-tuning? \nHow about reinforcement learning? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71d29be8-0163-4119-8a93-ad750f09a4b2": {"__data__": {"id_": "71d29be8-0163-4119-8a93-ad750f09a4b2", "embedding": null, "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.94, "y1": 96.95}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2e1acde8-e0d2-4522-b122-f6a11f2bb184", "node_type": "1", "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 139.43, "x1": 432.0, "y1": 363.67}]}, "hash": "df7b838f798a77dad81f10384929ff956f82bd272006f47bb82244c30670359c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cb96e89-332a-4220-8a31-0b99178d1a2c", "node_type": "1", "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.95, "x1": 467.25, "y1": 605.54}]}, "hash": "92fbe924fdf0bdfb903f4c719776c670cb06ba798fd256998feed2f8ab6d2c65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 Though superficially similar, Claude\u2019s implementation at the time we are writing this (July 2025) seems to <br><br>not employ constrained decoding. Instead, Anthropic documentation suggests prefilling the response with the \nstart of the desired format to avoid unwanted preambles. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3cb96e89-332a-4220-8a31-0b99178d1a2c": {"__data__": {"id_": "3cb96e89-332a-4220-8a31-0b99178d1a2c", "embedding": null, "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.95, "x1": 467.25, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "71d29be8-0163-4119-8a93-ad750f09a4b2", "node_type": "1", "metadata": {"bbox": [{"page": 80, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.94, "y1": 96.95}]}, "hash": "5007d7a9ddfed18f41106fc8145dd98bbeb4ee6a8f3ddc689a9d191823d43595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb6eaaf1-a5cb-4a2b-9399-112ff28937e1", "node_type": "1", "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.28, "x1": 441.75, "y1": 237.05}]}, "hash": "5b70acdeef40eb5acda84b6c61505ea51c6d44dc85ac0960915f44ad250ce08b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rewriting generic company content to reflect brand-specific style guidelines <br><br>For example, this could involve converting standard helpdesk documentation to \ngive it a warm, friendly tone that matches your business brand. <br><br>Transforming social media content across different platforms <br><br>This could involve taking a lengthy LinkedIn post and adapting it for X or \nBlueSky\u2019s short message format or Instagram\u2019s more visually focused style, while \nmaintaining the core message and brand voice. <br><br>Converting detailed technical documentation into different proficiency levels <br><br>This involves taking complex technical documentation and converting it into \nversions that are tailored to beginners, intermediate users, or experts, with appro\u2010 \npriate terminology and explanations. <br><br>Converting notes into professional emails <br><br>This involves taking a short message from an executive and converting it into a \nformal memo or email. <br><br>Simply asking a foundational model to carry out the task without providing examples \n(zero-shot learning) doesn\u2019t work well. For example, you could ask OpenAI to convert \nnotes into a fully written email by using the following: <br><br>messages = [ \n {'role': 'system', \n 'content': 'You are a helpful assistant converting notes to professional emails.'}, \n {'role': 'user', \n 'content': \"\"\" \nurgent: need meeting minutes by {deadline} \nfor stakeholder presentation**\\n**details: \n- This is needed for our upcoming presi to stakeholders. \n- will need ur help asap \n- send 2 me when rdy \n\"\"\"}] ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb6eaaf1-a5cb-4a2b-9399-112ff28937e1": {"__data__": {"id_": "bb6eaaf1-a5cb-4a2b-9399-112ff28937e1", "embedding": null, "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.28, "x1": 441.75, "y1": 237.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3cb96e89-332a-4220-8a31-0b99178d1a2c", "node_type": "1", "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 248.95, "x1": 467.25, "y1": 605.54}]}, "hash": "92fbe924fdf0bdfb903f4c719776c670cb06ba798fd256998feed2f8ab6d2c65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62478426-8e12-410a-b9b9-ca79d990eb3e", "node_type": "1", "metadata": {"bbox": [{"page": 83, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 102.88, "x1": 432.01, "y1": 325.47}]}, "hash": "ab18993e1340733c092ce1dcb4bbd4e969e083b0696789a1d89885262ea0a26b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages) <br><br>When we did this, the model responded with the following output: <br><br>Output: Subject: Request for Meeting Minutes by {Deadline} for Stakeholder Presenta\u2010 \ntion <br><br>Dear [Recipient], <br><br>I hope this message finds you well. <br><br>I am reaching out to request your urgent assistance in preparing meeting minutes for \nour recent meeting, which are required for our upcoming stakeholder presentation. <br><br>The meeting minutes are crucial for ensuring that we accurately represent the dis\u2010 \ncussions and decisions made during the meeting to our stakeholders. Your prompt \nattention to this matter is greatly appreciated. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62478426-8e12-410a-b9b9-ca79d990eb3e": {"__data__": {"id_": "62478426-8e12-410a-b9b9-ca79d990eb3e", "embedding": null, "metadata": {"bbox": [{"page": 83, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 102.88, "x1": 432.01, "y1": 325.47}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bb6eaaf1-a5cb-4a2b-9399-112ff28937e1", "node_type": "1", "metadata": {"bbox": [{"page": 82, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.28, "x1": 441.75, "y1": 237.05}]}, "hash": "5b70acdeef40eb5acda84b6c61505ea51c6d44dc85ac0960915f44ad250ce08b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "048f3745-8ae8-41b1-aa61-9b928c9b7717", "node_type": "1", "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 325.5, "x1": 432.0, "y1": 607.79}]}, "hash": "5562bdead6b83ef98f2c0b4b925b0b3a58a1c04b3193a1bd0715b1738960b3d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Suppose you have a large number of research papers you would like to convert \ninto blog posts. Let\u2019s also say you have a few examples of research papers that \nwere converted manually into blog posts. Style Transfer lets you use the example \ninput-and-output pairs (the few research papers that were converted into handcrafted \nblog posts) to convert any given research paper into a blog post, in the same style as \nthe handcrafted blog posts (refer back to Figure 2-5). <br><br>We\u2019ll discuss two approaches to Style Transfer: few-shot learning and model fine- \ntuning. We\u2019ll also demonstrate that this pattern is not limited to text by showing you \nan example of image style transfer. <br><br>**Option 1: Few-shot learning**<br><br>In few-shot learning, you provide a number of examples of input and expected \noutputs in your prompt (see Figure 2-6). The examples should be relevant to the \nexpected problem space, and you should typically add 1 to 10 examples to your \nprompts. Think of it like a student-teacher relationship: the teacher (you) shows the \nLLM (the student) a few examples and expects the student to mimic the examples in \nfuture problems. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "048f3745-8ae8-41b1-aa61-9b928c9b7717": {"__data__": {"id_": "048f3745-8ae8-41b1-aa61-9b928c9b7717", "embedding": null, "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 325.5, "x1": 432.0, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "62478426-8e12-410a-b9b9-ca79d990eb3e", "node_type": "1", "metadata": {"bbox": [{"page": 83, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 102.88, "x1": 432.01, "y1": 325.47}]}, "hash": "ab18993e1340733c092ce1dcb4bbd4e969e083b0696789a1d89885262ea0a26b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83f043ff-5932-448c-bfe1-dde1dab855c1", "node_type": "1", "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.1, "x1": 432.01, "y1": 317.4}]}, "hash": "43c7cdff5e13a03fa89cd2b6e6a290889988825a97acebb681d0a7f6fdad031b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Option 2: Model fine-tuning**<br><br>Fine-tuning an LLM is more complex than few-shot learning, but it provides you \nwith more fine-grained control and reduces inference costs. <br><br>In fine-tuning (see Adapter Tuning [Pattern 15] in Chapter 5), you use a number \nof examples as expected outputs of the LLM generation: usually a hundred or so, \nbut it could be in thousands. You then pair them with the corresponding inputs and \nfine-tune the LLM to generate the expected outputs (see Figure 2-7). <br><br>This approach has several advantages, including the following: <br><br>Higher fidelity <br><br>You can fine-tune a model beyond a few examples. Typically, this will lead \nto more satisfying results, especially for complex tasks. The task of restyling \nresearch papers as marketing brochures, for example, requires the model to learn \na lot of vocabulary mappings, and this is more likely with fine-tuning. <br><br>Faster, less expensive inference <br><br>You can reduce the prompt to a bare minimum because the model will already be \ntuned to the given task. The prompt doesn\u2019t include examples of other research \npapers, either\u2014only the one being converted. This reduces the inference latency \nbecause the LLM won\u2019t have to process the examples in the prompt. Shorter \nprompts also typically incur lower costs (but see this pattern\u2019s \u201cConsiderations\u201d \nsection for a more nuanced discussion of this point). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83f043ff-5932-448c-bfe1-dde1dab855c1": {"__data__": {"id_": "83f043ff-5932-448c-bfe1-dde1dab855c1", "embedding": null, "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.1, "x1": 432.01, "y1": 317.4}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "048f3745-8ae8-41b1-aa61-9b928c9b7717", "node_type": "1", "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 325.5, "x1": 432.0, "y1": 607.79}]}, "hash": "5562bdead6b83ef98f2c0b4b925b0b3a58a1c04b3193a1bd0715b1738960b3d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41c6c631-5c97-4609-8f65-0673ce5126d1", "node_type": "1", "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 329.95, "x1": 432.0, "y1": 605.54}]}, "hash": "70713c535d93312c6975f7c5ee7804b31dc6fda7bd6f64aac8ce1330384650c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These two advantages build on each other. In many cases, you can even tune a smaller \nmodel for a complex task that originally failed with the in-context few-shot learning \napproach. This leads to even faster inference and even lower inference and hosting \ncosts. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41c6c631-5c97-4609-8f65-0673ce5126d1": {"__data__": {"id_": "41c6c631-5c97-4609-8f65-0673ce5126d1", "embedding": null, "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 329.95, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "83f043ff-5932-448c-bfe1-dde1dab855c1", "node_type": "1", "metadata": {"bbox": [{"page": 85, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 269.1, "x1": 432.01, "y1": 317.4}]}, "hash": "43c7cdff5e13a03fa89cd2b6e6a290889988825a97acebb681d0a7f6fdad031b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da448aae-1a67-47d8-bc10-4e53abac7bdf", "node_type": "1", "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.32, "x1": 432.01, "y1": 321.14}]}, "hash": "c9c32e4aa866bacd179d79fd7c07fe35cd40f23c75624dfdc2a971781a879a35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ops expertise <br><br>You often need AI engineering and large language model operations (LLMOps) \nexpertise in your company to host and operate a fine-tuned LLM for this specific \ntask. This is a rarer, more expensive skill than being able to stuff examples into a \nprompt. <br><br>You can fine-tune OpenAI\u2019s GPT-3.5-turbo model by uploading the training file and \ncreating a fine-tuning job (see this fine-tuning example): <br><br>training_file = client.files.create( \n file=open(\"fine_tuning_dataset.jsonl\", \"rb\"), \n purpose=\"fine-tune\" \n) \njob = client.fine_tuning.jobs.create( \n training_file=training_file.id, \n model=\"gpt-3.5-turbo\"*# Base model to fine-tune*\n) <br><br>In this case, OpenAI deploys and hosts the model for you, thus reducing the engi\u2010 \nneering expertise needed. <br><br>Then, poll the job status and wait until it either succeeds or fails: <br><br>**while True**: \n job_status = client.fine_tuning.jobs.retrieve(job.id) \n**if**job_status.status**in**['succeeded', 'failed']: \n**break**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da448aae-1a67-47d8-bc10-4e53abac7bdf": {"__data__": {"id_": "da448aae-1a67-47d8-bc10-4e53abac7bdf", "embedding": null, "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.32, "x1": 432.01, "y1": 321.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "41c6c631-5c97-4609-8f65-0673ce5126d1", "node_type": "1", "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 329.95, "x1": 432.0, "y1": 605.54}]}, "hash": "70713c535d93312c6975f7c5ee7804b31dc6fda7bd6f64aac8ce1330384650c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fa0d55d-b2ec-4d4f-a4ab-002df04e51f6", "node_type": "1", "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.15, "x1": 432.01, "y1": 605.54}]}, "hash": "768a774aa75fa43e69b965ccb8d7b95fcb28742ace6508db0764fdc900076283", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the job is complete, you can use the OpenAI client API to invoke the fine-tuned \nmodel the same way you invoke a foundational model: <br><br>completion = client.chat.completions.create( \n model=job_status.fine_tuned_model,*# Use the fine-tuned model*\n messages=messages \n) \nprint(completion.choices[0].message.content) <br><br>The result matches our desired style: <br><br>Subject: Request for meeting minutes Body: <br><br>Hi {name}, <br><br>I hope you\u2019re doing well. <br><br>I\u2019m reaching out because I need meeting minutes by {deadline}. <br><br>This is needed for our upcoming presentation to stakeholders. Could you please help \nme with this? <br><br>Let me know if you need any additional information. <br><br>Thank you in advance for your help. <br><br>Best, <br><br>{name} ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fa0d55d-b2ec-4d4f-a4ab-002df04e51f6": {"__data__": {"id_": "8fa0d55d-b2ec-4d4f-a4ab-002df04e51f6", "embedding": null, "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.15, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "da448aae-1a67-47d8-bc10-4e53abac7bdf", "node_type": "1", "metadata": {"bbox": [{"page": 87, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.32, "x1": 432.01, "y1": 321.14}]}, "hash": "c9c32e4aa866bacd179d79fd7c07fe35cd40f23c75624dfdc2a971781a879a35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aa18a0f-0a11-431f-868c-0d2ddaaeffae", "node_type": "1", "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.44, "x1": 432.01, "y1": 322.34}]}, "hash": "46d63d3eddbeade4a6a8ff1b96bd209e7746b020a00c6ca558b72d465ba7101d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can perform Style Transfer by using the RunwayML Stable Diffusion model \nand the diffusers library from Hugging Face (the complete code is in the GitHub \nrepository): <br><br>pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained( \n \"stable-diffusion-v1-5/stable-diffusion-v1-5\", \n controlnet=controlnet, \n torch_dtype=torch.float16, \n use_safetensors=**True**\n) <br><br>While it is possible to carry out style transfer using just the original painting, it has \nbeen found that you get better results if you provide a control image that gives the \nStyle Transfer pattern hints about what aspect(s) of style are important to preserve. \nTraditional neural Style Transfer often distorts an image\u2019s spatial layout and depth \nrelationships, but depth-aware approaches can help preserve these important aspects. <br><br>Because the perspective view is so important in the painting, let\u2019s use a depth map \nas our control image. A depth map builds a 3D model of the scene in the image by \ncomputing the distance of each pixel from a vantage point. A pipeline to estimate the \ndepth already exists in the diffusers library: <br><br>depth_estimator = pipeline(\"depth-estimation\") \ndepth_image = depth_estimator(image)[\"depth\"] \n\u2026 \nwander_depth_map = ...unsqueeze(0).half().to(\"cuda\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6aa18a0f-0a11-431f-868c-0d2ddaaeffae": {"__data__": {"id_": "6aa18a0f-0a11-431f-868c-0d2ddaaeffae", "embedding": null, "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.44, "x1": 432.01, "y1": 322.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8fa0d55d-b2ec-4d4f-a4ab-002df04e51f6", "node_type": "1", "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.15, "x1": 432.01, "y1": 605.54}]}, "hash": "768a774aa75fa43e69b965ccb8d7b95fcb28742ace6508db0764fdc900076283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3ba60f0-8a70-4493-8753-6074700af988", "node_type": "1", "metadata": {"bbox": [{"page": 90, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.03, "x1": 432.01, "y1": 256.33}]}, "hash": "733765abed964c7cca29e7122be3a94c561e2b6a8c3082b32c94e1688ed1f796", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting depth map is shown to the right of the painting in Figure 2-8. In it, the \ncloser pixels are whiter and pixels farther away are darker. <br><br>Next, you send a text prompt, the style image, and the spatial-information control \nimage to the pipeline, as follows: <br><br>prompt = \"Star Wars' Darth Vader with a red light saber\" \noutput = pipe( \n prompt, \n image=wanderer_image, \n control_image=wanderer_depth_map, \u2026 \n).images[0] <br><br>The result, shown at the bottom of Figure 2-8, is a Star Wars version of Caspar \nDavid Friedrich\u2019s painting. Note that the wanderer has been replaced by Darth Vader \nbut that the inspiration of the original painting is quite obvious in the way the \nspatial perspective, the character\u2019s pose, and the background have been preserved. \nThe wanderer\u2019s cane has become a lightsaber, but it remains in the same position. <br><br>That said, there is some creativity as well\u2014the generated image includes spaceships \non the distant horizon and the character faces the viewer, whereas the wanderer in \nthe painting has his back to us. If you don\u2019t want such creativity, you could change \nthe relative weights of the style image and the content prompt. For more details and ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3ba60f0-8a70-4493-8753-6074700af988": {"__data__": {"id_": "f3ba60f0-8a70-4493-8753-6074700af988", "embedding": null, "metadata": {"bbox": [{"page": 90, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.03, "x1": 432.01, "y1": 256.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6aa18a0f-0a11-431f-868c-0d2ddaaeffae", "node_type": "1", "metadata": {"bbox": [{"page": 89, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.44, "x1": 432.01, "y1": 322.34}]}, "hash": "46d63d3eddbeade4a6a8ff1b96bd209e7746b020a00c6ca558b72d465ba7101d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15cc0e04-f9af-4734-8ab8-3a9410c25f59", "node_type": "1", "metadata": {"bbox": [{"page": 91, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 90.02, "x1": 432.0, "y1": 371.73}]}, "hash": "9bab594344cf9b31a261de4bc0dc6abb2f3f25678af6f84bb305189a06609b93", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Limits of the context <br><br>Adding examples to an LLM prompt via in-context learning will increase the \ninput token length. This has several effects on the LLM\u2019s generation. First, \nalthough LLMs have shown significant improvements in context lengths, they\u2019re \ngenerally limited in their context length. Therefore, the more examples you add, \nthe more you lose the context window availability for the actual prompt. With \ntoo many examples, it is also possible that the \u201cmessage\u201d of the examples will get \nlost or that the examples can contradict one another. That \u201cconfuses\u201d the LLM, \nand the quality of the results is often poor. <br><br>Inference speed <br><br>As you know, the more examples you add to your prompt, the longer the input \ninto the LLM will be. Longer inputs into the LLM also mean longer latency \nbecause the model needs to apply the attention mechanism to a more extended \ncontext. This will ultimately affect the inference speed from input to generation, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15cc0e04-f9af-4734-8ab8-3a9410c25f59": {"__data__": {"id_": "15cc0e04-f9af-4734-8ab8-3a9410c25f59", "embedding": null, "metadata": {"bbox": [{"page": 91, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 90.02, "x1": 432.0, "y1": 371.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f3ba60f0-8a70-4493-8753-6074700af988", "node_type": "1", "metadata": {"bbox": [{"page": 90, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.03, "x1": 432.01, "y1": 256.33}]}, "hash": "733765abed964c7cca29e7122be3a94c561e2b6a8c3082b32c94e1688ed1f796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7ad00ac-f2af-4d32-ab9a-9c10484babed", "node_type": "1", "metadata": {"bbox": [{"page": 92, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.63, "x1": 432.01, "y1": 340.53}]}, "hash": "2478f58fc75ed73b0019ef2718a0c2ae9e37779597bc374879c45dec4741b26a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Addlly transfers the style of existing product descriptions to new products to be listed \nin Shopify in order to maintain brand voice and format. <br><br>**Pattern 4: Reverse Neutralization**\nReverse Neutralization allows you to generate content in some desired style. You do \nthis by using a fine-tuned model to postprocess the output of a GenAI model (which \nwill be in a neutral form) into the desired style. <br><br>As with Pattern 3, Style Transfer (and unlike with Pattern 1, Logits Masking, or \nPattern 2, Grammar), all you need for this pattern are examples\u2014you don\u2019t need to \nrepresent the desired style using rules or grammar. Unlike with Style Transfer, you \ncan use Reverse Neutralization even if you don\u2019t have handcrafted input-and-output \npairs. <br><br>**Problem**\nSuppose you wish to generate content in your personal style. You\u2019ll ask the bot to \ngenerate a letter to Lufthansa complaining about lost baggage, and you want the letter \nto be written as if you wrote it. The letter has to use the words you tend to use, in the \ntone you tend to use, be of the same length as your letters, and so on. <br><br>Zero-shot learning is out: you can ask an LLM to write a complaint letter, but it will \nnot be in your personal style. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7ad00ac-f2af-4d32-ab9a-9c10484babed": {"__data__": {"id_": "f7ad00ac-f2af-4d32-ab9a-9c10484babed", "embedding": null, "metadata": {"bbox": [{"page": 92, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.63, "x1": 432.01, "y1": 340.53}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "15cc0e04-f9af-4734-8ab8-3a9410c25f59", "node_type": "1", "metadata": {"bbox": [{"page": 91, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 90.02, "x1": 432.0, "y1": 371.73}]}, "hash": "9bab594344cf9b31a261de4bc0dc6abb2f3f25678af6f84bb305189a06609b93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0867d141-9d3f-4c1d-a593-6539b7776f8d", "node_type": "1", "metadata": {"bbox": [{"page": 94, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "2d8deedd4edd01a9730a46d7d46b76851c0c0446c730e072c5970c6a732f2be4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, you create a fine-tuned model that can convert text from the intermediate \nneutral form into the desired style. Creating the fine-tuned model consists of the \nfollowing three steps (also see Figure 2-9): <br><br>Step 1. Generating a neutral form <br><br>Take several emails written in your personal style and ask the foundational model \nto rephrase them into a neutral form, perhaps as professional emails exchanged \nbetween two executives. <br><br>Step 2. Creating a training dataset <br><br>Reverse the inputs and outputs of Step 1, so your inputs are the professional \nemails and the outputs are the emails in your personal style. This forms the \ntraining dataset. <br><br>Step 3. Fine-tuning the model <br><br>Create a fine-tuned model by training the base model on the training dataset. \nThe resulting model is capable of converting neutral text into emails in the \ndesired style. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0867d141-9d3f-4c1d-a593-6539b7776f8d": {"__data__": {"id_": "0867d141-9d3f-4c1d-a593-6539b7776f8d", "embedding": null, "metadata": {"bbox": [{"page": 94, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f7ad00ac-f2af-4d32-ab9a-9c10484babed", "node_type": "1", "metadata": {"bbox": [{"page": 92, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.63, "x1": 432.01, "y1": 340.53}]}, "hash": "2478f58fc75ed73b0019ef2718a0c2ae9e37779597bc374879c45dec4741b26a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc2c83c2-eb1d-4288-846d-c36e466830e8", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 435.84, "x1": 1823.0, "y1": 606.65}]}, "hash": "0f3e8572d5dbacd45b09ef9acd7c0ace4639920e01eca6405ad9e208c96ba814", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "form. In our example, this would be a letter from an attorney to an executive at \nLufthansa. Second, you\u2019ll use the fine-tuned model to convert the neutral letter into \none that reflects your personal style. <br><br><br><br>Figure 2-10. Inference in Reverse Neutralization: one LLM call to the foundational \nmodel to generate the intermediate neutral form and a second to the fine-tuned LLM to \nconvert the content to the desired style <br><br>This pattern gives you a way to train models that emit content that matches your \ncompany\u2019s brand. You can use it to generate training data for LLM fine-tuning \ntasks when styled content is readily available (e.g., product descriptions or customer \nreviews) but a neutral version as generation input is missing. <br><br>**Example 1: Generating Legal Text**\nLet\u2019s look at an end-to-end example of generating a legal complaint that we want to \nsend to Lufthansa about lost luggage. The foundational model will struggle to create \nan appropriate legal notice, especially given that legal standards and norms vary from \none place to the next. <br><br>Let\u2019s suppose that your law firm has a set of legal notices on a wide variety of topics. \nThe first step is to use an LLM to neutralize these legal texts (the full code is on \nGitHub): <br><br>messages = [ \n {'role': 'system', <br><br>**Pattern 4: Reverse Neutralization** **|** **77**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc2c83c2-eb1d-4288-846d-c36e466830e8": {"__data__": {"id_": "dc2c83c2-eb1d-4288-846d-c36e466830e8", "embedding": null, "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 435.84, "x1": 1823.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0867d141-9d3f-4c1d-a593-6539b7776f8d", "node_type": "1", "metadata": {"bbox": [{"page": 94, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "2d8deedd4edd01a9730a46d7d46b76851c0c0446c730e072c5970c6a732f2be4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3008201e-df95-4390-8b75-c5f47500a460", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.65, "x1": 432.01, "y1": 426.26}]}, "hash": "58c17d03ea5958f20650c11e960bde5eb26952d9fe5cc31f207967f3b488070e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " 'content': 'You are a helpful assistant who will convert the given text into text that is understandable by a freshman college student.'}, \n {'role': 'user', \n 'content': 'Neutralize the tone and style from the following legal text and express it for a nonlegal audience: \u201cThe plaintiff hereby moves for summary judgment pursuant to Rule 56(c), asserting that the defendant's aforementioned conduct constitutes a material breach of the contractual obligations set forth**in**Exhibit A, thus entitling the moving party to compensatory damages**as**a matter of law.\u201d'}] <br><br>You\u2019ll notice a number of legal terms, like plaintiff, material breach, and summary \njudgment, as well as specific stylistic elements, like hereby moves for, aforementioned \nconduct constitutes, and thus entitling. Notice also that the prompt\u2019s system instruction \nspecifies that the LLM should rephrase the text in \u201clanguage suitable for a college \nstudent.\u201d This will elicit plain English, which will be our neutral form. <br><br>When you ask the LLM to neutralize this highly specific legal text, you\u2019ll get a \nresponse similar to the following: <br><br>Output: The person suing is asking the court to decide the case without a full trial. \nThey say that the other person broke the contract listed in Exhibit A, and they should \nget money to make up for it. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3008201e-df95-4390-8b75-c5f47500a460": {"__data__": {"id_": "3008201e-df95-4390-8b75-c5f47500a460", "embedding": null, "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.65, "x1": 432.01, "y1": 426.26}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dc2c83c2-eb1d-4288-846d-c36e466830e8", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 435.84, "x1": 1823.0, "y1": 606.65}]}, "hash": "0f3e8572d5dbacd45b09ef9acd7c0ace4639920e01eca6405ad9e208c96ba814", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faec23ef-4523-436e-88d4-ab359d43721b", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 137.55}]}, "hash": "3de92a7112ee428eb85702351f00ea3dce0b8781883222eb8cdaf76f00729fc5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notice that this preserves the meaning while neutralizing distinctive stylistic elements \nlike the complex vocabulary, formal tone, and elaborate structure. For example, the \nmodel replaces plaintiff with person and material breach with broke the contract. The \nneutral version of the text preserves the core content of someone wanting a court \ndecision without going to trial. <br><br>Very importantly, the neutral content is rather generic and can be generated by an \nLLM. So, if you want to take your Lufthansa complaint and make it a legal action, you \ncan use a fine-tuned LLM that has been trained on the reversed inputs and outputs of \nthis process. In short, the foundational model will generate a complaint letter in plain \nEnglish, and the fine-tuned model will convert it into legalese. <br><br>**Example 2: Personal style**\nLet\u2019s build an end-to-end example of using Reverse Neutralization for personaliza\u2010 \ntion. Here again, you have a lot of content in your personal style (the desired output), \nbut you don\u2019t have an input to use as training for the fine-tuning operation. <br><br>**Step 1: Neutralization**<br><br>In the first step, you need to generate the input for your fine-tuning process. Let\u2019s say \nyou have a number of old emails that reflect your tone and style, and you want to \ngenerate a neutral representation of that. To clearly show the effect of personalization, \nwe\u2019re going to use a set of emails that\u2019s a bit over-the-top in terms of personal style. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faec23ef-4523-436e-88d4-ab359d43721b": {"__data__": {"id_": "faec23ef-4523-436e-88d4-ab359d43721b", "embedding": null, "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 137.55}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3008201e-df95-4390-8b75-c5f47500a460", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.65, "x1": 432.01, "y1": 426.26}]}, "hash": "58c17d03ea5958f20650c11e960bde5eb26952d9fe5cc31f207967f3b488070e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93cee7f8-9a47-4f46-8220-64eaf1a16c9d", "node_type": "1", "metadata": {"bbox": [{"page": 96, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.88, "x1": 2120.5, "y1": 254.05}]}, "hash": "496230b1061d28e77e5feae98867b3053b2587534d3569e6acb9a81ccff917b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first step is to use an LLM to generate neutral versions of these highly personal\u2010 \nized emails. You can do so by using the following prompt: <br><br>Neutralize the tone and style from the following email to make it professional and \nsuitable for communication between executives who may not know each other very \nwell. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93cee7f8-9a47-4f46-8220-64eaf1a16c9d": {"__data__": {"id_": "93cee7f8-9a47-4f46-8220-64eaf1a16c9d", "embedding": null, "metadata": {"bbox": [{"page": 96, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.88, "x1": 2120.5, "y1": 254.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "faec23ef-4523-436e-88d4-ab359d43721b", "node_type": "1", "metadata": {"bbox": [{"page": 95, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 137.55}]}, "hash": "3de92a7112ee428eb85702351f00ea3dce0b8781883222eb8cdaf76f00729fc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9569bafb-0029-408d-beb1-6942471d4387", "node_type": "1", "metadata": {"bbox": [{"page": 97, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.65, "x1": 432.0, "y1": 388.04}]}, "hash": "3486d2c61d8bbecf99b6323c21b5db9dfdf0b848bfc1bd8eb3fb9e5585cdd2bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 2: Dataset creation**<br><br>After generating the neutral version via an LLM, you can flip the results from the \noutput and generate training pairs, like the following example does. <br><br>The data structure changes with the model provider, but the general concept remains \nunchanged: you pair the system prompt and user prompt (the inputs) with your \nexpected assistant response (the output). <br><br>Here\u2019s an example of such a pair using the OpenAI fine-tuning API: <br><br>{\"messages\": [ \n {\"role\": \"system\", \n \"content\": \"You are a helpful assistant converting the neutralized email into personalized email.\"}, <br><br> { \n \"role\": \"user\", \n \"content\": \"Subject: Welcome to the Team\\n\\nHi Emily,\\n\\nI would like to extend a warm welcome to you as a new member of the Customer Success team. We are looking forward to your contributions and are confident that your skills and experience will be valuable to our objectives.\\n\\nPlease know that you have the support of the team as you transition into your new role. \\n\\nI look forward to working together.\\n\\nBest regards,\\n\\nCasey Brown \\nCustomer Success Team Lead\" ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9569bafb-0029-408d-beb1-6942471d4387": {"__data__": {"id_": "9569bafb-0029-408d-beb1-6942471d4387", "embedding": null, "metadata": {"bbox": [{"page": 97, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.65, "x1": 432.0, "y1": 388.04}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "93cee7f8-9a47-4f46-8220-64eaf1a16c9d", "node_type": "1", "metadata": {"bbox": [{"page": 96, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.88, "x1": 2120.5, "y1": 254.05}]}, "hash": "496230b1061d28e77e5feae98867b3053b2587534d3569e6acb9a81ccff917b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19a0da94-ad15-4e9e-953b-ed0b016f47fc", "node_type": "1", "metadata": {"bbox": [{"page": 98, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.22, "x1": 432.0, "y1": 227.9}]}, "hash": "47e4f4217f5efc3909a5b4af0d07d1f5e7c6554d76eca961db828fcb64b4ad9a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can try out the model by sending it a neutral email: <br><br>Subject: Request for Project Timeline Update <br><br>Body: Hi Sam, <br><br>I am writing to request an update on the project timeline. Please provide the update by \nthe end of the day, as it is important for our upcoming steps. <br><br>Thank you. <br><br>Best, <br><br>Alex <br><br>The response is in Alex\u2019s personal style: <br><br>Subject:   Exciting Update: Project Timeline Request! <br><br>Body: Hi Sam, <br><br> I\u2019m absolutely thrilled about the \nI hope this message finds you well and energized! \nfantastic progress we\u2019re making on our project! I just couldn\u2019t resist popping in to ask \nfor a quick update on our timeline. Your insight and planning have been invaluable, \nand I can\u2019t wait to see what\u2019s next on our journey to success! <br><br>Looking forward to hearing from you soon! Keep shining! <br><br>Warm regards, <br><br>Alex <br><br>**Step 4: Inference**<br><br>Now, let\u2019s do the end-to-end inference to generate a personal email on a topic that\u2019s \nnot in the training dataset. You can find the full version of this example on GitHub. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19a0da94-ad15-4e9e-953b-ed0b016f47fc": {"__data__": {"id_": "19a0da94-ad15-4e9e-953b-ed0b016f47fc", "embedding": null, "metadata": {"bbox": [{"page": 98, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.22, "x1": 432.0, "y1": 227.9}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9569bafb-0029-408d-beb1-6942471d4387", "node_type": "1", "metadata": {"bbox": [{"page": 97, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.65, "x1": 432.0, "y1": 388.04}]}, "hash": "3486d2c61d8bbecf99b6323c21b5db9dfdf0b848bfc1bd8eb3fb9e5585cdd2bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e5a6d24-5167-47b9-92fe-673daafda8df", "node_type": "1", "metadata": {"bbox": [{"page": 99, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.96, "x1": 432.01, "y1": 296.26}]}, "hash": "29635c0a630ed7f2e0930c911a5816a09081e85fe33a1852c94068fbcbfb0af8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, you can convert the neutral email into your personal style by using the fine- \ntuned LLM. The personalized output will look like this: <br><br>Subject: \nWorld Cup! \n Exciting Opportunity: Unleash Your Marketing Magic at the 2026 FIFA <br><br>Hi Gretl! <br><br>I hope this message finds you in fantastic spirits! \n I am absolutely thrilled to invite \nyou to present on our electrifying marketing campaign for the 2026 FIFA World Cup! \nYour creativity and insights are like a burst of sunshine, and I can\u2019t wait to see the \nmagic you\u2019ll bring to this spectacular event! <br><br>Your innovative perspective is just what we need to make this campaign unforgettable. \nImagine the impact we can create together! Your presentation will be the highlight of \nour team\u2019s year, and I\u2019m beyond excited to hear your thoughts and ideas! ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e5a6d24-5167-47b9-92fe-673daafda8df": {"__data__": {"id_": "3e5a6d24-5167-47b9-92fe-673daafda8df", "embedding": null, "metadata": {"bbox": [{"page": 99, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.96, "x1": 432.01, "y1": 296.26}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "19a0da94-ad15-4e9e-953b-ed0b016f47fc", "node_type": "1", "metadata": {"bbox": [{"page": 98, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.22, "x1": 432.0, "y1": 227.9}]}, "hash": "47e4f4217f5efc3909a5b4af0d07d1f5e7c6554d76eca961db828fcb64b4ad9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4df85add-8ee4-4596-a602-a850bd5dcdb7", "node_type": "1", "metadata": {"bbox": [{"page": 100, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.19, "x1": 432.01, "y1": 329.44}]}, "hash": "c7a90a094d363d4b0ab4a523d81e966544fe9f3819d3416e6f0c7ab4eeecd12b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One way to evaluate this is by using the embeddings of the original text and the \ngenerated neutral text. Their semantic meanings should be highly similar, so the \ncosine similarity of the two embeddings should also be close. Of course, you should \nuse this with care\u2014the closest embedding is the one that doesn\u2019t change the text in \nany way, but such a transformation will make it hard for the LLM to generate text in \nthe desired style. <br><br>Sometimes, the content is highly intertwined with the style, and neutralizing the style \nmay mean also losing informational content. So, you should check the neutralization \nresults against the styled input to make sure the content and intention are unchanged. \nIn addition, neutralizing the text too much can cause it to lose clarity, which is known \nas over-neutralization. <br><br>In practice, you might have to experiment with various neutral forms and evaluate \nhow well they preserve content during neutralization as well as how well they gener\u2010 \nate the desired style on unseen topics. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4df85add-8ee4-4596-a602-a850bd5dcdb7": {"__data__": {"id_": "4df85add-8ee4-4596-a602-a850bd5dcdb7", "embedding": null, "metadata": {"bbox": [{"page": 100, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.19, "x1": 432.01, "y1": 329.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3e5a6d24-5167-47b9-92fe-673daafda8df", "node_type": "1", "metadata": {"bbox": [{"page": 99, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 109.96, "x1": 432.01, "y1": 296.26}]}, "hash": "29635c0a630ed7f2e0930c911a5816a09081e85fe33a1852c94068fbcbfb0af8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cda2accd-db5d-495b-8dd5-994a90561bee", "node_type": "1", "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 322.64, "x1": 432.0, "y1": 605.54}]}, "hash": "9403f72e59252854cec5577afe7d755e43eecff6258e100492fc59b93402a937", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nThis pattern is analogous to back translation, which has long been used to expand \ndatasets (Beddiar, Jahan, and Oussalah, 2021) in the field of machine translation. \nFor example, if you have an English-to-French translation model, you can reverse \nthe inputs and outputs to create inputs for a French-to-English translation model. \nEdunov et al. (2018) discuss the use of synthetic source sentences in back translation. <br><br>**Pattern 5: Content Optimization**\nContent Optimization is a pattern that uses preference tuning to produce optimally \nperforming content. Preference tuning is a way of training an LLM on a dataset of \npaired items so that it outputs responses that are closer to the chosen items than \nthe rejected ones. Unlike optimization methods, in which optimization attempts to \napproximate an independent source of truth, this pattern attempts to hack the model \nto create content the evaluation method will consider great. <br><br>**Problem**\nContent is created to serve some purpose: to educate, entertain, or drive sales. So, \nwhen you create content, you want to do so in the style that best serves that purpose. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cda2accd-db5d-495b-8dd5-994a90561bee": {"__data__": {"id_": "cda2accd-db5d-495b-8dd5-994a90561bee", "embedding": null, "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 322.64, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4df85add-8ee4-4596-a602-a850bd5dcdb7", "node_type": "1", "metadata": {"bbox": [{"page": 100, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.19, "x1": 432.01, "y1": 329.44}]}, "hash": "c7a90a094d363d4b0ab4a523d81e966544fe9f3819d3416e6f0c7ab4eeecd12b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7ef32ee-70fd-440a-b41c-da9209aa76fd", "node_type": "1", "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.83, "x1": 432.01, "y1": 311.07}]}, "hash": "ebb062808cad0425f7c6105df012a3273c27043c7614a30f384e733ae6aba86f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If multiple factors affect the learning outcome, you can use a multiarmed bandit to \ntest all possible variations efficiently. <br><br>The drawback of the traditional A/B testing approach is that you need to know how \nStyles A and B differ, so that you can then always create content in the winning style. \nIf you don\u2019t have a hypothesis about the set of factors to test, it will be impossible to \nuse traditional A/B tests. The problem is threefold: <br><br>Indistinguishable sets <br><br>If you have no hypothesis of the factors that make a difference, there\u2019s nothing to \ndifferentiate Set A from Set B. <br><br>Indeterminate test <br><br>If the two sets are indistinguishable, it\u2019s likely that your test will never yield \nstatistically significant differences. <br><br>Inability to use results <br><br>Even if, by chance, you get to a point where it\u2019s clear that Set A is statistically \nbetter than Set B, how should you use the results? If you don\u2019t know what \ndifferentiates Set A from Set B, how can you reliably generate content in the \nwinning style? <br><br>Content Optimization is a design pattern for generating content in the style that \noffers the best performance, even when you don\u2019t know the factors that go into that \n\u201cwinning\u201d style. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7ef32ee-70fd-440a-b41c-da9209aa76fd": {"__data__": {"id_": "c7ef32ee-70fd-440a-b41c-da9209aa76fd", "embedding": null, "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.83, "x1": 432.01, "y1": 311.07}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cda2accd-db5d-495b-8dd5-994a90561bee", "node_type": "1", "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 322.64, "x1": 432.0, "y1": 605.54}]}, "hash": "9403f72e59252854cec5577afe7d755e43eecff6258e100492fc59b93402a937", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a53db9c-2191-498a-81a1-1dc8ac06d6bb", "node_type": "1", "metadata": {"bbox": [{"page": 103, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 280.44, "x1": 432.0, "y1": 334.74}]}, "hash": "d69546483403be1966a77d1030d38d637d5a347ee58073b0ddd0faf3a75d4701", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nStyle optimization with this pattern is sheer jujitsu. In Content Optimization, you \nreframe or redefine the three problems with the traditional A/B testing approach to \nmake them go away: <br><br>Indistinguishable sets <br><br>If you don\u2019t know what style factors make the most difference, then the elements \nof sets A and B will be indistinguishable. In Content Optimization, you get \naround this problem by comparing just two pieces of content at a time and \ndefining Set A to be the content that \u201cwins\u201d the pair comparison and Set B to be \nthe content that doesn\u2019t. <br><br>Indeterminate test <br><br>You define a \u201ctest\u201d as just a single comparison and drop the need for the test itself \nto be statistically significant. The two content pieces have to be comparable, of \ncourse, so they should be created by the same LLM in response to prompts that \nare nearly the same. (We\u2019ll assume we\u2019re dealing with text for now, although this \napproach generalizes to images.) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a53db9c-2191-498a-81a1-1dc8ac06d6bb": {"__data__": {"id_": "6a53db9c-2191-498a-81a1-1dc8ac06d6bb", "embedding": null, "metadata": {"bbox": [{"page": 103, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 280.44, "x1": 432.0, "y1": 334.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c7ef32ee-70fd-440a-b41c-da9209aa76fd", "node_type": "1", "metadata": {"bbox": [{"page": 102, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.83, "x1": 432.01, "y1": 311.07}]}, "hash": "ebb062808cad0425f7c6105df012a3273c27043c7614a30f384e733ae6aba86f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "868ccb0e-b94b-4803-ab4b-deae42d35cdf", "node_type": "1", "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.64, "x1": 432.0, "y1": 605.54}]}, "hash": "70ad1f121cd23be03ac33b73f2675903849b006587affab1159cb63921671242", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first three steps have to do with creating a preference dataset, and the fourth \ninvolves using direct preference optimization (DPO) to train an LLM to produce \ndocuments in a preferred style. <br><br>Let\u2019s look at each of these steps. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "868ccb0e-b94b-4803-ab4b-deae42d35cdf": {"__data__": {"id_": "868ccb0e-b94b-4803-ab4b-deae42d35cdf", "embedding": null, "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.64, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6a53db9c-2191-498a-81a1-1dc8ac06d6bb", "node_type": "1", "metadata": {"bbox": [{"page": 103, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 280.44, "x1": 432.0, "y1": 334.74}]}, "hash": "d69546483403be1966a77d1030d38d637d5a347ee58073b0ddd0faf3a75d4701", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7d00da-f76b-49d6-98c5-ca0fe7f2dd4b", "node_type": "1", "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.05, "x1": 432.01, "y1": 293.07}]}, "hash": "723b63ec65813f9db7c5887e6e71bf275d57ec89951a1fcc616fd3f9f7cb8445", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate this, we sent the following prompt to Mistral-7B-Instruct-v0.2 twice with \na nonzero temperature: <br><br>Where does the term \u201cknee-jerk reaction\u201d come from? <br><br>The first time, the response that came back started with this: <br><br>The term \u201cknee-jerk reaction\u201d refers to an immediate, often unreflective response to a \nstimulus. It comes from the medical procedure used to test the reflexes in the knee. <br><br>The second time, the response started with this: <br><br>The term \u201cknee-jerk reaction\u201d comes from the medical reflex test where the knee \njerks up when the patellar tendon is tapped. This reflex is an automatic, unreflective \nresponse to a stimulus. <br><br>This prompt asks for a factual answer, so the responses should vary less than most\u2014 \nyet their styles vary quite dramatically. The first seems to be targeting a layperson, \nwhile the second is addressed to someone who knows what the patellar tendon is. <br><br>**Changing the generation settings.**You can change settings, such as the temperature and \ntop-P, to get two pieces of content that respond to the same prompt: <br><br>paired_content = [] \n**for**iter**in**range(2): \nresponse = pipe(input_message, \n temperature=random.uniform(0.2, 0.9), \n ) \npaired_content.append(response[0]['generated_text'][-1]['content']) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b7d00da-f76b-49d6-98c5-ca0fe7f2dd4b": {"__data__": {"id_": "3b7d00da-f76b-49d6-98c5-ca0fe7f2dd4b", "embedding": null, "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.05, "x1": 432.01, "y1": 293.07}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "868ccb0e-b94b-4803-ab4b-deae42d35cdf", "node_type": "1", "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.64, "x1": 432.0, "y1": 605.54}]}, "hash": "70ad1f121cd23be03ac33b73f2675903849b006587affab1159cb63921671242", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "788d0fcc-c201-4a1e-bb3f-4d0f3d3757de", "node_type": "1", "metadata": {"bbox": [{"page": 106, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.28, "x1": 432.0, "y1": 311.93}]}, "hash": "a686f41af715ee03cece54ccba85365cc9d635a0ab938193a7bf32ed68e83156", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Prompt rewriting.**You can have an LLM reword the original prompt in arbitrary \nways. For example, you might instruct the LLM to do this: <br><br>Rephrase this question at a grade school reading level: \u201cWhere does the term \u201cknee- \njerk reaction\u201d come from?\u201d <br><br>When we did this with Mistral-7B-Instruct-v0.2, we got this alternate prompt: <br><br>What\u2019s the origin of the phrase \u201cknee-jerk reaction\u201d? <br><br>Note that you are rewriting the prompt\u2014not rephrasing the content at a grade school \nreading level! One idea for rewriting prompts without changing their intent is asking \nthe LLM to make the prompt more concise or verbose. <br><br>Another powerful prompt-rewriting technique is to ask the LLM to add an appropri\u2010 \nate follow-on question to the prompt you provide. For example, we asked Mistral this \nquestion: <br><br>What is an appropriate follow-on question to this query: \u201cWhere does the term \n\u2018knee-jerk reaction\u2019 come from?\u201d ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "788d0fcc-c201-4a1e-bb3f-4d0f3d3757de": {"__data__": {"id_": "788d0fcc-c201-4a1e-bb3f-4d0f3d3757de", "embedding": null, "metadata": {"bbox": [{"page": 106, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.28, "x1": 432.0, "y1": 311.93}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3b7d00da-f76b-49d6-98c5-ca0fe7f2dd4b", "node_type": "1", "metadata": {"bbox": [{"page": 105, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.05, "x1": 432.01, "y1": 293.07}]}, "hash": "723b63ec65813f9db7c5887e6e71bf275d57ec89951a1fcc616fd3f9f7cb8445", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67d3617b-b79c-41c7-978e-ed896d62ccd2", "node_type": "1", "metadata": {"bbox": [{"page": 107, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.18, "x1": 432.01, "y1": 358.79}]}, "hash": "d6eafb4959390a58687b6e0279ff147500de504190eb1ac082395cfc9a548014", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Variant 1: Human labeling.**Show the two pieces of content to a human expert and \nask them to choose the one that\u2019s better. If you\u2019re in an application area where \nexperts tend to disagree, you can show them to a panel of experts and choose the \nmajority opinion, or you can discard pairs where the decision is not clear-cut. In \nsome situations, you can even show the two pieces of content side by side as drafts \nand see which one your end users actually select. <br><br>**Variant 2: Using an evaluator.**If your industry has metrics for content quality, you can \nscore the two pieces of content on a rubric that consists of one or a combination of \nthose metrics. For example, if you\u2019re generating SQL statements, your evaluator might \nconsist of a sandbox that runs those SQL statements on an in-memory database and \nrecords the output and the time taken to generate them. You might then choose \nthe statement that returns the correct result, or if both statements return the correct \nresult, you could choose the response that is more concise. If both statements are the \nsame length, choose the one that runs faster. <br><br>Similarly, if your industry has best practices, you might evaluate the content on \nits conformance with those best practices. For example, marketing content is com\u2010 \nmonly evaluated on the 4Ps and 3Cs framework. A frontier LLM might already have ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67d3617b-b79c-41c7-978e-ed896d62ccd2": {"__data__": {"id_": "67d3617b-b79c-41c7-978e-ed896d62ccd2", "embedding": null, "metadata": {"bbox": [{"page": 107, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.18, "x1": 432.01, "y1": 358.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "788d0fcc-c201-4a1e-bb3f-4d0f3d3757de", "node_type": "1", "metadata": {"bbox": [{"page": 106, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.28, "x1": 432.0, "y1": 311.93}]}, "hash": "a686f41af715ee03cece54ccba85365cc9d635a0ab938193a7bf32ed68e83156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd4bdda6-4072-43a9-95c0-845d5898a7b6", "node_type": "1", "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.41, "x1": 981.5, "y1": 607.79}]}, "hash": "7b2ef8126da98d122292c99860622837f8f08e2d433d2bb59e2f766f171bffb9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Variant 3: Choosing the item that provides a better outcome.**A third option for determin\u2010 \ning the winner is to use the actual outcome associated with each piece of content. \nThere are a couple of ways to do this, depending on the problem: <br><br>Direct measurement <br><br>Suppose you are creating content for a newsletter. You might push one piece of \ncontent to half of your user base and the other piece to the other half. Then, you \ncould measure the number of people who perform the call to action, whether it\u2019s \nto click on a link or to sign a petition. The piece of generated content that gathers \nmore clicks or more signatures is the one you\u2019ll choose. <br><br>Matching prompts <br><br>Suppose you\u2019re generating answers to user queries in an automated ticket- \nhandling system. You can identify semantically similar user queries and pair \nthem up. For each matched pair of queries, compare the outcome associated \nwith the corresponding content, such as how quickly the issue was resolved. The \ncontent that leads to a faster resolution is the winner. <br><br>At the end of Step 2, you\u2019ll have a way to compare two pieces of content that were \ngenerated in response to the same or a similar prompt and declare one of them the \nwinner. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd4bdda6-4072-43a9-95c0-845d5898a7b6": {"__data__": {"id_": "dd4bdda6-4072-43a9-95c0-845d5898a7b6", "embedding": null, "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.41, "x1": 981.5, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "67d3617b-b79c-41c7-978e-ed896d62ccd2", "node_type": "1", "metadata": {"bbox": [{"page": 107, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.18, "x1": 432.01, "y1": 358.79}]}, "hash": "d6eafb4959390a58687b6e0279ff147500de504190eb1ac082395cfc9a548014", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00506daa-8deb-4d03-9aa5-78e3d043e0f3", "node_type": "1", "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.01, "x1": 432.0, "y1": 320.31}]}, "hash": "1a2b1bb7652ca35f5b077903822245179a998fe803d60f600aa7febba849429c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 3: Create a training dataset**<br><br>Create a training dataset in which each example consists of three attributes: a prompt \n(use the original prompt if you did prompt rewriting), a chosen output, and a rejected \noutput. For example, create it this way: <br><br>{ \n \"prompt\": \"Where does the term**\\\"**knee-jerk reaction**\\\"**come from?\", \n \"chosen\": \"The term**\\\"**knee-jerk reaction**\\\"**refers to an immediate, often unreflective response to a stimulus. It comes from the medical procedure used to test the reflexes in the knee.\", \n \"rejected\": \"The term**\\\"**knee-jerk reaction**\\\"**comes from the medical reflex test where the knee jerks up when the patellar tendon is tapped. This reflex is an automatic, unreflective response to a stimulus.\", \n} <br><br>This is a standard format for datasets used in preference tuning, which is what we\u2019ll \ndo next. If your preference-tuning framework expects a different format, you\u2019ll have \nto create your training dataset in that format, of course. <br><br>You might also split this dataset into separate training and evaluation datasets if you \nplan to do early stopping. <br><br>**Step 4: Do preference tuning**<br><br>The original way to do preference tuning, which was introduced in 2017, was to use \nreinforcement learning. However, we recommend the direct preference optimization \n(DPO) approach, as described in the paper \u201cDirect Preference Optimization: Your \nLanguage Model Is Secretly a Reward Model\u201d by Raifalov et al. in 2023. The DPO \napproach is much faster, for reasons that are beyond the scope of this book. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00506daa-8deb-4d03-9aa5-78e3d043e0f3": {"__data__": {"id_": "00506daa-8deb-4d03-9aa5-78e3d043e0f3", "embedding": null, "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.01, "x1": 432.0, "y1": 320.31}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dd4bdda6-4072-43a9-95c0-845d5898a7b6", "node_type": "1", "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.41, "x1": 981.5, "y1": 607.79}]}, "hash": "7b2ef8126da98d122292c99860622837f8f08e2d433d2bb59e2f766f171bffb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee5dd382-a261-48db-8ebe-557b84e7345c", "node_type": "1", "metadata": {"bbox": [{"page": 110, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 84.23, "x1": 432.0, "y1": 306.58}]}, "hash": "06bbf57bbc931105d078c0d8b906c814d5d7814b105333cdc80c251a0f7142fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Transformer Reinforcement Learning (TRL) package provides a DPO imple\u2010 \nmentation. To train a model using DPO, first instantiate your model and a tokenizer \nfor it: <br><br>MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID) \ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID) <br><br>Then, launch the training by using this code: <br><br>training_args = DPOConfig(output_dir=\"Qwen-DPO\") \ntrainer = DPOTrainer(model=model, args=training_args, \nprocessing_class=tokenizer, train_dataset=train_dataset) \ntrainer.train() <br><br>Once the training is complete, save the model: <br><br>trainer.save_model(training_args.output_dir) <br><br>You can deploy the trained model or load it into a Transformer pipeline from local \nstorage and use it in lieu of the original model. This model will produce output in the \nstyle it learned from the set of chosen and rejected responses. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee5dd382-a261-48db-8ebe-557b84e7345c": {"__data__": {"id_": "ee5dd382-a261-48db-8ebe-557b84e7345c", "embedding": null, "metadata": {"bbox": [{"page": 110, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 84.23, "x1": 432.0, "y1": 306.58}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "00506daa-8deb-4d03-9aa5-78e3d043e0f3", "node_type": "1", "metadata": {"bbox": [{"page": 109, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 104.01, "x1": 432.0, "y1": 320.31}]}, "hash": "1a2b1bb7652ca35f5b077903822245179a998fe803d60f600aa7febba849429c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3edaad65-0949-49c5-a4cc-c39b19aecaa4", "node_type": "1", "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 108.0, "y0": 212.19, "x1": 414.01, "y1": 368.25}]}, "hash": "0db7aae0b8b74a34adeee9fa278bb6125bfe502b221cf79b8c8f00cfa32e34f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is quite obviously a bad ad. It sounds as if the ad is for a new bike from the \nmanufacturer, whereas we wanted a for-sale-by-owner ad to sell a three-year-old bike. <br><br>The model is too small to generate good ads, but, as you\u2019ll see, we can use the Content \nOptimization pattern to train this model to produce good ad copy. <br><br>**Step 1: Generate pairs of ads**<br><br>The first step is to generate pairs of ads. Let\u2019s change the generation settings, choosing \na random temperature each time. <br><br>We got two ads for the $300 bike when we did this. Here\u2019s the first one: <br><br>Hey there! We\u2019re looking for someone who\u2019s ready to take their riding game to the \nnext level with our 3-year-old Specialized bike. This bike is a great investment that \nwill keep your kids engaged and safe all year round. It features durable components, a \ncomfortable seat, and a powerful frame that can handle any terrain. Plus, it comes with \na lifetime warranty, so you can be sure you\u2019re getting a high-quality product. So why \nwait? Get yours today and start enjoying the thrill of riding on wheels! <br><br>And here\u2019s the second one: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3edaad65-0949-49c5-a4cc-c39b19aecaa4": {"__data__": {"id_": "3edaad65-0949-49c5-a4cc-c39b19aecaa4", "embedding": null, "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 108.0, "y0": 212.19, "x1": 414.01, "y1": 368.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ee5dd382-a261-48db-8ebe-557b84e7345c", "node_type": "1", "metadata": {"bbox": [{"page": 110, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 84.23, "x1": 432.0, "y1": 306.58}]}, "hash": "06bbf57bbc931105d078c0d8b906c814d5d7814b105333cdc80c251a0f7142fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "917b5be9-4e8e-4990-8159-c20ccec54095", "node_type": "1", "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.42, "x1": 432.01, "y1": 189.6}]}, "hash": "d8fc7ba0fd74e3eae8926a55debcd9d8f988f221b7b2bd47a2bdfc63048df3c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Is it clear what\u2019s being sold? Age, brand, price, and condition are important. <br><br>*Does it target the most relevant audience for the item? Is the text persuasive to \nthat audience? <br><br>*Is it concise and easy to read? An ideal ad is at most three sentences. <br><br>*Does it include contact information? Ideally, the ad specifies the preferred means \nof communication. <br><br>*Is the ad truthful? Remember that the item is likely used and not being sold by \nthe manufacturer. <br><br>ad_a: <br><br>{ad_a} <br><br>ad_b: <br><br>{ad_b} ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "917b5be9-4e8e-4990-8159-c20ccec54095": {"__data__": {"id_": "917b5be9-4e8e-4990-8159-c20ccec54095", "embedding": null, "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.42, "x1": 432.01, "y1": 189.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3edaad65-0949-49c5-a4cc-c39b19aecaa4", "node_type": "1", "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 108.0, "y0": 212.19, "x1": 414.01, "y1": 368.25}]}, "hash": "0db7aae0b8b74a34adeee9fa278bb6125bfe502b221cf79b8c8f00cfa32e34f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79fcd7f9-3c4f-4987-8844-23a46b6e76fe", "node_type": "1", "metadata": {"bbox": [{"page": 112, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.54, "x1": 458.75, "y1": 331.44}]}, "hash": "23acea2f613a023c9fd20f2056f4880811944c32b03f42c989462b112643753d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By optimizing to this evaluation, you\u2019re saying that you don\u2019t care what style the ads \nare written in as long as they\u2019re clear, relevant to the audience, short, actionable, and \ntruthful. If writing the ads in rhyme is what makes the bike sell, you are perfectly OK \nwith the LLM choosing to output rhymes. <br><br>Asking for the reason forces the LLM to do the comparison deliberatively (see Chain \nof Thought [Pattern 13] in Chapter 5). The output can be in this form: <br><br>@dataclass \n**class AdsComparison**: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79fcd7f9-3c4f-4987-8844-23a46b6e76fe": {"__data__": {"id_": "79fcd7f9-3c4f-4987-8844-23a46b6e76fe", "embedding": null, "metadata": {"bbox": [{"page": 112, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.54, "x1": 458.75, "y1": 331.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "917b5be9-4e8e-4990-8159-c20ccec54095", "node_type": "1", "metadata": {"bbox": [{"page": 111, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.42, "x1": 432.01, "y1": 189.6}]}, "hash": "d8fc7ba0fd74e3eae8926a55debcd9d8f988f221b7b2bd47a2bdfc63048df3c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c09a772c-12ac-4ffe-847a-52f0ae58c353", "node_type": "1", "metadata": {"bbox": [{"page": 113, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.17, "x1": 432.0, "y1": 283.74}]}, "hash": "a5363378155bda7a3d07d82bd0fd2ee617ab2b052b5bbee7e9ef34d15a52b756", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " preference_example = { \n \"prompt\": SYSTEM_PROMPT + f\"\"\"Write an ad to sell a {item} priced at {price}\"\"\" \n } <br><br>**if**score.ad_a_is_better_than_ad_b: \n preference_example['chosen'] = ad1 \n preference_example['rejected'] = ad2 \n**else**: \n preference_example['chosen'] = ad2 \n preference_example['rejected'] = ad1 \n preference_example['score_reason'] = score.reasoning <br><br>**return**preference_example <br><br>Then, call this in a loop with a random selection from a set of items to create a JSONL \nfile: <br><br>items_for_sale = [ \n (\"3-year old Specialized road bike\", \"$300\"), \n (\"Amazing Spider-Man 361\", \"$200\"), \n \u2026 \n (\"Set of 2 vintage Pyrex coffee mugs (in original box)\", \"$20\"), \n (\"Hand-carved wooden serving spoon\", \"$15\") \n] <br><br>def write_jsonl(num_examples: int, filename: str): ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c09a772c-12ac-4ffe-847a-52f0ae58c353": {"__data__": {"id_": "c09a772c-12ac-4ffe-847a-52f0ae58c353", "embedding": null, "metadata": {"bbox": [{"page": 113, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.17, "x1": 432.0, "y1": 283.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "79fcd7f9-3c4f-4987-8844-23a46b6e76fe", "node_type": "1", "metadata": {"bbox": [{"page": 112, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.54, "x1": 458.75, "y1": 331.44}]}, "hash": "23acea2f613a023c9fd20f2056f4880811944c32b03f42c989462b112643753d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b3370d-544d-42ee-8290-bdb711cad978", "node_type": "1", "metadata": {"bbox": [{"page": 114, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.09, "x1": 432.0, "y1": 276.18}]}, "hash": "c68da43c42000d23e1b68e94c5a28d500abb49fce69f28838ab04f89af133a1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Inference**<br><br>You can create a pipeline with the local trained model, just as if the model were on \nthe Hugging Face hub: <br><br>pipe = pipeline( \n task=\"text-generation\", \n model=\"ClassifiedAds-DPO\", \n use_fast=**True**, \n kwargs={ \n \"return_full_text\":**False**, \n }, \n model_kwargs={} \n) <br><br>Then, create new content, using the same prompt as before: <br><br>**def**create_classified_ad(item: str, price: str) -> str: \n system_prompt = SYSTEM_PROMPT \n user_prompt = f\"\"\" \n Write an ad to sell a {item} priced at {price} ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10b3370d-544d-42ee-8290-bdb711cad978": {"__data__": {"id_": "10b3370d-544d-42ee-8290-bdb711cad978", "embedding": null, "metadata": {"bbox": [{"page": 114, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.09, "x1": 432.0, "y1": 276.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c09a772c-12ac-4ffe-847a-52f0ae58c353", "node_type": "1", "metadata": {"bbox": [{"page": 113, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.17, "x1": 432.0, "y1": 283.74}]}, "hash": "a5363378155bda7a3d07d82bd0fd2ee617ab2b052b5bbee7e9ef34d15a52b756", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4963c118-1a02-455b-89f0-7eb9d077910f", "node_type": "1", "metadata": {"bbox": [{"page": 115, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 103.14, "x1": 432.0, "y1": 371.34}]}, "hash": "44100f0af90ee211dfb780b9067892bf72c2f97d93f3f5ee4595411588aca8aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Choosing between the variants**<br><br>There are several variants of the pattern that differ primarily in how you create the \npreference-tuning dataset. <br><br>In Step 1, you can choose how to generate two pieces of content in response to the \nsame prompt. While repeated generation can work, changing the settings or rewriting \nthe prompt will typically yield content that is more meaningfully different and covers \nmore of the style space. <br><br>In Step 2, you can choose how to compare the two pieces of content to determine \nwhich one is better. The approach you should take often depends on how the content \nwill be used: <br><br>\u2022 \n\u2022 For user-facing content that end users will consume when they\u2019re performing a \nwide variety of tasks, \u201cbetter\u201d is pretty much in the eye of the beholder, so human \nlabeling might be the most appropriate approach. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4963c118-1a02-455b-89f0-7eb9d077910f": {"__data__": {"id_": "4963c118-1a02-455b-89f0-7eb9d077910f", "embedding": null, "metadata": {"bbox": [{"page": 115, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 103.14, "x1": 432.0, "y1": 371.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "10b3370d-544d-42ee-8290-bdb711cad978", "node_type": "1", "metadata": {"bbox": [{"page": 114, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 86.09, "x1": 432.0, "y1": 276.18}]}, "hash": "c68da43c42000d23e1b68e94c5a28d500abb49fce69f28838ab04f89af133a1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3d35803-f1c2-4dcb-adf0-b0affb69ecb4", "node_type": "1", "metadata": {"bbox": [{"page": 116, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.14, "x1": 432.0, "y1": 334.64}]}, "hash": "cb70ff3a622dfda3a171d3d0394bf6a5ee0a81a83a773ee59ab97e05f8aa15ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\u2019s unlikely that you\u2019ll have only one option available to you, so try multiple \napproaches and see which one gives you the best results. <br><br>**The in-distribution requirement**<br><br>There are several places in this pattern where an LLM is being used: <br><br>\u2022 \n\u2022 To rephrase a prompt <br><br>\u2022 \n\u2022 To create content from a prompt <br><br>\u2022 \n\u2022 To evaluate <br><br>\u2022 \n\u2022 As input to the preference tuning <br><br>LLM-generated content (the second item on the list) has to be text that the LLM \nbeing trained, through preference tuning, (the last item on the list) can produce. This \nis called the in-distribution requirement. No amount of preference tuning can teach an \nLLM new facts or new tokens, for example. There are two ways to ensure compliance \nwith this in-distribution requirement: <br><br>The easy way <br><br>Use the same LLM in both steps. We used the Qwen2 0.5-billion-parameter \nmodel, both to generate the example content and as the model that was prefer\u2010 \nence trained. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3d35803-f1c2-4dcb-adf0-b0affb69ecb4": {"__data__": {"id_": "b3d35803-f1c2-4dcb-adf0-b0affb69ecb4", "embedding": null, "metadata": {"bbox": [{"page": 116, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.14, "x1": 432.0, "y1": 334.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4963c118-1a02-455b-89f0-7eb9d077910f", "node_type": "1", "metadata": {"bbox": [{"page": 115, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 103.14, "x1": 432.0, "y1": 371.34}]}, "hash": "44100f0af90ee211dfb780b9067892bf72c2f97d93f3f5ee4595411588aca8aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b963ba37-3438-448f-89cf-87093022a2fb", "node_type": "1", "metadata": {"bbox": [{"page": 118, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "59afeb4ce63d3b5d4ae2b852387731baffb56ffec50d614a633abf949cc54b1a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, use repeated generation (that is, just run the prompt twice) to generate two \nimages that differ in unknown ways: <br><br>**for**iter**in**range(2): \n ... \n img = pipe(prompt=prompt, generator=generator, guidance_scale=7.5).images[0] \n ... <br><br>Now comes the key step: evaluation. How is one image better than another? Assum\u2010 \ning that the images are being generated for a newsletter, you could measure the \noutcome to select the image in a pair that drives more article clicks. <br><br>With the dataset of images created in this manner, you can launch the training script \nto create a preference-tuned RunwayML Stable Diffusion model. <br><br>accelerate launch --mixed_precision=\"fp16\" train.py \\ \n --pretrained_model_name_or_path=$MODEL_NAME \\ \n --dataset_name=$DATASET_NAME \\ \n \u2026 \n --output_dir=\"tmp-sd15\" <br><br>The output of this model is now in the style that drives more clicks. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b963ba37-3438-448f-89cf-87093022a2fb": {"__data__": {"id_": "b963ba37-3438-448f-89cf-87093022a2fb", "embedding": null, "metadata": {"bbox": [{"page": 118, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b3d35803-f1c2-4dcb-adf0-b0affb69ecb4", "node_type": "1", "metadata": {"bbox": [{"page": 116, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 114.14, "x1": 432.0, "y1": 334.64}]}, "hash": "cb70ff3a622dfda3a171d3d0394bf6a5ee0a81a83a773ee59ab97e05f8aa15ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e66644d-f3d1-4b42-9fc2-a005bfab1663", "node_type": "1", "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 329.83, "x1": 432.01, "y1": 605.54}]}, "hash": "ac57cc4fca4d885aee757c793f32365cfe88e3b5f7124b89c622561d3d271ad3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it doesn\u2019t involve human labeling), then you can perform all four steps of the pattern \nquickly. <br><br>If you can perform all four steps quickly, training need not be a one-and-done pro\u2010 \ncess. You can repeat the four steps with the newly trained LLM to create a preference \ndataset where all the examples are improvements on the original (see Figure 2-14). \nThen, you can invoke the DPO trainer to obtain an LLM that is an improvement on \nthe one you used to create the content in the examples. Of course, the process will \nsaturate at some point, but such metatraining lets you maximize the quality that an \nLLM of that size is capable of achieving, at a very reasonable cost. <br><br><br><br>Figure 2-14. Using a large prompt library or a number of easily verifiable problems to do \nmultiple iterations of training <br><br>You will need a larger library of examples to do multiple training runs over your \nprompt library. Consider increasing the number of iterations, once you have a \ndeployed system and have been logging user prompts for a while. Another way to get \na larger library of examples is to train the LLM on prompts that are associated with \nmany different problems. DeepSeek-R1 burst onto the scene in early 2025 when users <br><br>**Pattern 5: Content Optimization** **|** **101**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e66644d-f3d1-4b42-9fc2-a005bfab1663": {"__data__": {"id_": "3e66644d-f3d1-4b42-9fc2-a005bfab1663", "embedding": null, "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 329.83, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b963ba37-3438-448f-89cf-87093022a2fb", "node_type": "1", "metadata": {"bbox": [{"page": 118, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "59afeb4ce63d3b5d4ae2b852387731baffb56ffec50d614a633abf949cc54b1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d35fac06-d906-4172-bb35-465861a433a1", "node_type": "1", "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.19, "x1": 427.91, "y1": 315.32}]}, "hash": "ba392a9532a927fe496264512ea26f3ffad6da58e1020739f5fccdd5307b8241", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "had an aha moment, realizing they could make substantial improvements by using \niterative DPO on synthetic data that corresponded to easily verifiable problems. You \nshould consider using the same approach if you have a number of easily verifiable \nproblems that you can use to expand the training dataset. <br><br>The important ingredients you need to set up a continuous improvement cycle are \na fast and high-quality evaluator, a systematic approach to collecting and managing \nprompts, and training until saturation is reached. <br><br>**References**\nThis pattern relies on preference tuning, but it applies the technique outside of \nfoundational model training and not necessarily on human feedback. Reinforcement \nlearning based on human feedback was introduced by Christiano et al. in 2017 \nand adapted to LLMs by Ouyang et al. in 2022. Preference tuning using DPO was \nintroduced by Raifalov et al. in 2023. <br><br>**Summary**\nThis chapter examines patterns for controlling the style of content generated by \nfoundation models. We address the challenge that model responses can be stochastic \nand variable, which makes it difficult for applications to handle diverse outputs \nconsistently. We present five key patterns, as shown in Table 2-2. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d35fac06-d906-4172-bb35-465861a433a1": {"__data__": {"id_": "d35fac06-d906-4172-bb35-465861a433a1", "embedding": null, "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.19, "x1": 427.91, "y1": 315.32}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3e66644d-f3d1-4b42-9fc2-a005bfab1663", "node_type": "1", "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 329.83, "x1": 432.01, "y1": 605.54}]}, "hash": "ac57cc4fca4d885aee757c793f32365cfe88e3b5f7124b89c622561d3d271ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bba0ea9-ca07-4019-aa8a-25b5e7d5cf1a", "node_type": "1", "metadata": {"bbox": [{"page": 122, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 192.87, "x1": 432.01, "y1": 582.33}]}, "hash": "d4f356dc6e04ec7eea90d557f878aa290174de3287a1f5efd59435bee153dc94", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2-2. Patterns for controlling style <br><br>**Patterns**\n**Logits Masking**\n**(Pattern 1)**<br><br>**Problems**\nYou need to ensure generated \ntext conforms to specific style \nrules for brand, accuracy, or \ncompliance reasons.  **Solutions**\nIntercept the generation at the \nsampling stage to zero out \nprobabilities of continuations that \ndon\u2019t meet the rules. <br><br>**Usage scenarios**\nUsing words associated with \na specific brand; avoiding \nrepeating factual information; \nmaking content compliant with \nthe style guide. \nGenerating valid SQL \ntimestamps; extracting \nstructured data in a specific \nformat; ensuring output \nconforms to JSON schema. \nRewriting generic content \nto match brand guidelines; \nconverting academic papers \nto blog posts; transforming \nimage and text content for \ndifferent social media platforms \nor audiences. <br><br>**Grammar**\n**(Pattern 2)**\nYou need text to conform to a \nspecific format or data schema \nfor downstream processing. \nSpecify rules as a formal grammar \n(such as BNF) or schema that \nthe model framework applies to \nconstrain token generation. <br><br>**Style Transfer**\n**(Pattern 3)**<br><br>You need to convert content \ninto a form that mimics specific \ntone and style that is difficult \nto express through rules but \ncan be shown through example \nconversions. <br><br>Use few-shot learning or model \nfine-tuning to teach the model \nhow to convert content into the \ndesired style. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bba0ea9-ca07-4019-aa8a-25b5e7d5cf1a": {"__data__": {"id_": "5bba0ea9-ca07-4019-aa8a-25b5e7d5cf1a", "embedding": null, "metadata": {"bbox": [{"page": 122, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 192.87, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d35fac06-d906-4172-bb35-465861a433a1", "node_type": "1", "metadata": {"bbox": [{"page": 119, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.19, "x1": 427.91, "y1": 315.32}]}, "hash": "ba392a9532a927fe496264512ea26f3ffad6da58e1020739f5fccdd5307b8241", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2a789ac-b0e2-47e9-ac6c-da03ce62bd8b", "node_type": "1", "metadata": {"bbox": [{"page": 123, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 419.24, "x1": 432.0, "y1": 492.74}]}, "hash": "386f511829c9a6a2784f8b554322fe4cc1bf4450fd0f7b91995e3f8622eeaa46", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 3**\n**Adding Knowledge: Bass**<br><br>Foundational models are closed systems that are limited by their training data. In \nmany cases, you\u2019ll need to give a foundational model additional information. For \nexample, the information may be based on recent events that had not occurred when \nthe foundational model was being trained, or the information may have been private, \nconfidential, or otherwise unavailable to the foundational model trainers. <br><br>It\u2019s impractical to retrain an LLM with additional knowledge or even perform con\u2010 \ntinuing pretraining (CPT) on a foundational model to add knowledge to it. The cost \nof even a single training run is significant, and information changes so fast that CPT \nwould have to be done very frequently. These costs can add up to tens of millions of \ndollars,1 so you\u2019ll typically want to use a foundational model as-is and add knowledge \nto it at runtime. <br><br>The key pattern that\u2019s used to provide additional information to a foundational model \nat runtime was introduced in the landmark paper \u201cRetrieval-Augmented Generation \nfor Knowledge-Intensive NLP Tasks\u201d in 2020 by researchers working at Facebook AI \nResearch (which is now part of Meta). Retrieval-augmented generation (RAG) trans\u2010 \nforms foundational models from closed systems that are limited by their training data \ninto open systems that can leverage external knowledge on demand. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2a789ac-b0e2-47e9-ac6c-da03ce62bd8b": {"__data__": {"id_": "a2a789ac-b0e2-47e9-ac6c-da03ce62bd8b", "embedding": null, "metadata": {"bbox": [{"page": 123, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 419.24, "x1": 432.0, "y1": 492.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5bba0ea9-ca07-4019-aa8a-25b5e7d5cf1a", "node_type": "1", "metadata": {"bbox": [{"page": 122, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 192.87, "x1": 432.01, "y1": 582.33}]}, "hash": "d4f356dc6e04ec7eea90d557f878aa290174de3287a1f5efd59435bee153dc94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22bf4fca-3ef4-444a-a281-2c7d2825c1a0", "node_type": "1", "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 285.1, "x1": 432.01, "y1": 607.97}]}, "hash": "197c637653dfe39446d3a7b7809ecdb7d5d98e288576d24ae4e80bf93c10c775", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That said, you won\u2019t need every single idea we present here to build your systems\u2014. \nWhich ones you\u2019ll need will depend on what your system is for. (Is it an internal \ntool meant for experts, or is it a product that millions of your customers will use?) \nIt will also depend on whether the problems being addressed by a given idea in this \nchapter are pertinent to the type of questions and documents that your RAG system \nwill encounter. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22bf4fca-3ef4-444a-a281-2c7d2825c1a0": {"__data__": {"id_": "22bf4fca-3ef4-444a-a281-2c7d2825c1a0", "embedding": null, "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 285.1, "x1": 432.01, "y1": 607.97}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a2a789ac-b0e2-47e9-ac6c-da03ce62bd8b", "node_type": "1", "metadata": {"bbox": [{"page": 123, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 419.24, "x1": 432.0, "y1": 492.74}]}, "hash": "386f511829c9a6a2784f8b554322fe4cc1bf4450fd0f7b91995e3f8622eeaa46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e873403b-3c8c-4084-a195-006e5315488b", "node_type": "1", "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.63, "x1": 432.01, "y1": 276.0}]}, "hash": "bfa36fd5be9b478c653b4a0c0d7eedd6ceb789e60578affb791a50c5afeaec6a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Pattern 6: Basic RAG**\nIn the Basic RAG pattern, a system consists of three core steps: indexing, retrieval, \nand generation. We don\u2019t really expect that you\u2019ll employ the Basic RAG pattern \ncovered in this section as-is, but the limitations of the rather simplistic implementa\u2010 \ntion provide the justification for the other patterns in this chapter. It is important to \nunderstand the reasons for and tradeoffs among the more advanced techniques so \nthat you don\u2019t overcomplicate your implementations. <br><br>**Problem**\nFoundational text generation models have been trained on a large corpus of digitally \navailable documents such as Common Crawl (which captures text from billions of \nweb pages across the internet), digital libraries and book corpora such as Books2 and \nProject Gutenberg, Wikipedia, academic repositories such as arxiv.org, code reposito\u2010 \nries such as GitHub, social media forums such as Reddit and X (formerly Twitter), \nnewspaper archives, financial filings such as Edgar, and government publications \nsuch as the European Parliament proceedings. This builds up the world knowledge of \nthe foundational model. <br><br>In many enterprise use cases, this world knowledge is insufficient for the following \nreasons: <br><br>Static knowledge cutoff <br><br>Foundational models cannot access information beyond the date their training \ndata was collected. This limitation can result in outdated or inaccurate responses, \nespecially when you\u2019re dealing with rapidly changing topics or current events. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e873403b-3c8c-4084-a195-006e5315488b": {"__data__": {"id_": "e873403b-3c8c-4084-a195-006e5315488b", "embedding": null, "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.63, "x1": 432.01, "y1": 276.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "22bf4fca-3ef4-444a-a281-2c7d2825c1a0", "node_type": "1", "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 285.1, "x1": 432.01, "y1": 607.97}]}, "hash": "197c637653dfe39446d3a7b7809ecdb7d5d98e288576d24ae4e80bf93c10c775", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ba30efe-c50a-42b9-8f62-df14b8d36a5c", "node_type": "1", "metadata": {"bbox": [{"page": 126, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.83, "x1": 432.01, "y1": 268.13}]}, "hash": "421af6baf7d1ef895390bab81b3ffc96c3f8c175c5e3b669bb49ff2155801737", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model capacity limits <br><br>Foundational models, despite their massive size, have a limit on how much infor\u2010 \nmation they can store within their parameters. You can think of a foundational \nmodel as a lossy compression of the datasets it was trained on. <br><br>Lack of access to private data <br><br>Foundational models are trained with digital data that is available to the model \nprovider. Those datasets do not include confidential data, such as internal com\u2010 \npany reports, industry-specific reports that are available only to subscribers, and \npersonalized data (such as a customer\u2019s last few orders). <br><br>When you ask an LLM to generate responses on a topic that lies beyond the scope of \nits training data, it will nevertheless pick among likely token continuations. This is an \nimportant reason why LLMs are good writers and can be creative. Also, even though \nRumi never heard of time zones, we can ask an LLM to do this: <br><br>Generate a poem in the style of Rumi about a lover in a different time zone. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ba30efe-c50a-42b9-8f62-df14b8d36a5c": {"__data__": {"id_": "2ba30efe-c50a-42b9-8f62-df14b8d36a5c", "embedding": null, "metadata": {"bbox": [{"page": 126, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.83, "x1": 432.01, "y1": 268.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e873403b-3c8c-4084-a195-006e5315488b", "node_type": "1", "metadata": {"bbox": [{"page": 125, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.63, "x1": 432.01, "y1": 276.0}]}, "hash": "bfa36fd5be9b478c653b4a0c0d7eedd6ceb789e60578affb791a50c5afeaec6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1bf1178-3ca0-494b-8f65-d94ff40ee14d", "node_type": "1", "metadata": {"bbox": [{"page": 127, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.79, "x1": 432.0, "y1": 225.49}]}, "hash": "d6427711197d9908a7341e14580891c0c03f8a33bbfc86e01e9253f807d595a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model capacity limits <br><br>RAG connects the model to external knowledge bases, effectively expanding the \nmodel\u2019s knowledge capacity beyond what\u2019s stored within its own parameters. \nRAG turns a foundational model into a \u201csmart researcher\u201d that can look up \ninformation in external sources to supplement its own knowledge and provide \nmore comprehensive and accurate responses. <br><br>Lack of access to private data <br><br>RAG allows the model to be used even if the knowledge required consists of \nconfidential, industry-specific, or personalized data. This data needs only to be \navailable at runtime to the client of the foundational model. <br><br>Hallucinations <br><br>By retrieving and incorporating relevant information from these sources, RAG \ntries to ensure that the model\u2019s responses are based on factual data, thereby \nreducing (but not eliminating) the likelihood of hallucinations. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1bf1178-3ca0-494b-8f65-d94ff40ee14d": {"__data__": {"id_": "e1bf1178-3ca0-494b-8f65-d94ff40ee14d", "embedding": null, "metadata": {"bbox": [{"page": 127, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.79, "x1": 432.0, "y1": 225.49}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2ba30efe-c50a-42b9-8f62-df14b8d36a5c", "node_type": "1", "metadata": {"bbox": [{"page": 126, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.83, "x1": 432.01, "y1": 268.13}]}, "hash": "421af6baf7d1ef895390bab81b3ffc96c3f8c175c5e3b669bb49ff2155801737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6a8ef04-62d3-48f3-ae7d-6d0013de8220", "node_type": "1", "metadata": {"bbox": [{"page": 128, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 149.87, "x1": 432.01, "y1": 338.42}]}, "hash": "c002473aad5081641aa5f00e78077f3233b696ea6aaef88c4e0b893dc202332c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can take advantage of this phenomenon to add knowledge to the LLM or \noverride knowledge that it has. For example, you can add information on a recent \nevent and then ask about it immediately: <br><br>The Seahawks traded two offensive stars over the weekend, with receiver DK Metcalf \ngoing to the Steelers and quarterback Geno Smith headed to the Raiders. <br><br>Who does Geno Smith play for? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6a8ef04-62d3-48f3-ae7d-6d0013de8220": {"__data__": {"id_": "f6a8ef04-62d3-48f3-ae7d-6d0013de8220", "embedding": null, "metadata": {"bbox": [{"page": 128, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 149.87, "x1": 432.01, "y1": 338.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e1bf1178-3ca0-494b-8f65-d94ff40ee14d", "node_type": "1", "metadata": {"bbox": [{"page": 127, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.79, "x1": 432.0, "y1": 225.49}]}, "hash": "d6427711197d9908a7341e14580891c0c03f8a33bbfc86e01e9253f807d595a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d61712a8-e22e-454d-8ccc-67216043231e", "node_type": "1", "metadata": {"bbox": [{"page": 129, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.4, "x1": 432.01, "y1": 473.6}]}, "hash": "725cd14a7f1faa2d727dadf033582586dd1eaf04a3c2993c044e8096ff9b20c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that in the last line of the prompt above, we make sure to \u201ctell\u201d the model what \nto do when no match is found. You need to make sure that the schema (assuming we \nuse Grammar [Pattern 2]) allows for \u201cNone of them\u201d as one of the responses of the \nmodel. <br><br>**Relevance.**In RAG, grounding works by adding relevant chunks (relevant to the \nquery, that is) from the knowledge base into the prompt. Because you can only \nidentify relevant chunks after you know what the query is, you do it at runtime, \nrather than at the time you train the LLM. This is an example of runtime compute. <br><br>For example, in the following prompt, there are three chunks of text that are relevant \nto the query and that are included before the question: <br><br>Use the following trustworthy information to answer the given query. <br><br>**Text:**Remove deflector tube from head (using hand, not wrench). <br><br>Inspect to see if diaphragm is intact. If diaphragm is ruptured, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d61712a8-e22e-454d-8ccc-67216043231e": {"__data__": {"id_": "d61712a8-e22e-454d-8ccc-67216043231e", "embedding": null, "metadata": {"bbox": [{"page": 129, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.4, "x1": 432.01, "y1": 473.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f6a8ef04-62d3-48f3-ae7d-6d0013de8220", "node_type": "1", "metadata": {"bbox": [{"page": 128, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 149.87, "x1": 432.01, "y1": 338.42}]}, "hash": "c002473aad5081641aa5f00e78077f3233b696ea6aaef88c4e0b893dc202332c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7f2d70e-40db-4f7a-b9fd-4996fdb0a846", "node_type": "1", "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 239.06, "x1": 432.0, "y1": 605.54}]}, "hash": "c02a9fce34a479b3e470b0d20465aa3b0e82f9e19ee3cd6deba497e5e77f642f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A RAG system needs to identify relevant text chunks from the knowledge base. It \ndoes that by building a datastore of chunks and organizing the chunks in a way \nthat makes searching for chunks that are relevant to a query efficient and fast. This \npreparatory step of building an efficient data store is called indexing, and the runtime \nstep of searching the datastore for relevant chunks is called retrieval. <br><br>**Pipelines**<br><br>A basic RAG system consists of two pipelines, as depicted in Figure 3-2. You run \nan indexing pipeline beforehand to convert knowledge sources into indexed chunks \nand store them in a document store. Then, the document store organizes the indexed \nchunks in a way that makes searching for chunks by index very fast. The indexing \npipeline is usually run in batch mode, but it could also be triggered by the arrival of a \nnew knowledge source item. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7f2d70e-40db-4f7a-b9fd-4996fdb0a846": {"__data__": {"id_": "b7f2d70e-40db-4f7a-b9fd-4996fdb0a846", "embedding": null, "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 239.06, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d61712a8-e22e-454d-8ccc-67216043231e", "node_type": "1", "metadata": {"bbox": [{"page": 129, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.4, "x1": 432.01, "y1": 473.6}]}, "hash": "725cd14a7f1faa2d727dadf033582586dd1eaf04a3c2993c044e8096ff9b20c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88ee2470-b313-4a4e-8d10-985343910a21", "node_type": "1", "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.14, "x1": 432.01, "y1": 230.24}]}, "hash": "c623ea0399845c5f565abcc1423e76e08849a6208fbe7f08fc3d086a96c3dc51", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the generation step, these relevant chunks are added to the context of the prompt \nand used along with the user query to generate the answer. In this way, the generated \nanswer is grounded in the relevant information from trusted knowledge sources. The \nresponse consists of the generated text as well as the relevant chunks that were used \nto generate the answer. <br><br>Let\u2019s see how these steps work. We would like to reiterate that a production RAG will \nbe considerably more sophisticated than what\u2019s described here. <br><br>**Indexing**<br><br>The job of the indexing pipeline is to index the source documents so that they can be \nsearched efficiently. Because you\u2019ll need the search to return smallish chunks of the \ndocuments, you don\u2019t index the document as a whole\u2014instead, you need to split the \ndocuments into chunks of the appropriate size and index those chunks. The chunks \nwill be stored in a persistent store so that they can be searched by the retriever. <br><br>We\u2019ll use the LlamaIndex framework to illustrate this pattern. In LlamaIndex, you can \nconvert read-in text into a Document object using the following code (the full code is \non GitHub): <br><br>*# Remove extra whitespace*\ncontent = text[start_pos:end_pos].strip() \ncontent = re.sub(r'\\n{3,}', '**\\n\\n**', content) <br><br>*# convert into a Document*\ndocument = Document( \n text=content, \n**metadata={**\n**\"source\": url,**\n \"filename\": filename, \n \"date_loaded\": time.strftime(\"%Y-%m-%d %H:%M:%S\") \n } \n ) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88ee2470-b313-4a4e-8d10-985343910a21": {"__data__": {"id_": "88ee2470-b313-4a4e-8d10-985343910a21", "embedding": null, "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.14, "x1": 432.01, "y1": 230.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b7f2d70e-40db-4f7a-b9fd-4996fdb0a846", "node_type": "1", "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 239.06, "x1": 432.0, "y1": 605.54}]}, "hash": "c02a9fce34a479b3e470b0d20465aa3b0e82f9e19ee3cd6deba497e5e77f642f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "728d203b-8dfb-4e0c-a8c6-09cfa9762a19", "node_type": "1", "metadata": {"bbox": [{"page": 132, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.21, "x1": 432.0, "y1": 312.76}]}, "hash": "54452cd853adf5df81a651905e2df78b5504b30e7705f4626fefd7240a4cf22b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This code hints at the following two ideas (see the bolded lines) that we\u2019ll develop \nfurther later. First, we want chunks to be information dense because we have a limited \nnumber of tokens that we can add to the input prompt when generating, and we want \nthe chunks we add to add as much knowledge as possible. As a simple matter, a chunk \nthat consists mostly of whitespace is not informative, so we remove unnecessary \nwhitespace from the knowledge source. Second, we attach metadata to the document \nso that we can cite chunks that come from this document. Pattern 8, Indexing at \nScale, covers how to do this later in this chapter, but we want to remind you here that \nyou need to keep track of metadata. <br><br>Once you have a document, you can use any splitter class to split it into chunks. The \nsimplest approach is to try to obtain chunks of a desired number of characters but to \nalso have chunks consist of complete sentences to the greatest extent possible: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "728d203b-8dfb-4e0c-a8c6-09cfa9762a19": {"__data__": {"id_": "728d203b-8dfb-4e0c-a8c6-09cfa9762a19", "embedding": null, "metadata": {"bbox": [{"page": 132, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.21, "x1": 432.0, "y1": 312.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "88ee2470-b313-4a4e-8d10-985343910a21", "node_type": "1", "metadata": {"bbox": [{"page": 131, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.14, "x1": 432.01, "y1": 230.24}]}, "hash": "c623ea0399845c5f565abcc1423e76e08849a6208fbe7f08fc3d086a96c3dc51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab9da156-7610-421e-b09e-f3c189968f56", "node_type": "1", "metadata": {"bbox": [{"page": 133, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 123.41, "x1": 432.0, "y1": 364.79}]}, "hash": "31e477519ad8213816a4ac6596779c963dc37d692dbbfcae237eb915da56d45e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The indexing point splits documents retrieved from the knowledge sources into \nchunks and makes the chunks searchable. <br><br>**Retrieval**<br><br>The job of the retrieval pipeline is to efficiently search for chunks in the document \nstore that are relevant to a given query. <br><br>Suppose, for example, you have a document store that\u2019s populated with chunks of text \nderived from The Anabasis of Alexander, a history of Alexander the Great\u2019s conquest \nof the Persian empire. You wish to search for chunks that are relevant to the question, \nso you ask the model to do this: <br><br>Describe the relationship between Alexander and Diogenes <br><br>Which chunks would you retrieve? <br><br>The intuitive answer is that you\u2019d look for chunks that contain the terms in the query. \nA chunk that contains more of the words in the query is more likely to be relevant to \nthe query. This is called term frequency (TF). At the same time, words like describe or \nthe are unlikely to matter, so the relative rarity of words also matters. As a quick fix, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab9da156-7610-421e-b09e-f3c189968f56": {"__data__": {"id_": "ab9da156-7610-421e-b09e-f3c189968f56", "embedding": null, "metadata": {"bbox": [{"page": 133, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 123.41, "x1": 432.0, "y1": 364.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "728d203b-8dfb-4e0c-a8c6-09cfa9762a19", "node_type": "1", "metadata": {"bbox": [{"page": 132, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.21, "x1": 432.0, "y1": 312.76}]}, "hash": "54452cd853adf5df81a651905e2df78b5504b30e7705f4626fefd7240a4cf22b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "736993d7-9622-4a4a-81b6-ba09b1f574a8", "node_type": "1", "metadata": {"bbox": [{"page": 134, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.55, "x1": 432.01, "y1": 235.05}]}, "hash": "e7487c4cf21c7072cb3785b0ff64b917f0f985870ed2e7476af6b9a5820eb7fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Suppose you split the Alexander biography into 200-character chunks. You will find \nthat the TF-IDF \u201cweight\u201d of the term relationship is 0.26, the weight of Alexander is \n61.04, and the weight of Diogenes is 1.01. This means that we\u2019ll get a lot of chunks \nthat contain the word Alexander, whether or not they contain the word Diogenes. \nThe TF-IDF of Alexander dominates that of Diogenes because the term frequency \nof the word Alexander in a book about Alexander is much higher than that of Dio\u2010 \ngenes: Alexander appears 1,311 times, while Diogenes appears only six times. So, the \nnumerator overwhelms the denominator. One solution is to saturate the numerator \nby defining the term frequency as count / (count + k), where k is a positive number. \nYou can easily check that this formula increases with count but never exceeds 1. <br><br>An algorithm called BM25 modifies TF-IDF by implementing term saturation in \nthe numerator and a different correction that\u2019s based on information theory and \nprobabilistic relevance to the denominator. The result is a measure that has proven \nvery successful at identifying relevant text in search problems. To build a BM25 \nretriever in LlamaIndex, you pass in the document store so that it can compute the \nIDF of all the terms found in the chunks stored there: <br><br>retriever = BM25Retriever.from_defaults( \n docstore=index.get_docstore(), \n similarity_top_k=5) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "736993d7-9622-4a4a-81b6-ba09b1f574a8": {"__data__": {"id_": "736993d7-9622-4a4a-81b6-ba09b1f574a8", "embedding": null, "metadata": {"bbox": [{"page": 134, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.55, "x1": 432.01, "y1": 235.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ab9da156-7610-421e-b09e-f3c189968f56", "node_type": "1", "metadata": {"bbox": [{"page": 133, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 123.41, "x1": 432.0, "y1": 364.79}]}, "hash": "31e477519ad8213816a4ac6596779c963dc37d692dbbfcae237eb915da56d45e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b908fe54-350f-4c72-8308-d01cada0e168", "node_type": "1", "metadata": {"bbox": [{"page": 135, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 98.7, "x1": 423.75, "y1": 340.2}]}, "hash": "f5bc4cd44145ca197ee887da588431deb7f54d10c86c39bdd40eccdba257f0ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the first message is an instruction, the intermediate messages form the \ncontext, and the last message is the query being answered. Then, send these messages \nto the LLM: <br><br>llm = Anthropic( \n model=\"claude-3-7-sonnet-latest\", \n api_key=os.environ['ANTHROPIC_API_KEY'], \n temperature=0.2 \n) \nresponse = llm.chat(messages) <br><br>When we did it, this was the answer we got: <br><br>Based on the text, Alexander and Diogenes had a brief but notable interaction. When \nAlexander met Diogenes, Diogenes simply requested that Alexander and his attendants \n\u201cstand out of the sunlight\u201d rather than asking for any favors or gifts. Alexander is ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b908fe54-350f-4c72-8308-d01cada0e168": {"__data__": {"id_": "b908fe54-350f-4c72-8308-d01cada0e168", "embedding": null, "metadata": {"bbox": [{"page": 135, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 98.7, "x1": 423.75, "y1": 340.2}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "736993d7-9622-4a4a-81b6-ba09b1f574a8", "node_type": "1", "metadata": {"bbox": [{"page": 134, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.55, "x1": 432.01, "y1": 235.05}]}, "hash": "e7487c4cf21c7072cb3785b0ff64b917f0f985870ed2e7476af6b9a5820eb7fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0be24095-b356-4576-af53-6b06b415ba96", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 485.83, "x1": 432.0, "y1": 560.29}]}, "hash": "f5313f987220f90e7626f5f96d75acc59fb1cb9a5951969b615fa741743ac094", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Ingesting PDF Documents**\nIngesting PDF documents into RAG systems presents unique challenges due to the \ncomplex structure of PDFs, which often contain a mix of text, tables, images, and \nother embedded objects. PDF ingestion also needs to handle headers, footers, and \nnonlinear layouts. Therefore, the primary approaches to PDF ingestion include direct \ntext extraction that uses specialized libraries, pipeline-based parsing that preserves \ndocument structure, and advanced AI-enabled parsing that can interpret complex \nelements. <br><br>At the time of writing, there are several tools and libraries that facilitate PDF inges\u2010 \ntion for RAG systems. LlamaParse is a generative AI\u2013enabled document-parsing \ntechnology that is designed for complex documents, and it offers capabilities to \nextract text, tables, images, and even mathematical equations while transforming \nthem into easily ingestible formats like Markdown. The unstructured, docling, paper\u2010 \nmage, and marker Python libraries offer different approaches to pipeline-based PDF \nparsing: unstructured and docling provide flexibility across multiple document types \nand tools, papermage specializes in academic research papers, and marker focuses on \nconverting PDFs to Markdown format. Fully managed RAG systems such as Vertex \nAI RAG Engine and Glean offer direct PDF ingestion, although it\u2019s unclear what \nframework they use under the hood. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0be24095-b356-4576-af53-6b06b415ba96": {"__data__": {"id_": "0be24095-b356-4576-af53-6b06b415ba96", "embedding": null, "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 485.83, "x1": 432.0, "y1": 560.29}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b908fe54-350f-4c72-8308-d01cada0e168", "node_type": "1", "metadata": {"bbox": [{"page": 135, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 98.7, "x1": 423.75, "y1": 340.2}]}, "hash": "f5bc4cd44145ca197ee887da588431deb7f54d10c86c39bdd40eccdba257f0ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e057da30-afe5-4db1-97d3-b006bf386052", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 280.91, "x1": 414.0, "y1": 476.45}]}, "hash": "c340cfa5f2ce63a43588b4a50f05c25bd47782039ac94c653894ff7bbaaba5f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A basic RAG like this (one that is based on keyword search) is best suited to factual \nmaterials. Therefore, we loaded an equipment manual into the RAG system and \nasked it this question: <br><br>What should I do if the diaphragm is ruptured? <br><br>The answer was this: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e057da30-afe5-4db1-97d3-b006bf386052": {"__data__": {"id_": "e057da30-afe5-4db1-97d3-b006bf386052", "embedding": null, "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 280.91, "x1": 414.0, "y1": 476.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0be24095-b356-4576-af53-6b06b415ba96", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 485.83, "x1": 432.0, "y1": 560.29}]}, "hash": "f5313f987220f90e7626f5f96d75acc59fb1cb9a5951969b615fa741743ac094", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1275ecb4-a93f-4658-b474-451738bac950", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.76, "x1": 432.0, "y1": 276.05}]}, "hash": "12fd404dbddc3337c796ea07bca06a76ec1aed49c17461d3e8e38fcd69625f5a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the diaphragm is ruptured, you should replace the safety head with an unbroken \nhead. Additionally, if you notice any tears, separation, or leaks occurring at the dia\u2010 \nphragm, you should replace the entire valve-diaphragm assembly. <br><br>When handling the diaphragm components, remember to unscrew the diaphragm cap \nby hand (not with a wrench) and be careful not to disturb the position of the yoke \nblock by turning the needle, as this would affect the valve-needle adjustment. <br><br>**Sources**: <br><br>Node ID: 6afc9709-b53b-4fc9-8f7e-b1bbf198f9b0 <br><br>Text: Inspect to see if diaphragm is intact. If diaphragm is ruptured, <br><br>replace the safety head with an unbroken head. <br><br>Score: 4.869 <br><br>Node ID: b84ca5bf-c79d-4040-8766-7c528e693559 <br><br>Text: (3) Unscrew diaphragm cap and pull out washer, support, and <br><br>valve-diaphragm assembly. To prevent loss of valve-needle adjustment <br><br>(Fig 54), do not disturb position of yoke block by turning the needle. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1275ecb4-a93f-4658-b474-451738bac950": {"__data__": {"id_": "1275ecb4-a93f-4658-b474-451738bac950", "embedding": null, "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.76, "x1": 432.0, "y1": 276.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e057da30-afe5-4db1-97d3-b006bf386052", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 280.91, "x1": 414.0, "y1": 476.45}]}, "hash": "c340cfa5f2ce63a43588b4a50f05c25bd47782039ac94c653894ff7bbaaba5f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a505cd9-3b07-4027-b23e-99fa4e622d4c", "node_type": "1", "metadata": {"bbox": [{"page": 137, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 131.93, "x1": 505.25, "y1": 512.02}]}, "hash": "40da1111151d3f72f1a624d4b5f12ad275c2852f4b260a437e54d1687a5b154a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Score: 3.282 <br><br>It\u2019s clear that the RAG system has picked up a very relevant part of the text and has \nused it to generate a good answer. <br><br>**Considerations**\nRAG is well suited to adding factual knowledge to an LLM. It works by finding the \nmost relevant knowledge (relevant to the query at hand) and incorporating it into the \ncontext of the prompt. <br><br>**Alternatives**<br><br>None of the techniques for controlling style (see Chapter 2) will be able to add \nknowledge. It is clear why patterns such as Logits Masking (Pattern 1) don\u2019t work\u2014 \nyou need the LLM to be able to generate the tokens before you can mask them. If the \nLLM has only seen text from before 2025, tokens related to Pope Leo XIV will be so \nunlikely that they will not even be a choice. This is also the case for few-shot learning ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a505cd9-3b07-4027-b23e-99fa4e622d4c": {"__data__": {"id_": "8a505cd9-3b07-4027-b23e-99fa4e622d4c", "embedding": null, "metadata": {"bbox": [{"page": 137, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 131.93, "x1": 505.25, "y1": 512.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1275ecb4-a93f-4658-b474-451738bac950", "node_type": "1", "metadata": {"bbox": [{"page": 136, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.76, "x1": 432.0, "y1": 276.05}]}, "hash": "12fd404dbddc3337c796ea07bca06a76ec1aed49c17461d3e8e38fcd69625f5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c071bf-e148-4e30-adec-0c99e42f65b7", "node_type": "1", "metadata": {"bbox": [{"page": 138, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.41, "x1": 432.01, "y1": 265.92}]}, "hash": "1732b31344fd4a5bb017f50a24d863ab333f124785cdf6bfb7a7715f9760ec85", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**RAG versus Large Context Window**\nIf your document is small enough, you may not need the retrieval part of RAG. You \ncould simply include the complete document in the context of the prompt and ask \nthe model to generate the answer. If you include the full document, you don\u2019t need to \nworry about the errors associated with chunking or with retrieval. <br><br>For example, if you wish to query a tax return, you could load the entire tax return \ninto the prompt and ask questions. This is shown in our GitHub repository in the \nlarge_context_window.ipynb notebook: <br><br>**def**answer_question(prompt: str, cached_tax_return: str) -> str: \n response = client.models.generate_content( \n model=GEMINI, \n contents=prompt, \n config=types.GenerateContentConfig( \n cached_content=cached_tax_return \n )) \n print(f'{response.usage_metadata=}') \n**return**response.text <br><br>answer_question(\"How much did Obama claim in business expenses?\", \"cachedContents/wc0yof...\") \n... \nanswer_question(\"Did Obama make any retirement plan contributions?\", \"cachedContents/wc0yof...\") <br><br>In this code, we employ server-side Prompt Caching (Pattern 25 from Chapter 8) \nto avoid actually repeating the tax return document in the prompt. The multimodal \nLLM will precompute the tokenization and embeddings as necessary and reuse them \nfor each of the queries. This helps to keep costs reasonable. (See the code for full \ndetails.) <br><br>Of course, when we say, \u201csmall document,\u201d we are comparing it to the size of the \ncontext window supported by the model. At the time we are writing this section (June \n2025), frontier models offer context windows of as many as two million tokens, which \nis just about enough to hold the entire French tax code. Instead of building a RAG \nsystem on the tax code, you could include the entirety of it with a query! ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56c071bf-e148-4e30-adec-0c99e42f65b7": {"__data__": {"id_": "56c071bf-e148-4e30-adec-0c99e42f65b7", "embedding": null, "metadata": {"bbox": [{"page": 138, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.41, "x1": 432.01, "y1": 265.92}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8a505cd9-3b07-4027-b23e-99fa4e622d4c", "node_type": "1", "metadata": {"bbox": [{"page": 137, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 131.93, "x1": 505.25, "y1": 512.02}]}, "hash": "40da1111151d3f72f1a624d4b5f12ad275c2852f4b260a437e54d1687a5b154a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bfb3d03-59a4-4171-8e95-fec483887533", "node_type": "1", "metadata": {"bbox": [{"page": 139, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.63, "x1": 432.0, "y1": 261.13}]}, "hash": "62e99c341f6fd49a7a5f6ea552c8f73d31a87f57d37770b36e33bca90173c4e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Several patterns that follow in this and the next chapter \u2014Semantic Indexing (Pattern \n7), Indexing at Scale (Pattern 8), Index-Aware Retrieval (Pattern 11 from Chapter 4,), \nNode Postprocessing (Pattern 10 from Chapter 4),Trustworthy Generation (Pattern \n11 from Chapter 4, ), and Deep Search (Pattern 12 from Chapter 4)\u2014all improve \nthe capability of a basic RAG system. First, however, you should carefully evaluate \nwhether adding complexity by applying these patterns brings quantifiable benefits. \nMoreover, these patterns should be considered add-ons to a basic RAG. For example, \nin many cases, a RAG system that has only an embedding-based retriever (with \nno keyword-based one such as BM25) will be inadequate. In the absence of exact \nmatches on keywords in queries, your RAG system will struggle to identify the right \ntext to retrieve when searching for specific products, item-codes, and the like. <br><br>**References**\nIn 2020, RAG was introduced in the canonical paper \u201cRetrieval-Augmented Genera\u2010 \ntion for Knowledge-Intensive NLP Tasks\u201d by Lewis et al.. In 2024, Gao et al. reviewed ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bfb3d03-59a4-4171-8e95-fec483887533": {"__data__": {"id_": "7bfb3d03-59a4-4171-8e95-fec483887533", "embedding": null, "metadata": {"bbox": [{"page": 139, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.63, "x1": 432.0, "y1": 261.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "56c071bf-e148-4e30-adec-0c99e42f65b7", "node_type": "1", "metadata": {"bbox": [{"page": 138, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.41, "x1": 432.01, "y1": 265.92}]}, "hash": "1732b31344fd4a5bb017f50a24d863ab333f124785cdf6bfb7a7715f9760ec85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "781f495a-d752-4383-9987-2d165972c5a8", "node_type": "1", "metadata": {"bbox": [{"page": 142, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 606.65}]}, "hash": "3a4a54ed8050b415f589165ee308edab349940d79e53e660def1e2001ca0b6a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you use the Basic RAG approach of keyword indexing, you\u2019ll probably fail to \nretrieve the chunk that contains the above snippet. This is because the query men\u2010 \ntions \u201cAI,\u201d while the chunk mentions \u201cArtificial Intelligence\u201d and doesn\u2019t mention \n\u201cterminology ambiguity\u201d anywhere in the text. <br><br>Keyword-based indexing methods struggle with matching queries to chunks when \ndealing with the following elements and aspects of text, writing, communication, and \ndocuments: <br><br>Synonyms and pronouns <br><br>Traditional keyword indexing fails at handling ambiguity and industry jargon \n(such as the word AI). If a text chunk refers to \u201cThe President\u201d and the query \ncontains the name of the president, the two will not get matched. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "781f495a-d752-4383-9987-2d165972c5a8": {"__data__": {"id_": "781f495a-d752-4383-9987-2d165972c5a8", "embedding": null, "metadata": {"bbox": [{"page": 142, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7bfb3d03-59a4-4171-8e95-fec483887533", "node_type": "1", "metadata": {"bbox": [{"page": 139, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.63, "x1": 432.0, "y1": 261.13}]}, "hash": "62e99c341f6fd49a7a5f6ea552c8f73d31a87f57d37770b36e33bca90173c4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb64e334-d48a-42cc-97a7-8f31f56cac94", "node_type": "1", "metadata": {"bbox": [{"page": 143, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 396.0, "y1": 602.250002}]}, "hash": "55838d1a5551631e552d5619c2ed3fa99192130a62f053e2783ac0dd8eda5898", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " title=\"Keyword-based Similarity Matrix\" \n) <br><br>If you run the example, you\u2019ll get the following similarity results. Using the keyword \napproach, none of the \u201cdocuments\u201d are actually close (highly similar documents have \na score close to 1). Figure 3-4 presents a visualization of the similarity matrix. <br><br><br><br>Figure 3-4. Matrix based on similarities between keywords <br><br>If you now run the same similarity lookup but use embeddings instead, you\u2019ll see that \nall scores are higher (see Figure 3-5). This is because all documents talk about movies \nand one document is very similar to the query. In the keyword approach, the score \nwas low because movie \u2260 film and excellent \u2260 great, but embeddings can capture the \nsimilarity between those terms. <br><br>model = SentenceTransformer('all-MiniLM-L6-v2') \nembedding_vectors = model.encode(chunks) <br><br>*# Query*\nquery = \"The film was great\" \nquery_embedding = model.encode([query])[0] <br><br>embedding_sim_matrix = plot_similarity_matrix( \n chunks, \n embedding_vectors, \n title=\"Embedding-based Similarity Matrix\" \n) <br><br>**Pattern 7: Semantic Indexing** **|** **125**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb64e334-d48a-42cc-97a7-8f31f56cac94": {"__data__": {"id_": "eb64e334-d48a-42cc-97a7-8f31f56cac94", "embedding": null, "metadata": {"bbox": [{"page": 143, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 396.0, "y1": 602.250002}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "781f495a-d752-4383-9987-2d165972c5a8", "node_type": "1", "metadata": {"bbox": [{"page": 142, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 606.65}]}, "hash": "3a4a54ed8050b415f589165ee308edab349940d79e53e660def1e2001ca0b6a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df5244bc-a822-40aa-ad54-b3d66cb17415", "node_type": "1", "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.34, "x1": 432.01, "y1": 605.54}]}, "hash": "99d3f6985d09c8d97dc678604bd7c158283cdf653118497257f0cccdbbaad0a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 3-5. Matrix based on similarities between embeddings <br><br>While increasing embedding dimensionality could capture more \nsemantic information, this approach faces significant computa\u2010 \ntional limitations. With higher dimensions, the similarity search \nbetween embeddings grows in complexity proportional to N \n(dimensionality) \u00d7 M (number of embeddings), potentially reach\u2010 \ning N\u00b2 time complexity in worst-case scenarios. This curse of \ndimensionality creates substantial performance bottlenecks in pro\u2010 \nduction applications. <br><br>Therefore, optimal embedding design balances representational \npower with computational efficiency by doing the following: <br><br>\u2022 \n\u2022 Using the minimum dimensionality that adequately captures \nsemantic relationships <br><br>\u2022 \n\u2022 Employing dimensionality reduction techniques when possi\u2010 \nble <br><br>\u2022 \n\u2022 Trying to use nearest-neighbor approximations when possible <br><br>The goal is to create embeddings that provide sufficient seman\u2010 \ntic representation while maintaining reasonable computational \nrequirements for practical applications. <br><br>**126** **|** **Chapter 3: Adding Knowledge: Bass**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df5244bc-a822-40aa-ad54-b3d66cb17415": {"__data__": {"id_": "df5244bc-a822-40aa-ad54-b3d66cb17415", "embedding": null, "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.34, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "eb64e334-d48a-42cc-97a7-8f31f56cac94", "node_type": "1", "metadata": {"bbox": [{"page": 143, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 396.0, "y1": 602.250002}]}, "hash": "55838d1a5551631e552d5619c2ed3fa99192130a62f053e2783ac0dd8eda5898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46581f03-2256-49ce-b7c2-44004525add4", "node_type": "1", "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 115.14, "x1": 432.01, "y1": 278.24}]}, "hash": "e681da3106cd55a1eb92919126d356af2e28c52a0a152553eab61bb41394589b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, you might wonder what the limits of the embeddings are. Could you embed \nentire books? <br><br>While the latest LLMs provide massive context windows, encoding entire documents \nin a single embedding vector is often not advisable. You\u2019ll lose the nuances of the \ndifferent parts of a text, lose critical information, and hinder the RAG system from \nretrieving relevant information. Plus, it gets very expensive to use large context \nwindows at scale. It\u2019s preferable to chunk documents into information-dense and \nself-consistent chunks of text. <br><br>**Semantic chunking**<br><br>Semantic chunking is dividing text into meaningful segments based on their semantic \ncontent, rather than arbitrary length. The naivest way of chunking text is based on \na word or character count, but that can break up closely related sections and lose \ncontext. On the other hand, semantic chunking preserves context and meaning. <br><br>There are several ways to divide text into segments (also see Figure 3-6): <br><br>Length-based with overlap <br><br>This is the most straightforward option. You can split the document into fixed- \nlength chunks that overlap between segments. This helps preserve context across \nchunk boundaries but can still split context blocks. <br><br>Sentence-based chunking <br><br>This method chunks text into groups of complete sentences until a size threshold \nis reached. It preserves the basic semantic meaning of a block of text but struggles \nwith capturing the topic transitions. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46581f03-2256-49ce-b7c2-44004525add4": {"__data__": {"id_": "46581f03-2256-49ce-b7c2-44004525add4", "embedding": null, "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 115.14, "x1": 432.01, "y1": 278.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "df5244bc-a822-40aa-ad54-b3d66cb17415", "node_type": "1", "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.34, "x1": 432.01, "y1": 605.54}]}, "hash": "99d3f6985d09c8d97dc678604bd7c158283cdf653118497257f0cccdbbaad0a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2973c2d-b33a-4a58-ab4b-7d9373157e5e", "node_type": "1", "metadata": {"bbox": [{"page": 146, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.249999}]}, "hash": "8cb1760f5d211b3400a0c67a53211f212fd86932320cdcfed6e94b36c0dd1586", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Paragraph-based chunking <br><br>This method uses natural paragraph breaks as chunk boundaries. It works well \nfor structured documents. <br><br>Document-structure chunking <br><br>Various document formats, like Markdown or Markup, provide a structure that \nyou can use to chunk a document into subsections. For example, you can chunk a \nMarkdown file into sections of second-level heading sections. <br><br>Chunking based on semantic shifts <br><br>You can use topic modeling techniques (such as latent dirichlet allocation and \nnonnegative matrix factorization) or embeddings of smaller document subsec\u2010 \ntions to identify semantic shifts. When you detect a shift, split the document into \na new chunk. This method guarantees that the chunks are of coherent topics. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2973c2d-b33a-4a58-ab4b-7d9373157e5e": {"__data__": {"id_": "e2973c2d-b33a-4a58-ab4b-7d9373157e5e", "embedding": null, "metadata": {"bbox": [{"page": 146, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.249999}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "46581f03-2256-49ce-b7c2-44004525add4", "node_type": "1", "metadata": {"bbox": [{"page": 144, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 115.14, "x1": 432.01, "y1": 278.24}]}, "hash": "e681da3106cd55a1eb92919126d356af2e28c52a0a152553eab61bb41394589b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef50a8d9-0cee-43a2-b0a3-f8dcd4e661a0", "node_type": "1", "metadata": {"bbox": [{"page": 147, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "hash": "ac709a452d4c5caef4ce2b0a5ea3112af05a1318ae0db4132bcc202612474d9a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 3-7. Handling of images (photo by Elise Farrow on Unsplash) <br><br>The first option is to use an LLM like Llama-3.2-9B to describe the image. You can \nthen replace the image with the text description, and after that, you can chunk the \ntext as discussed earlier and index the individual chunks. <br><br>The second option is to pass the images to an LLM directly for embedding genera\u2010 \ntion. Multimodal models allow the creation of embeddings for text and images so that \ntheir vector space overlaps semantically. <br><br>You can handle video very similarly. You can transcribe the video into text if you \nwant to add videos to your index. As we discussed earlier, you can handle, chunk, and \nembed this text like any other text. In addition, you can break down the video into \nindividual images by sampling keyframes at regular intervals or scene changes. You \ncan then embed the resulting images just like regular images. <br><br>**Pattern 7: Semantic Indexing** **|** **129**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef50a8d9-0cee-43a2-b0a3-f8dcd4e661a0": {"__data__": {"id_": "ef50a8d9-0cee-43a2-b0a3-f8dcd4e661a0", "embedding": null, "metadata": {"bbox": [{"page": 147, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e2973c2d-b33a-4a58-ab4b-7d9373157e5e", "node_type": "1", "metadata": {"bbox": [{"page": 146, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.249999}]}, "hash": "8cb1760f5d211b3400a0c67a53211f212fd86932320cdcfed6e94b36c0dd1586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6760a32-cec3-4fb9-829e-cb8960675992", "node_type": "1", "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 286.34, "x1": 432.01, "y1": 605.54}]}, "hash": "6b2a6b89bf573eb8c1f7376fd736025ef0e0b49290667df2e58062a0024e97ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 3-8. Handling of videos <br><br>**Handling tables**<br><br>Tables serve as essential structures for organizing key information in documents. \nThey may contain critical data such as research results or participant lists that are of \nsufficient importance to warrant structured presentation in tabular format. <br><br>When implementing RAG systems, special attention must be given to table process\u2010 \ning. Tables require specific indexing approaches to preserve their structural relation\u2010 \nships and ensure that both the content and the meaningful organization of that \ncontent are captured. Proper table handling in RAG pipelines enhances the retrieval \nsystem\u2019s ability to find and utilize this structured information during generation. <br><br>When it comes to the indexing of the table, you have multiple options. First of \nall, you should handle any missing values and ensure that the table is consistently \nformatted. <br><br>With respect to the chunking strategies, you have four options: <br><br>**130** **|** **Chapter 3: Adding Knowledge: Bass**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6760a32-cec3-4fb9-829e-cb8960675992": {"__data__": {"id_": "e6760a32-cec3-4fb9-829e-cb8960675992", "embedding": null, "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 286.34, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ef50a8d9-0cee-43a2-b0a3-f8dcd4e661a0", "node_type": "1", "metadata": {"bbox": [{"page": 147, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "hash": "ac709a452d4c5caef4ce2b0a5ea3112af05a1318ae0db4132bcc202612474d9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d4fe9f2-a942-4d1a-9e2d-406e6e1904f9", "node_type": "1", "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.7, "x1": 432.0, "y1": 276.97}]}, "hash": "67be640fd0920e2825defd9c3712307d4d65dba0f98f3a8596f0d5ff51718ac4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table-based chunking <br><br>If a table is small, you can embed the entire table. This is the least complex \nchunking method, but it will not provide fine-grained results for queries. <br><br>Table-based chunking with a sliding window <br><br>As you do when handling text, you can slide over tables and index the chunks. \nKeep track of the table headers and attach the headers to each chunk for more \ncontext. This method works well for large tables. <br><br>Row-based chunking <br><br>You can chunk large tables by rows and embed each row. This method works well \nfor semantically disconnected rows, such as a table of random bank transactions. <br><br>Column-based chunking <br><br>Various types of tables (such as measurements over time) benefit from keeping \nthe information \u201ctogether.\u201d If the table is large, you can embed the individual \ncolumns. <br><br>Before chunking the overall text surrounding the table, try to preserve the table\u2019s \nmetadata in the text chuck. This could be the table name, or it could be a detailed \ndescription of the column headers (which probably mention specific units) or the \noverall table. <br><br>**Handling industry jargon**<br><br>In traditional RAG systems, if your user searches for the term heart attack, the RAG \nsystem will fail to retrieve documents that use very specific medical terms for heart \nattack, like acute myocardial infarction or cardiac infarction. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d4fe9f2-a942-4d1a-9e2d-406e6e1904f9": {"__data__": {"id_": "9d4fe9f2-a942-4d1a-9e2d-406e6e1904f9", "embedding": null, "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.7, "x1": 432.0, "y1": 276.97}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e6760a32-cec3-4fb9-829e-cb8960675992", "node_type": "1", "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 286.34, "x1": 432.01, "y1": 605.54}]}, "hash": "6b2a6b89bf573eb8c1f7376fd736025ef0e0b49290667df2e58062a0024e97ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46537374-ffaa-4477-8c10-ad766a1947c5", "node_type": "1", "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.55, "x1": 951.75, "y1": 605.54}]}, "hash": "da6afdf92f0efda6c5802003f498536e5813ebdcf3842050d62d30a536ca3d02", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What are the symptoms of a heart attack? -> What are the symptoms of a heart attack| \nacute myocardial infarction|cardiac infarction? <br><br>Similarly, attorneys might use a RAG-based application to search for the term discov\u2010 \nery. A query like \u201cWhat was the timeline for discovery in federal court\u201d should \nalso include documents that refer to disclosure or deposition. You can do this with \nsynonym expansion in the query as follows: <br><br>What was the timeline for discovery in federal court? -> What was the timeline for \ndiscovery|disclosure|deposition in federal court? <br><br>Synonym expansion enriches queries by adding related terms and therefore allowing \nyour system to match content even when different terminology is used for the same \nconcept. But you can also expand terms in the original documents. That\u2019s more \ncomprehensive, but it increases your index size. <br><br>In traditional NLP, the use of different words by different roles or in different geogra\u2010 \nphies to represent the same term requires building a lexicon\u2014a structured vocabulary ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46537374-ffaa-4477-8c10-ad766a1947c5": {"__data__": {"id_": "46537374-ffaa-4477-8c10-ad766a1947c5", "embedding": null, "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.55, "x1": 951.75, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9d4fe9f2-a942-4d1a-9e2d-406e6e1904f9", "node_type": "1", "metadata": {"bbox": [{"page": 148, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.7, "x1": 432.0, "y1": 276.97}]}, "hash": "67be640fd0920e2825defd9c3712307d4d65dba0f98f3a8596f0d5ff51718ac4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b36dfaaa-0868-4393-86ff-264b8978d56c", "node_type": "1", "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.95, "x1": 432.01, "y1": 279.45}]}, "hash": "1075f5b20297ebd5a5deaf37d5a451986b6e4803efd02c039d686c51e001599c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To create such a contextualized chunk, you\u2019ll have to summarize the document before \nprepending it to the document chunk. To perform this operation efficiently, you can \nfollow Anthropic\u2019s suggestion to employ an LLM and send it the following prompt: <br><br><document> \n{{WHOLE_DOCUMENT}} \n</document> <br><br>Here is the chunk we want to situate within the whole document \n<chunk> \n{{CHUNK_CONTENT}} \n</chunk> <br><br>Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. <br><br>Anthropic has demonstrated that contextual retrieval with this prompt structure and \nthe Claude LLM has reduced incorrect retrieval rates by 67% consistently across \ndifferent content domains. Because it is the first part of the prompt, the document \ncontent (between the document tags) can be cached, which makes the processing \ncheap and efficient (see Pattern 25, Prompt Caching, in Chapter 8). Once you \nprepend your document chunks with the document context, you can embed the \ncontextualized chunks into semantic and BM25 vectors for the most accurate RAG \nresponses. <br><br>**Hierarchical chunking (alternative chunking techniques)**<br><br>Traditional RAG systems typically only retrieve short document chunks, which limits \ntheir holistic understanding of the overall context. This is troublesome for long \ndocuments like research papers and entire books. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b36dfaaa-0868-4393-86ff-264b8978d56c": {"__data__": {"id_": "b36dfaaa-0868-4393-86ff-264b8978d56c", "embedding": null, "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.95, "x1": 432.01, "y1": 279.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "46537374-ffaa-4477-8c10-ad766a1947c5", "node_type": "1", "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 287.55, "x1": 951.75, "y1": 605.54}]}, "hash": "da6afdf92f0efda6c5802003f498536e5813ebdcf3842050d62d30a536ca3d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "410d2fa3-4993-420c-9790-ebd2b90f7597", "node_type": "1", "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.77, "x1": 531.0, "y1": 607.89}]}, "hash": "a6fc472aeda7ef2158544de48648a39829b835477a030e2c38a2aae333e78526", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To address this issue, you could increase the chunk sizes and overlap them. That \nwould increase the context window for a better understanding, but nuances might be \nlost during embedding generation. <br><br>Hierarchical chunking is an extension of semantic chunking that can address this issue \nmore effectively. During the processing phase, you build a reversed tree structure \nstarting at the bottom by chunking documents into smaller pieces and embedding \nthe chunks. You can then cluster the chunks into groups and create summaries of \nthe clusters. You then embed, cluster, and summarize those, and so on until you \nhave just one note, which is your tree root (depicted at the top right in Figure 3-9). \nAn advanced approach to hierarchical chunking and retrieval is called Recursive \nAbstractive Processing for Tree-Organized Retrieval (RAPTOR). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "410d2fa3-4993-420c-9790-ebd2b90f7597": {"__data__": {"id_": "410d2fa3-4993-420c-9790-ebd2b90f7597", "embedding": null, "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.77, "x1": 531.0, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b36dfaaa-0868-4393-86ff-264b8978d56c", "node_type": "1", "metadata": {"bbox": [{"page": 150, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 136.95, "x1": 432.01, "y1": 279.45}]}, "hash": "1075f5b20297ebd5a5deaf37d5a451986b6e4803efd02c039d686c51e001599c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "972f5b4b-dfa5-42d0-98b0-ae805faa68b0", "node_type": "1", "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.01, "y1": 223.96}]}, "hash": "ae901f9baa6aa342bf00eef22d7612a96f0cd1e7b0d3fe3299f0ca84c94f24ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Example**\nIn this example, we\u2019ll demonstrate how you can index tables to query for particular \ntabular data. For simplicity, we only focus on the indexing and querying of the \ntabular data. <br><br>Let\u2019s say we have the tabular data available in a structured format\u2014this will be the \ncase if you extracted the table data from a relational database or a spreadsheet. If \nyou don\u2019t have the data available in a structured format, you need to convert it \nbeforehand. <br><br>Suppose that we\u2019re indexing a little product catalog. Each item has a name, a descrip\u2010 \ntion, and many numerical and binary fields: <br><br>[{ \n 'product_id': 2, \n 'name': 'Wireless Noise-Cancelling Headphones', \n 'description': headphones with active noise cancellation', \n 'category': 'Electronics', \n 'price': 349.99, \n 'in_store': True \n ... \n}, \n...] <br><br>First, we need to preprocess the product fields, and we perform the preprocessing by \ncolumn types. For example, all text fields of a row get concatenated and embedded \ninto an embedding: <br><br>**def**encode_text_embeddings(model, text_data): \n*# Combine text fields into a single string for each row*\n embeddings = [] \n**for**_, row**in**text_data.iterrows(): \n text = f\"Product: {row['name']}. Description: {row['description']}. Category: {row['category']}\" \n embeddings.append(model.encode(text)) \n**return**embeddings ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "972f5b4b-dfa5-42d0-98b0-ae805faa68b0": {"__data__": {"id_": "972f5b4b-dfa5-42d0-98b0-ae805faa68b0", "embedding": null, "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.01, "y1": 223.96}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "410d2fa3-4993-420c-9790-ebd2b90f7597", "node_type": "1", "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.77, "x1": 531.0, "y1": 607.89}]}, "hash": "a6fc472aeda7ef2158544de48648a39829b835477a030e2c38a2aae333e78526", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91f9cc84-8370-431b-8907-05ffe0352aec", "node_type": "1", "metadata": {"bbox": [{"page": 153, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.64, "x1": 432.01, "y1": 220.85}]}, "hash": "c11020923f62725665dc34744a261bac9dbae7c53d565f7607a918857babd7fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When working with numerical data (such as sales prices or stock values), you need \nto prepare it differently than text data. You should scale all numerical values to fall \nbetween 0 and 1, in a process called normalization. <br><br>Why is normalization important? Later in the process, you\u2019ll combine these normal\u2010 \nized numbers with text embeddings (numerical representations of text), and by \nensuring that all numerical values are on the same 0\u20131 scale, you enable the RAG \nsystem to properly consider both text and numerical information when it searches for \nsimilar content. <br><br>The following example shows how to implement a normalization function. Also, \nhere\u2019s an important note for production systems: you must save the scaling param\u2010 \neters (the scaler) that you use during initial setup. This allows you to apply the ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91f9cc84-8370-431b-8907-05ffe0352aec": {"__data__": {"id_": "91f9cc84-8370-431b-8907-05ffe0352aec", "embedding": null, "metadata": {"bbox": [{"page": 153, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.64, "x1": 432.01, "y1": 220.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "972f5b4b-dfa5-42d0-98b0-ae805faa68b0", "node_type": "1", "metadata": {"bbox": [{"page": 152, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.46, "x1": 432.01, "y1": 223.96}]}, "hash": "ae901f9baa6aa342bf00eef22d7612a96f0cd1e7b0d3fe3299f0ca84c94f24ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c527133b-9f43-4579-97fd-4e2755394148", "node_type": "1", "metadata": {"bbox": [{"page": 154, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.27, "x1": 432.0, "y1": 192.01}]}, "hash": "a23a48b5acada6c00701efef801fcac6384cc8c699688018f48515cf60558090", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "... \ncollection.add( \n ids=ids, \n embeddings=create_hybrid_embeddings(...), \n) <br><br>If you want to run queries against the embedded tabular data, you need to create \nan embedding for your search data. The text embedding is generated from the user \nquery: for example, \u201cTop Notebook for Gaming and Work.\u201d Since you don\u2019t know the \nnumeric features of the product you want during the inference step, you can use the \nmedian vector of all products. (This simplified approach is shown in the section on \nPattern 9, Index-Aware Retrieval, in Chapter 4.) After embedding all the elements, \nyou need to concatenate the embeddings as you did during the indexing phase. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c527133b-9f43-4579-97fd-4e2755394148": {"__data__": {"id_": "c527133b-9f43-4579-97fd-4e2755394148", "embedding": null, "metadata": {"bbox": [{"page": 154, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.27, "x1": 432.0, "y1": 192.01}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "91f9cc84-8370-431b-8907-05ffe0352aec", "node_type": "1", "metadata": {"bbox": [{"page": 153, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.64, "x1": 432.01, "y1": 220.85}]}, "hash": "c11020923f62725665dc34744a261bac9dbae7c53d565f7607a918857babd7fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7629348-8104-40a1-b1c2-66786e45f4b2", "node_type": "1", "metadata": {"bbox": [{"page": 155, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.09, "x1": 432.01, "y1": 362.97}]}, "hash": "cbbd01db46d7435911b77ca0645b332bd303d14fa453dbdca576862ddd17a1aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Alternatives**<br><br>Instead of using a general-purpose embedding model, you could use a domain- \nspecific model. If your entire knowledge base consists of a single PDF (for example), \nyou could stuff the entire document into the prompt. <br><br>**Domain-specific embedding models.**Using embedding models that are pretrained \nspecifically on the literature of your industry (such as the medical, legal, or finan\u2010 \ncial domain) can dramatically improve retrieval precision and latency. These special\u2010 \nized models capture domain-specific terminology, relationships, and nuances that ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7629348-8104-40a1-b1c2-66786e45f4b2": {"__data__": {"id_": "c7629348-8104-40a1-b1c2-66786e45f4b2", "embedding": null, "metadata": {"bbox": [{"page": 155, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.09, "x1": 432.01, "y1": 362.97}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c527133b-9f43-4579-97fd-4e2755394148", "node_type": "1", "metadata": {"bbox": [{"page": 154, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.27, "x1": 432.0, "y1": 192.01}]}, "hash": "a23a48b5acada6c00701efef801fcac6384cc8c699688018f48515cf60558090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4965ae00-7b58-40ab-a04c-a2623ea21675", "node_type": "1", "metadata": {"bbox": [{"page": 156, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.23, "x1": 432.01, "y1": 287.74}]}, "hash": "48aa8136f34434149897314b986f9ff78dd67a6278e234e887e9ce9b16a4156a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Limitations**<br><br>While semantic indexing is very powerful, you should be aware of its limitations: <br><br>Fixed-dimensional representation constraints <br><br>Most embedding models map text to fixed-dimensional vectors (typically 768\u2013 \n4096 dimensions) and create an inherent information bottleneck in the process. \nThis means complex, nuanced chunks must be compressed into the same vector \nspace as simple ones, which means they inevitably lose some information depth. \nAs document complexity increases, this compression becomes more problematic, \nespecially for highly technical or multifaceted content. <br><br>Chunking challenges <br><br>Breaking documents into smaller pieces for indexing purposes can disrupt their \nsemantic coherence and make them lose important cross-reference connections \nbetween sections. For example, if a concept is introduced in one section of a \ndocument and elaborated on in another, chunking may separate these related \nparts and make it difficult to retrieve the complete context. Additionally, deter\u2010 \nmining optimal chunk sizes remains more of an art than a science, and it often \nrequires domain-specific tuning. <br><br>Vector database scalability issues <br><br>As vector collections grow into millions or billions of embeddings, you\u2019ll need to \nemploy approximate nearest neighbor (ANN) search techniques such as ScaNN \nand Faiss to maintain reasonable query times. This sacrifices retrieval accuracy ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4965ae00-7b58-40ab-a04c-a2623ea21675": {"__data__": {"id_": "4965ae00-7b58-40ab-a04c-a2623ea21675", "embedding": null, "metadata": {"bbox": [{"page": 156, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.23, "x1": 432.01, "y1": 287.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c7629348-8104-40a1-b1c2-66786e45f4b2", "node_type": "1", "metadata": {"bbox": [{"page": 155, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.09, "x1": 432.01, "y1": 362.97}]}, "hash": "cbbd01db46d7435911b77ca0645b332bd303d14fa453dbdca576862ddd17a1aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3447e428-cf3e-4fae-a5ea-2217c5141c4c", "node_type": "1", "metadata": {"bbox": [{"page": 157, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.04, "x1": 432.01, "y1": 262.13}]}, "hash": "76a83c21fb523789850830f014842f1b69ebc2b5edd52e29abd2dbaa5f207825", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Problems with representation of tabular data <br><br>When you concatenate normalized numerical data to text embeddings, the text \nembedding can outweigh the numerical information. Imagine you have a 512- \ndimensional text embedding and two numerical columns you want to add. In \nthis case, the dimensionality increases from 512 to 514 dimensions, and the \ntwo additional numerical dimensions will be heavily overshadowed by the much \nlarger text embedding. You therefore need to carefully design the embedding \nstructure to ensure that the numerical data maintains its influence in the similar\u2010 \nity calculations. <br><br>In spite of the above limitations, semantic indexing forms the core of most RAG \napplications in production today. <br><br>**References**\nThe idea of embeddings and why they are so powerful was introduced by Bengio et \nal. (2000). Chris Olah (2014) has an excellent visual explanation of embeddings, and ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3447e428-cf3e-4fae-a5ea-2217c5141c4c": {"__data__": {"id_": "3447e428-cf3e-4fae-a5ea-2217c5141c4c", "embedding": null, "metadata": {"bbox": [{"page": 157, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.04, "x1": 432.01, "y1": 262.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4965ae00-7b58-40ab-a04c-a2623ea21675", "node_type": "1", "metadata": {"bbox": [{"page": 156, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.23, "x1": 432.01, "y1": 287.74}]}, "hash": "48aa8136f34434149897314b986f9ff78dd67a6278e234e887e9ce9b16a4156a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3136f44-1227-4cca-8dcf-dec55eb6347e", "node_type": "1", "metadata": {"bbox": [{"page": 158, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 252.14, "x1": 432.0, "y1": 344.24}]}, "hash": "26fb548d07fd633a5e0376d129ab8f8874ed760f4a61bcd017f886c16f17feed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Data freshness**<br><br>Once you deploy your initial documents into the document store, they inevitably \nbecome outdated as newer information becomes available. Without a systematic \napproach to making content updates, your RAG system will deliver increasingly \nobsolete responses. <br><br>Imagine you\u2019re using a RAG system to advise users on medical questions. The \nhealth authorities might issue new warnings. Medical boards might update dosage \nrecommendations. New viral strains may become prevalent. For example, in early \n2020, the Centers for Disease Control (CDC) recommended a 10-day isolation period \nfor all COVID patients. Then, in December 2021, it updated its guidelines to a 5-day \nisolation period, but only for symptomatic patients. And finally, in February 2024, the \nCDC updated its guidelines again to drop the isolation period completely and simply \nsay that people with COVID should stay home as long as they feel sick. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3136f44-1227-4cca-8dcf-dec55eb6347e": {"__data__": {"id_": "a3136f44-1227-4cca-8dcf-dec55eb6347e", "embedding": null, "metadata": {"bbox": [{"page": 158, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 252.14, "x1": 432.0, "y1": 344.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3447e428-cf3e-4fae-a5ea-2217c5141c4c", "node_type": "1", "metadata": {"bbox": [{"page": 157, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.04, "x1": 432.01, "y1": 262.13}]}, "hash": "76a83c21fb523789850830f014842f1b69ebc2b5edd52e29abd2dbaa5f207825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6e87c30-9065-4c1d-a37f-26a7e024c55b", "node_type": "1", "metadata": {"bbox": [{"page": 160, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.36, "x1": 432.01, "y1": 607.89}]}, "hash": "4b72d0b3e4ea6fd8aba9f1d5dddde187f30ea4a36c6a8f7ca1aab1eae20baee0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the information was revised again. In 2022, the American Academy of Family \nPhysicians (AAFP) released guidelines contradicting the lower ACC/AHA targets, \nnoting that \u201cusing a blood pressure target of 140 / 90 mm Hg reduces cardiovascular \nand all-cause mortality as much as lower targets\u201d while causing fewer adverse effects. <br><br>As you can see in Figure 3-11, adding the information continuously to your RAG sys\u2010 \ntem will lead to misleading and contradicting RAG responses, even in two different \nrequests for the exact same initial user query. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6e87c30-9065-4c1d-a37f-26a7e024c55b": {"__data__": {"id_": "c6e87c30-9065-4c1d-a37f-26a7e024c55b", "embedding": null, "metadata": {"bbox": [{"page": 160, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.36, "x1": 432.01, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a3136f44-1227-4cca-8dcf-dec55eb6347e", "node_type": "1", "metadata": {"bbox": [{"page": 158, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 252.14, "x1": 432.0, "y1": 344.24}]}, "hash": "26fb548d07fd633a5e0376d129ab8f8874ed760f4a61bcd017f886c16f17feed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94a243f9-0d3b-4ada-99ed-df3ef518ddce", "node_type": "1", "metadata": {"bbox": [{"page": 161, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.86, "x1": 543.75, "y1": 184.24}]}, "hash": "84dd3e7ba380769f88079f63a1afafdf80919c98f20f4dc2322800772b35a791", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nAppropriately using metadata can help you handle the need for disambiguation, \nensure that results reflect the latest information, and navigate through contradictory \ninformation. <br><br>**Metadata**<br><br>Using the document\u2019s metadata\u2014which is data that provides information about the \ndocument (such as its author, domain, and name)\u2014provides additional context that \ncan be very beneficial in improving retrieval performance. For example, if you know \nthat a user is interested in financial information, you could filter down the document \nstore to only financial documents (such as United States Securities and Exchange \nCommission [SEC] reports and CNBC news articles). By applying such a metadata \nfilter, you effectively remove any sports-related content, and a query or search for \n\u201cMVP-related information\u201d would not return any sports-related content. Similarly, if \nthe topic category contains physics, you\u2019ll know how to interpret the word fluid. <br><br>There are different types of metadata to include in your document store: <br><br>\u2022 \n\u2022 Document-level metadata <br><br>\u2014 \n\u2014 Source information (URL, document ID, and database origin) <br><br>\u2014 \n\u2014 Creation/modification timestamps <br><br>\u2014 \n\u2014 Author information <br><br>\u2014 \n\u2014 Topic categories or tags <br><br>\u2014 \n\u2014 Reading level or complexity score <br><br>\u2014 \n\u2014 Document length <br><br>\u2022 \n\u2022 Chunk-level metadata <br><br>\u2014 \n\u2014 Position within source document (chapter, section, and paragraph) <br><br>\u2014 \n\u2014 Entities mentioned (people, organizations, and places) <br><br>\u2014 \n\u2014 Semantic role (definition, example, or conclusion) <br><br>\u2014 \n\u2014 Language or locale information <br><br>\u2022 \n\u2022 Domain- and enterprise-specific metadata <br><br>\u2014 \n\u2014 For technical documentation: API versions and programming languages <br><br>\u2014 \n\u2014 For research papers: methodology, sample size, and key findings <br><br>\u2014 \n\u2014 For product information: stock-keeping units (SKUs), pricing, and availability <br><br>\u2014 \n\u2014 For legal documents: jurisdiction, precedents, and statutes <br><br>\u2014 \n\u2014 For knowledge of direct market access (DMA) requirements ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94a243f9-0d3b-4ada-99ed-df3ef518ddce": {"__data__": {"id_": "94a243f9-0d3b-4ada-99ed-df3ef518ddce", "embedding": null, "metadata": {"bbox": [{"page": 161, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.86, "x1": 543.75, "y1": 184.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c6e87c30-9065-4c1d-a37f-26a7e024c55b", "node_type": "1", "metadata": {"bbox": [{"page": 160, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.36, "x1": 432.01, "y1": 607.89}]}, "hash": "4b72d0b3e4ea6fd8aba9f1d5dddde187f30ea4a36c6a8f7ca1aab1eae20baee0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd238d85-0900-4fe3-90d1-895aa7480eaa", "node_type": "1", "metadata": {"bbox": [{"page": 162, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.94, "x1": 432.01, "y1": 324.64}]}, "hash": "25351bdba51af76b946d4e83e25b203b6ffccc371a9d90ac3a5fe3c880345b58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here is an example of how you can discover contradictions through metadata. \nConsider the following two retrieved chunks for the user query \u201cWhat\u2019s the recom\u2010 \nmended treatment for condition X?\u201d <br><br>Here\u2019s Chunk 1: <br><br>Content: \"For patients with condition X, treatment with medication A is recommended as first-line therapy.\" \nMetadata: \n*Source: National Health Guidelines \n*Publication date: March 2023 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd238d85-0900-4fe3-90d1-895aa7480eaa": {"__data__": {"id_": "fd238d85-0900-4fe3-90d1-895aa7480eaa", "embedding": null, "metadata": {"bbox": [{"page": 162, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.94, "x1": 432.01, "y1": 324.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "94a243f9-0d3b-4ada-99ed-df3ef518ddce", "node_type": "1", "metadata": {"bbox": [{"page": 161, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.86, "x1": 543.75, "y1": 184.24}]}, "hash": "84dd3e7ba380769f88079f63a1afafdf80919c98f20f4dc2322800772b35a791", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd9c54f6-ac14-4d5a-82b1-06857bdc3c08", "node_type": "1", "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 292.83, "x1": 432.0, "y1": 605.54}]}, "hash": "0f2bb0ec7b60059a95c83d62328614c576cb5459ca473a5c6f77bdee9b6471e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Retrieval filtering <br><br>You can limit outdated content by restricting the retrieval to only consider docu\u2010 \nment chunks that were published after a certain date. This method will ensure \nthat a result is current. <br><br>Document store pruning <br><br>You can use the metadata to remove from the document store document chunks \nthat are older than a certain number of days. Removing old content from your \ndocument store is preferable to retrieval filtering because the smaller index \nresults in faster retrievals. <br><br>Result reranking <br><br>You can use the metadata to rerank the document chunks after the retrieval step. \nAt that point, you can boost more recent document chunks or chunks from more \ntrustworthy sources. This method prioritizes more relevant content. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd9c54f6-ac14-4d5a-82b1-06857bdc3c08": {"__data__": {"id_": "dd9c54f6-ac14-4d5a-82b1-06857bdc3c08", "embedding": null, "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 292.83, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fd238d85-0900-4fe3-90d1-895aa7480eaa", "node_type": "1", "metadata": {"bbox": [{"page": 162, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.94, "x1": 432.01, "y1": 324.64}]}, "hash": "25351bdba51af76b946d4e83e25b203b6ffccc371a9d90ac3a5fe3c880345b58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35477b1d-1560-4dd5-b094-48c9242debe5", "node_type": "1", "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.34, "x1": 432.0, "y1": 284.73}]}, "hash": "9e77573d716743e99809a29038fd418ec5c1eb8d68f30fd3b958252175323f62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "faced by your RAG applications. At the time of writing, Gemini embeddings was the \nbest overall, but the open Qwen2 model from Alibaba was third on the leaderboard \nand only 10% lower on the aggregate score. OpenAI\u2019s best model, text-embedding-3- \nlarge, was well behind, in 13th position. So, using an open weights model instead of a \nproprietary one that gives you full control over the model lifecycle may involve little \nto no compromise on performance. <br><br>However, this doesn\u2019t mean that you should never switch. Here are some scenarios in \nwhich you should consider switching embedding models: <br><br>\u2022 When significantly more efficient embedding models are released (for example, \n\u2022 \nwhen a newer model can hold the same information content at 25% of the \noriginal dimensionality) <br><br>\u2022 When your application requires a fresh world view, since newer models have a \n\u2022 \nmore recent cutoff date and therefore know about recent events or new terms <br><br>The decision you make when selecting an embedding model is a critical one because \nit will significantly impact how well your RAG application can be maintained over \ntime. Therefore, you should carefully consider all the options before choosing a \nmodel. <br><br>**Example**\nPrefiltering your RAG index based on metadata can drastically improve response \naccuracy. Metadata filtering allows you to do things like filter to specific document \ndomains, creation dates, and document sources. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35477b1d-1560-4dd5-b094-48c9242debe5": {"__data__": {"id_": "35477b1d-1560-4dd5-b094-48c9242debe5", "embedding": null, "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.34, "x1": 432.0, "y1": 284.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dd9c54f6-ac14-4d5a-82b1-06857bdc3c08", "node_type": "1", "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 292.83, "x1": 432.0, "y1": 605.54}]}, "hash": "0f2bb0ec7b60059a95c83d62328614c576cb5459ca473a5c6f77bdee9b6471e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "852d8996-98c6-465f-ac51-c89f5b3e6afa", "node_type": "1", "metadata": {"bbox": [{"page": 165, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.37, "x1": 271.52, "y1": 267.14}]}, "hash": "88a7936c87318dc24b40d6d76c0a4642e1f2e18dba714ea0c20dc639a0543189", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s assume you have structured data around your document chunks. You need to \nannotate each chunk with the original document\u2019s metadata (for example, its creation \ndate and source), as shown here: <br><br>documents = [ \n {'id': 1, \n 'text': '...', \n 'source': 'New York Times', \n 'created_at': '2025-01-01' \n }, \n... \n] <br><br>Aside from your embeddings, you need to create a list of dictionaries that will contain \nthe metadata related to the embedded document chunk. Then, you can create your \ncollection with embeddings together with the metadata. Every vector database has a \nslightly different implementation, but the overall concept is always the same: <br><br>metadata = [] \n**for**j**in**range(len(documents)): ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "852d8996-98c6-465f-ac51-c89f5b3e6afa": {"__data__": {"id_": "852d8996-98c6-465f-ac51-c89f5b3e6afa", "embedding": null, "metadata": {"bbox": [{"page": 165, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.37, "x1": 271.52, "y1": 267.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "35477b1d-1560-4dd5-b094-48c9242debe5", "node_type": "1", "metadata": {"bbox": [{"page": 164, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.34, "x1": 432.0, "y1": 284.73}]}, "hash": "9e77573d716743e99809a29038fd418ec5c1eb8d68f30fd3b958252175323f62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c57d9e-16f2-499c-819b-686482af5725", "node_type": "1", "metadata": {"bbox": [{"page": 166, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.41, "x1": 432.0, "y1": 257.11}]}, "hash": "9ceb97c7915b8453e334797b6b56e5baee65590eddf5a03da09b2c76f70d82c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we receive the results: <br><br>Product: Gaming Laptop Pro <br><br>Created At: 2025-01-01 <br><br>Distance: 0.30149245262145996 <br><br>-------------------------------------------------- <br><br>Product: Gaming Laptop Pro <br><br>Created At: 2024-01-01 <br><br>Distance: 0.30149245262145996 <br><br>-------------------------------------------------- <br><br>Product: Wireless Noise-Cancelling Headphones <br><br>Created At: 2025-01-01 <br><br>Distance: 0.8581079244613647 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90c57d9e-16f2-499c-819b-686482af5725": {"__data__": {"id_": "90c57d9e-16f2-499c-819b-686482af5725", "embedding": null, "metadata": {"bbox": [{"page": 166, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.41, "x1": 432.0, "y1": 257.11}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "852d8996-98c6-465f-ac51-c89f5b3e6afa", "node_type": "1", "metadata": {"bbox": [{"page": 165, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.37, "x1": 271.52, "y1": 267.14}]}, "hash": "88a7936c87318dc24b40d6d76c0a4642e1f2e18dba714ea0c20dc639a0543189", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a74d0b8-1826-43f0-a5e0-b0f87f4efee3", "node_type": "1", "metadata": {"bbox": [{"page": 168, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 134.66, "x1": 432.01, "y1": 582.33}]}, "hash": "140ab6d6282384747639bdd935b7f9ea499574332f53a9fa3824decca5cbe5e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Binary limitations <br><br>Some vector databases only allow binary filters (for example, tag present or \nnot), which significantly restricts the nuance of filtering operations compared to \ncontinuous value filtering. <br><br>Temporal relevance issues <br><br>Simply using dates to determine content relevance can be misleading. An analysis \nfrom 2020 might still be highly relevant, yet recent technical documentation can \nalready be outdated. <br><br>Domain adaptation issues <br><br>Using a metadata filter is domain specific. For example, in a RAG system for \nacademic papers, you might want to filter by academic institution, whereas such \na filter is irrelevant for a customer service application. <br><br>If these limitations are deal breakers, you could consider the following: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a74d0b8-1826-43f0-a5e0-b0f87f4efee3": {"__data__": {"id_": "9a74d0b8-1826-43f0-a5e0-b0f87f4efee3", "embedding": null, "metadata": {"bbox": [{"page": 168, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 134.66, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "90c57d9e-16f2-499c-819b-686482af5725", "node_type": "1", "metadata": {"bbox": [{"page": 166, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.41, "x1": 432.0, "y1": 257.11}]}, "hash": "9ceb97c7915b8453e334797b6b56e5baee65590eddf5a03da09b2c76f70d82c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "945cdc6b-7980-4270-ab1d-201822740b26", "node_type": "1", "metadata": {"bbox": [{"page": 169, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.3, "x1": 432.01, "y1": 271.89}]}, "hash": "0abeeafa26fe655d5385077c7cd566e4dfd8d99143fbe5af060739689055b30b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 4**\n**Adding Knowledge: Syncopation**<br><br>The patterns in this chapter build on the fundamentals of RAG we discussed in \nChapter 3 (see Figure 3-1). We recommend that you read Chapter 3 before this one, \nto learn the fundamental concepts that underlie all RAG use cases. Once you gain an \nunderstanding of the possibilities, you can choose how to implement the components \nof your RAG pipelines based on the characteristics of your use case. We cover that in \nthis chapter. <br><br>**Pattern 9: Index-Aware Retrieval**\nYou can improve on Basic RAG (Pattern 6) and Semantic Indexing (Patterns 7) by \ntaking advantage of knowing what text the chunks contain and how they\u2019ve been \nindexed. Which specific components of this pattern you incorporate will depend on \nthe type of content you have. <br><br>**Problem**\nRAG is based on the assumptions that (1) you can search a knowledge base for \nchunks that are similar to a question and (2) you can use the retrieved chunks to \nground the answer. However, the first assumption does not hold in several situations: \nwhen the question is not present in the knowledge base, when the knowledge base \nuses technical language that is different from what users query for, when the answer \nis a fine detail hidden inside a chunk, and when the answer involves a holistic \ninterpretation of several chunks. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "945cdc6b-7980-4270-ab1d-201822740b26": {"__data__": {"id_": "945cdc6b-7980-4270-ab1d-201822740b26", "embedding": null, "metadata": {"bbox": [{"page": 169, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.3, "x1": 432.01, "y1": 271.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9a74d0b8-1826-43f0-a5e0-b0f87f4efee3", "node_type": "1", "metadata": {"bbox": [{"page": 168, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 134.66, "x1": 432.01, "y1": 582.33}]}, "hash": "140ab6d6282384747639bdd935b7f9ea499574332f53a9fa3824decca5cbe5e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a32575f-0055-4934-baa6-7b25a6f50b5e", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 360.02, "x1": 432.01, "y1": 607.79}]}, "hash": "89210f345c2a9f66754d228690b7d47793c2effddca45869fcbdfcb552029e0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, in the middle of a long paragraph about the architecture of a Nasrid \npalace, there might be a detail about muqarnas, or sculptures on the vaulted ceiling. \nThat detail may not be present in the embedding of the chunk and may therefore not \nbe discoverable. <br><br>To some extent, you can solve this problem by indexing smaller chunks. However, \nhaving small chunks incurs the next problem much more acutely. <br><br>**Answer involves holistic interpretation**<br><br>Suppose you ask a question that requires logical interpretation. The model will need \nto retrieve chunks that match the steps of the logic involved, and the generator will \nhave to piece together these chunks in a logical way. For example, say you were to ask \nthis: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a32575f-0055-4934-baa6-7b25a6f50b5e": {"__data__": {"id_": "1a32575f-0055-4934-baa6-7b25a6f50b5e", "embedding": null, "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 360.02, "x1": 432.01, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "945cdc6b-7980-4270-ab1d-201822740b26", "node_type": "1", "metadata": {"bbox": [{"page": 169, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 112.3, "x1": 432.01, "y1": 271.89}]}, "hash": "0abeeafa26fe655d5385077c7cd566e4dfd8d99143fbe5af060739689055b30b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57f7f572-852a-4d4d-88ef-a7f74d854a28", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 189.14, "x1": 662.75, "y1": 351.92}]}, "hash": "faba66154d94cac6a484ecf318d584382301aad7be0b3acada90fc3114f6db8c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Component 1: Hypothetical answers**<br><br>In situations where doing a semantic search for the query itself would work poorly, \ntry searching for sentences that would answer the query. Suppose you were to take \nthe question about historical attractions and ask a foundational LLM to create an \nanswer without the benefit of your knowledge base. The answer might be something \nalong the lines of this: <br><br>Segovia is a city famous for its well-preserved Roman aqueduct and Alc\u00e1zar castle. Its \nold town is a UNESCO World Heritage site. The train ride from Madrid to Segovia is \ntypically under an hour. <br><br>The issue, of course, is that this answer is not grounded in your knowledge base\u2014and \nyou want to find cities in your knowledge base (presumably because you offer guided \ntours to those places). <br><br>To solve this problem, instead of trying to find matches to the question in the \nknowledge base, you can try to find matches to the hypothetical answer generated by \nthe foundational LLM! Such an approach is likely to return chunks that discuss cities \nwith well-preserved old town cores or train ride times from Madrid. Because the \nacademic literature in this area refers to chunks as documents, this method is called \nhypothetical document embedding (HyDE). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57f7f572-852a-4d4d-88ef-a7f74d854a28": {"__data__": {"id_": "57f7f572-852a-4d4d-88ef-a7f74d854a28", "embedding": null, "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 189.14, "x1": 662.75, "y1": 351.92}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1a32575f-0055-4934-baa6-7b25a6f50b5e", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 360.02, "x1": 432.01, "y1": 607.79}]}, "hash": "89210f345c2a9f66754d228690b7d47793c2effddca45869fcbdfcb552029e0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a161451-7a2b-4465-b5ae-8b9a15bef626", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 70.92, "x1": 423.75, "y1": 168.41}]}, "hash": "33c5693befc20304dfa06d82aa84b7ed0940ea5953f31866e7f606e3ca7efce2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To perform HyDE, use a foundational model to generate a hypothetical answer: <br><br>**def**create_hypothetical_answer(question): \n messages = [ \n ChatMessage(role=\"system\", \n content=\"\"\"Answer the following question in 2-3 sentences. If you don't know the answer, make an educated guess.\"\"\" \n ), \n ChatMessage(role=\"user\", content=question) \n ] \n answer = str(llm.chat(messages)) \n**return**answer <br><br>Then pass this answer to the generation step: <br><br>**def**hyde_rag(question): \nanswer = create_hypothetical_answer(question) \n**return**semantic_rag(answer) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a161451-7a2b-4465-b5ae-8b9a15bef626": {"__data__": {"id_": "2a161451-7a2b-4465-b5ae-8b9a15bef626", "embedding": null, "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 70.92, "x1": 423.75, "y1": 168.41}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "57f7f572-852a-4d4d-88ef-a7f74d854a28", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 189.14, "x1": 662.75, "y1": 351.92}]}, "hash": "faba66154d94cac6a484ecf318d584382301aad7be0b3acada90fc3114f6db8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94d1d4b2-949d-4459-85c3-d1c4c21329ed", "node_type": "1", "metadata": {"bbox": [{"page": 173, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 259.01, "x1": 739.25, "y1": 505.58}]}, "hash": "598b680275ec10154084441938e45ed9c0a46ee82779808b7ee5f22d74591d85", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**HyDE as a Solution to the Problem of Perspectives**\nMany of our examples in this chapter assume that all information is correct and \ninarguable. However, many knowledge bases will contain articles by people with \ndifferent points of view or interpretations of the same events. <br><br>In public health, for example, topics such as mask use, quarantine, abortion, fluo\u2010 \nridation, and needle exchange centers have been subject to vigorous debate. Any \nlarge enough medical database will contain documents that express different points ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94d1d4b2-949d-4459-85c3-d1c4c21329ed": {"__data__": {"id_": "94d1d4b2-949d-4459-85c3-d1c4c21329ed", "embedding": null, "metadata": {"bbox": [{"page": 173, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 259.01, "x1": 739.25, "y1": 505.58}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2a161451-7a2b-4465-b5ae-8b9a15bef626", "node_type": "1", "metadata": {"bbox": [{"page": 172, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 70.92, "x1": 423.75, "y1": 168.41}]}, "hash": "33c5693befc20304dfa06d82aa84b7ed0940ea5953f31866e7f606e3ca7efce2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ed3308c-8ca8-4014-b467-b291fe445981", "node_type": "1", "metadata": {"bbox": [{"page": 174, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.36, "x1": 432.01, "y1": 480.15}]}, "hash": "726266caa40153d70851197c4d62e9c4d24ba5f0445030261845b6b9edac5813", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Component 2: Query expansion**<br><br>Instead of passing the query as-is to a semantic RAG, you can expand the query to \nadd context and translate terms used in it to the ones used in the chunks. Using such \na transformed query may help match chunks better. <br><br>To perform query transformations such as expansion, you can use a foundational \nmodel. The exact context and set of translations will depend on your use case. Here\u2019s \nan example: <br><br>**def**add_context_to_query(question): \n messages = [ \n ChatMessage(role=\"system\", \n content=\"\"\" \nThe following question is about topics discussed in a second century book about Alexander the Great. Clarify the question posed in the following ways: \n*Expand to include second century names. For example, a question about Iranians should include answers about Parthians, Persians, Medes, Bactrians, etc. \n*Provide context on terms. For example, explain that Ammonites came from Jordan or that Philip was the father of Alexander. \nProvide only the clarified question without any preamble or instructions. \n \"\"\".strip() \n ), \n ChatMessage(role=\"user\", content=question) \n ] \n expanded_question = str(llm.chat(messages)) \n**return**expanded_question ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ed3308c-8ca8-4014-b467-b291fe445981": {"__data__": {"id_": "5ed3308c-8ca8-4014-b467-b291fe445981", "embedding": null, "metadata": {"bbox": [{"page": 174, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.36, "x1": 432.01, "y1": 480.15}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "94d1d4b2-949d-4459-85c3-d1c4c21329ed", "node_type": "1", "metadata": {"bbox": [{"page": 173, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 259.01, "x1": 739.25, "y1": 505.58}]}, "hash": "598b680275ec10154084441938e45ed9c0a46ee82779808b7ee5f22d74591d85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17089537-2dd1-437c-b2ec-bf606a344ace", "node_type": "1", "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 288.63, "x1": 432.01, "y1": 605.54}]}, "hash": "68109e9788f10d9b5886bf870f56ca6258e9a267873031c97a2dc5c4e57bafe2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you have a vector store that doesn\u2019t support hybrid search, you can replicate it by \nusing two retrievers and combining their results. We\u2019ll discuss this approach in the \nreranking subsection of the section on Pattern 10, Node Postprocessing. <br><br>**Component 4: GraphRAG**<br><br>The idea behind a GraphRAG is that, having discovered a chunk that has a partial \nanswer to your question, you can retrieve related chunks. This can help you round \nout the context when each chunk is small. You can also index a document with a \nsmall chunk size and then store the embeddings in a tree form so that the embedding \nof a parent node captures the holistic meaning of all the nodes below it. You can even \nrepeat nodes within the RAG if they make sense in different contexts. You can also \nindex the nodes on both keywords and on embeddings to permit different types of \nretrieval for different types of queries. In addition, you can incorporate query-focused \nsummarization into a GraphRAG by pregenerating and storing summaries for all \ngroups of closely related entities. <br><br>In a GraphRAG, the document store is a graph database. A GraphRAG system will \nindex documents into a tree of nodes, use graphs to retrieve related nodes, and \nemploy the relationships between the retrieved nodes to generate the response (see \nFigure 4-2). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17089537-2dd1-437c-b2ec-bf606a344ace": {"__data__": {"id_": "17089537-2dd1-437c-b2ec-bf606a344ace", "embedding": null, "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 288.63, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5ed3308c-8ca8-4014-b467-b291fe445981", "node_type": "1", "metadata": {"bbox": [{"page": 174, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 232.36, "x1": 432.01, "y1": 480.15}]}, "hash": "726266caa40153d70851197c4d62e9c4d24ba5f0445030261845b6b9edac5813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc6da2ac-f693-4dd7-8093-2feba2d5ba00", "node_type": "1", "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.3, "x1": 432.0, "y1": 275.82}]}, "hash": "2f153eaab81db9df2e86b99fa59e24cbd49f699a95949f17d7b7b515a4623dae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Typically, you\u2019ll build a knowledge graph in a graph database such as Neo4j to \nrepresent your knowledge in a domain-specific, structured way, where the nodes \ncapture relationships that are appropriate for your domain. If you don\u2019t have such a \nknowledge graph, you can ask an LLM to do its best to extract entities and tie them \ntogether by using, for example, LangChain: <br><br>llm_transformer = LLMGraphTransformer(llm=llm) \ngraph_documents = llm_transformer.convert_to_graph_documents(text) <br><br>You can then store the extracted graphs in a GraphStore: <br><br>graph_store = Neo4jGraphStore(...) \ngraph_store.write_graph(graph_documents) <br><br>Retrieving chunks from a GraphStore works similarly to using the BM25 or Semantic \nretriever: <br><br>graph_rag_retriever = KnowledgeGraphRAGRetriever(...) \nquery_engine = RetrieverQueryEngine.from_args(graph_rag_retriever) <br><br>GraphRAG systems can leverage explicit entity and relationship representations to \nretrieve related chunks and use them to generate responses. In this way, GraphRAG \nsystems go beyond vector database\u2013based systems that treat retrieved chunks as being \nindependent. <br><br>**Example**\nWe\u2019ll use a second century history book about Alexander the Great to illustrate the \nlimitations of Semantic Indexing and the benefits of index-aware retrieval. The full \ncode is on GitHub. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc6da2ac-f693-4dd7-8093-2feba2d5ba00": {"__data__": {"id_": "fc6da2ac-f693-4dd7-8093-2feba2d5ba00", "embedding": null, "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.3, "x1": 432.0, "y1": 275.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "17089537-2dd1-437c-b2ec-bf606a344ace", "node_type": "1", "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 288.63, "x1": 432.01, "y1": 605.54}]}, "hash": "68109e9788f10d9b5886bf870f56ca6258e9a267873031c97a2dc5c4e57bafe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0138b286-6ae8-4638-b3be-8f80897904dc", "node_type": "1", "metadata": {"bbox": [{"page": 177, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.8, "x1": 432.0, "y1": 326.06}]}, "hash": "a307f8c355cf2219d48c3ecc89ee39b65a978821dcb18bbafc13e0d726972d22", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Limitations of Semantic Indexing**<br><br>Semantic Indexing works well for the most part. For example, you might ask this: <br><br>How did Alexander treat the people of the places he conquered? <br><br>This returns a response grounded in specifics from the book: <br><br>Alexander took Hyparna at first assault but allowed the Greeks to leave the citadel \nunder a truce. \u2026 When the Thebans were defeated, their territory was distributed, \nand the women and children were sold into slavery, with exceptions made for priests, \npriestesses, those bound by hospitality to Philip or Alexander, and public agents of the \nMacedonians. <br><br>Even in situations where the foundational model has the knowledge from its pretrain\u2010 \ning, the RAG response stays within the confines of the knowledge in the book. For \nexample, say you asked this: <br><br>Where did Alexander die? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0138b286-6ae8-4638-b3be-8f80897904dc": {"__data__": {"id_": "0138b286-6ae8-4638-b3be-8f80897904dc", "embedding": null, "metadata": {"bbox": [{"page": 177, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.8, "x1": 432.0, "y1": 326.06}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fc6da2ac-f693-4dd7-8093-2feba2d5ba00", "node_type": "1", "metadata": {"bbox": [{"page": 176, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.3, "x1": 432.0, "y1": 275.82}]}, "hash": "2f153eaab81db9df2e86b99fa59e24cbd49f699a95949f17d7b7b515a4623dae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9db786c1-7f44-4d38-adb2-5025c3408a5f", "node_type": "1", "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 303.07, "x1": 414.0, "y1": 446.29}]}, "hash": "6838abe8000da233b355cccd8ad469fdd8723a14973f5f3f7daf57c527117382", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This produces the following answer: <br><br>Alexander was advised to advance against Darius and the Persians without delay. He \nmarched his troops towards Darius, but a storm delayed him. Upon hearing that \nDarius was in his rear, Alexander sent a ship to Issus to confirm the report. <br><br>The problem here is that this answer is about tactics in a single battle, rather than an \noverarching strategy. <br><br>Another limitation of semantic RAG shows up when the question uses 21st-century \nterms: <br><br>How did the Persian king fight the Greeks? <br><br>This returns the following answer: <br><br>The Persians struck the Greeks on their faces with lances and were repulsed by the \ncavalry. <br><br>This is woefully limited because the retrieved chunks are quite sparse. In Alexander\u2019s \ntime, it was the Parthians fighting the Macedonians, so there are not many chunks \nthat use the words Persian or Greek. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9db786c1-7f44-4d38-adb2-5025c3408a5f": {"__data__": {"id_": "9db786c1-7f44-4d38-adb2-5025c3408a5f", "embedding": null, "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 303.07, "x1": 414.0, "y1": 446.29}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0138b286-6ae8-4638-b3be-8f80897904dc", "node_type": "1", "metadata": {"bbox": [{"page": 177, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.8, "x1": 432.0, "y1": 326.06}]}, "hash": "a307f8c355cf2219d48c3ecc89ee39b65a978821dcb18bbafc13e0d726972d22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "220d9f8c-358c-414e-b5e7-5c90be6a4978", "node_type": "1", "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.94, "x1": 432.0, "y1": 293.48}]}, "hash": "38f722d6f21d0daa45f588bffb04e68f1777974a9f198d228733ecfa48e00148", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Alexander\u2019s army was arranged with the cavalry Companions on the right wing, fol\u2010 \nlowed by the royal squadron and other squadrons. The phalanx of Macedonian infan\u2010 \ntry was positioned near the cavalry, with select corps of shield-bearing guards leading \nthe way. Darius\u2019s army was set up with Bactrian cavalry, Daans, and Arachotians on \nthe left wing; Persians, Susians, and Cadusians near them; and men from Coele-Syria \nand Mesopotamia on the right. Medes, Parthians, Sacians, Tapurians, Hyrcanians, \nAlbanians, and Sacesinians were also positioned on the right. In the center, where \nDarius was, were his kinsmen, Persian guards, Indians, Carians, and Mardian archers. \nThe Uxians, Babylonians, men from the Red Sea, and Sitacenians were also in the \ncenter. Scythian cavalry, Bactrians, and scythe-bearing chariots were posted on the \nleft, while Armenian and Cappadocian cavalry with scythe-bearing chariots were on \nthe right. Greek mercenaries were stationed near Darius, opposite the Macedonian \nphalanx. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "220d9f8c-358c-414e-b5e7-5c90be6a4978": {"__data__": {"id_": "220d9f8c-358c-414e-b5e7-5c90be6a4978", "embedding": null, "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.94, "x1": 432.0, "y1": 293.48}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9db786c1-7f44-4d38-adb2-5025c3408a5f", "node_type": "1", "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 303.07, "x1": 414.0, "y1": 446.29}]}, "hash": "6838abe8000da233b355cccd8ad469fdd8723a14973f5f3f7daf57c527117382", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bde8ec2d-732d-44fb-bb2b-69a4ab8bef65", "node_type": "1", "metadata": {"bbox": [{"page": 179, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.9, "x1": 432.0, "y1": 218.0}]}, "hash": "e0d510a4bf5f2c5d4b9eeaf27baa5714323eb75f2f8e7364396d2195baf01189", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, the HyDE approach can surface the relevant episode about Diogenes, even \nwhen the chunk size is 1,024. The hypothetical answer is this: <br><br>Alexander the Great, known for his ambition and power, is said to have encountered \nthe Cynic philosopher Diogenes, who lived in a large jar. Alexander, impressed by \nDiogenes\u2019s lack of material desires, reportedly asked if he could grant him any wish, to \nwhich Diogenes replied, \u201cStand out of my sun.\u201d This anecdote highlights the contrast \nbetween worldly power and philosophical detachment. <br><br>Enough surrounding details are captured that the model retrieves the relevant \nchunks. A HyDE-based RAG can generate only grounded information, so there\u2019s \nnothing about the philosopher living in a jar, for example: <br><br>Alexander met Diogenes of Sinope in the Isthmus and asked if he wanted anything. \nDiogenes replied that he wanted nothing other than for Alexander and his attendants \nto stand out of the sunlight. Alexander is said to have expressed his admiration of \nDiogenes\u2019s conduct. <br><br>As this example shows, HyDE is helpful when your answer involves logic and you \nneed to retrieve chunks that match that logical structure. It\u2019s also helpful when you ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bde8ec2d-732d-44fb-bb2b-69a4ab8bef65": {"__data__": {"id_": "bde8ec2d-732d-44fb-bb2b-69a4ab8bef65", "embedding": null, "metadata": {"bbox": [{"page": 179, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.9, "x1": 432.0, "y1": 218.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "220d9f8c-358c-414e-b5e7-5c90be6a4978", "node_type": "1", "metadata": {"bbox": [{"page": 178, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.94, "x1": 432.0, "y1": 293.48}]}, "hash": "38f722d6f21d0daa45f588bffb04e68f1777974a9f198d228733ecfa48e00148", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "762f678c-2396-409c-b812-a0133f647080", "node_type": "1", "metadata": {"bbox": [{"page": 180, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 110.23, "x1": 432.01, "y1": 385.94}]}, "hash": "a66386534ede6fca69a0e769b28c3e9029c6a924ece199f59dc708223952d00a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Query expansion is particularly helpful when your users will use nontechnical lan\u2010 \nguage, but your knowledge base is quite technical. You can also use query expansion \nto add meaningful context and disambiguate users\u2019 queries. <br><br>**Considerations**\nBoth hypothetical answers and query expansions are generated based on knowledge \nthe foundational model already has. The foundational model in our example had \nseen training data about Diogenes, so it could expand the query with incidents that \nallowed the RAG system to match a relevant chunk. However, when the RAG system \noperates in a domain that the foundational model has not adequately encountered, \nthe hypothetical answer or query expansion could include hallucinated, obsolete, or ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "762f678c-2396-409c-b812-a0133f647080": {"__data__": {"id_": "762f678c-2396-409c-b812-a0133f647080", "embedding": null, "metadata": {"bbox": [{"page": 180, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 110.23, "x1": 432.01, "y1": 385.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bde8ec2d-732d-44fb-bb2b-69a4ab8bef65", "node_type": "1", "metadata": {"bbox": [{"page": 179, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.9, "x1": 432.0, "y1": 218.0}]}, "hash": "e0d510a4bf5f2c5d4b9eeaf27baa5714323eb75f2f8e7364396d2195baf01189", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ddcd4f9-1413-44d5-b2a0-25a476896701", "node_type": "1", "metadata": {"bbox": [{"page": 181, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.99, "x1": 432.0, "y1": 334.65}]}, "hash": "4ff02161c8c4da04e1b03b1a5105c91bf5194fb98615c2213ffbedbe545fc7b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Graph RAG can also introduce errors in the way related chunks are identified. A \npoorly thought-through solution might bring in conflicting information or earlier \nversions of your documents. <br><br>**References**\nHyDE was introduced by Gao et al. (2022) as a way to improve retrieval when the \ncontent being retrieved won\u2019t match the question but will match its hypothetical \nanswer, even when the answer is wrong. Query expansion has a long history in infor\u2010 \nmation retrieval; a survey of different techniques is available in Azad and Deepak \n(2017), although Song and Zheng (2024) note that the onset of LLMs has caused \na renewal of interest in the broader topic of query optimization, which consists \nof query expansion, decomposition, disambiguation, and abstraction. A survey of \nGraph RAG systems is available in Peng et al. (2024). <br><br>The deepset team employs graph-based RAG as an iterative process and demonstrates \nit on financial data. <br><br>**Pattern 10: Node Postprocessing**\nThe Node Postprocessing pattern inserts a step between retrieval and generation \nto increase relevance, reduce ambiguity, and handle requirements such as content \nupdates and personalization. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ddcd4f9-1413-44d5-b2a0-25a476896701": {"__data__": {"id_": "1ddcd4f9-1413-44d5-b2a0-25a476896701", "embedding": null, "metadata": {"bbox": [{"page": 181, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.99, "x1": 432.0, "y1": 334.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "762f678c-2396-409c-b812-a0133f647080", "node_type": "1", "metadata": {"bbox": [{"page": 180, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 110.23, "x1": 432.01, "y1": 385.94}]}, "hash": "a66386534ede6fca69a0e769b28c3e9029c6a924ece199f59dc708223952d00a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bd1ff87-d0cd-4c1a-894a-649b787d8d0b", "node_type": "1", "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 299.9, "x1": 432.01, "y1": 605.76}]}, "hash": "3fad124b51789a62f182630d39d0fd6600d0eabaeb71e923a5be1966d071e102", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you are adding many chunks, the generation LLM may not be able to keep track of \nthe relevant information it needs to synthesize from the different chunks. <br><br>**Ambiguous entities.**You might get some chunks that refer to the Grand Canyon of \nthe Colorado River and others that are about the Grand Canyon of the Yellowstone. \nThere is no way to disambiguate all such possible pairs. In many cases, you won\u2019t \nknow that you have an ambiguity problem until you get back retrieved nodes that \nrefer to two different entities (for example, if you didn\u2019t know there were two Grand \nCanyons). <br><br>**Conflicting or obsolete content.**Knowledge bases are often updated over time. When \nyou search for support tickets or legal articles, it\u2019s likely that you\u2019ll get some chunks \nthat correspond to obsolete content. While you could build a RAG system that \ndepends on keeping the knowledge base pristine, it\u2019s also likely that you\u2019ll have users \nwho are using different versions of the software or are subject to different local laws, \nso you\u2019ll probably encounter conflicting information in the chunks that you retrieve. <br><br>**Generic answers.**The RAG system works the same way for everyone, so it provides \nthe same answer to a question regardless of who asks it. So how do you personalize \nthe answer it generates with information that\u2019s relevant to the person asking the \nquestion? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bd1ff87-d0cd-4c1a-894a-649b787d8d0b": {"__data__": {"id_": "0bd1ff87-d0cd-4c1a-894a-649b787d8d0b", "embedding": null, "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 299.9, "x1": 432.01, "y1": 605.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1ddcd4f9-1413-44d5-b2a0-25a476896701", "node_type": "1", "metadata": {"bbox": [{"page": 181, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.99, "x1": 432.0, "y1": 334.65}]}, "hash": "4ff02161c8c4da04e1b03b1a5105c91bf5194fb98615c2213ffbedbe545fc7b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50896a58-ea65-4fb7-b26b-f7fa3ffb0801", "node_type": "1", "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 107.01, "x1": 432.0, "y1": 287.09}]}, "hash": "045de7993bcc0ce339c7c0b34f1297457aedb8a2f1be233f6f974f87aaa45575", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Query**: <br><br>{query} <br><br>**Full Text**: <br><br>{node.text} <br><br>This is an example of Pattern 17, LLM-as-Judge, which we\u2019ll discuss in Chapter 6. <br><br>Rerankers are much more accurate than embedding models because an embedding \nmodel has to compress all the information in the chunk into a single embedding \nvector. On the other hand, a reranker can look at the chunk in detail and determine \nwhether the answer exists. <br><br>Reranking is so beneficial to RAG systems that there are models, such as BGE, that \nare fine-tuned to perform this task very well. You don\u2019t need to run BGE locally\u2014 \nhosted versions are also available. To use the hosted version of BGE to rerank, for \nexample, you can use the Pinecone API to hit a deployed BGE endpoint: <br><br>reranked_nodes = pc.inference.rerank( \n model=\"bge-reranker-v2-m3\", \n query=query, \n documents=nodes, \n top_n=3, \n return_documents=**True**, \n) <br><br>You\u2019ll need to pass each of the retrieved nodes to the reranker so that it can compute \na relevance score. Therefore, reranking greatly increases the number of LLM calls \nyou\u2019ll need to perform, and it also adds latency and cost. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50896a58-ea65-4fb7-b26b-f7fa3ffb0801": {"__data__": {"id_": "50896a58-ea65-4fb7-b26b-f7fa3ffb0801", "embedding": null, "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 107.01, "x1": 432.0, "y1": 287.09}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0bd1ff87-d0cd-4c1a-894a-649b787d8d0b", "node_type": "1", "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 299.9, "x1": 432.01, "y1": 605.76}]}, "hash": "3fad124b51789a62f182630d39d0fd6600d0eabaeb71e923a5be1966d071e102", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13c8e459-b0f9-4c66-bcfa-7a76a5090bf1", "node_type": "1", "metadata": {"bbox": [{"page": 185, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.24, "x1": 505.5, "y1": 347.31}]}, "hash": "1507dd35b2427d237166cbb40c2e707ccccaa28bf1cb23a27de2ef9ecc774d94", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Hybrid search**<br><br>If you are not going to use the nodes from the retriever as is but you plan to rerank \nthem, you don\u2019t have to settle for using just one retriever. The main stumbling block \nto using both a BM25 retriever and a semantic retriever is that you have no easy way \nto compare a chunk that has a BM25 score with a chunk that has a cosine similarity. <br><br>The solution we suggested in Pattern 9, Index-Aware Retrieval, was to compute both \nscores for each chunk and then to compute a weighted sum of the two scores. But if \nyou\u2019re reranking chunks anyway, there\u2019s a better approach. <br><br>You can use different types of retrievers and combine all of the results into a single \nlist. Then, you can pass this list of chunks, from different retrievers, into the reranker. \nFinally, you can take the top-K chunks based on the relevance score. This idea \nis called hybrid search because it was originally used to combine keyword-based \nindexing with semantic indexing. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13c8e459-b0f9-4c66-bcfa-7a76a5090bf1": {"__data__": {"id_": "13c8e459-b0f9-4c66-bcfa-7a76a5090bf1", "embedding": null, "metadata": {"bbox": [{"page": 185, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.24, "x1": 505.5, "y1": 347.31}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "50896a58-ea65-4fb7-b26b-f7fa3ffb0801", "node_type": "1", "metadata": {"bbox": [{"page": 184, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 107.01, "x1": 432.0, "y1": 287.09}]}, "hash": "045de7993bcc0ce339c7c0b34f1297457aedb8a2f1be233f6f974f87aaa45575", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "854b0a35-d101-4c22-b6aa-44a9feb47bcc", "node_type": "1", "metadata": {"bbox": [{"page": 186, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 85.65, "x1": 432.0, "y1": 298.05}]}, "hash": "5e5db5b5b7d3e519994a736d5f03531e3dce0062afe16b4dc69053e670383faa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You could use an LLM to identify whether two chunks have conflicting information, \nbut this involves processing nodes in pairs, and the combinatorial explosion in the \nnumber of LLM calls (N*(N-1)) may make it cost prohibitive in practical applica\u2010 \ntions. <br><br>**Contextual compression**<br><br>The retrieved chunks may contain a lot of irrelevant information. If you are going to \nbe processing each chunk individually with an LLM to compute its relevance score, \nyou can use that opportunity to also compress the text to just the parts that are \nrelevant to answering the query: <br><br>**from pydantic_ai import**Agent \n@dataclass \n**class Chunk**: \n full_text: str \n relevant_text: str \n relevance_score: float <br><br>**def**process_node(query, node): \n system_prompt = \"\"\" \n You will be given a query and some text. \n 1. Remove information from the text that is not relevant to answering the question. \n 2. Assign a relevance score between 0 and 1, where 1 means that the text answers the question \n \"\"\" \n agent = Agent(model, result_type=Chunk, system_prompt=system_prompt) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "854b0a35-d101-4c22-b6aa-44a9feb47bcc": {"__data__": {"id_": "854b0a35-d101-4c22-b6aa-44a9feb47bcc", "embedding": null, "metadata": {"bbox": [{"page": 186, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 85.65, "x1": 432.0, "y1": 298.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "13c8e459-b0f9-4c66-bcfa-7a76a5090bf1", "node_type": "1", "metadata": {"bbox": [{"page": 185, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.24, "x1": 505.5, "y1": 347.31}]}, "hash": "1507dd35b2427d237166cbb40c2e707ccccaa28bf1cb23a27de2ef9ecc774d94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a865682-26ff-45b3-ade3-c6c1ff08a8c2", "node_type": "1", "metadata": {"bbox": [{"page": 187, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.95, "x1": 432.0, "y1": 308.82}]}, "hash": "0695be9e1912e2a0d91aa791ecd4a8b2f30993beaa7b9da6ffb4d0515a0d91c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unlike with conflicting information, there is no need to process all possible pairs of \nchunks\u2014it\u2019s enough to ensure that the entity referred to in the first chunk is the same \none referred to in subsequent chunks. So, the number of calls needed is only N \u2013 1: <br><br>**for**node**in**response['source_nodes'][1:]: \n result = disambiguate(query, response['source_nodes'][0], node) \n**if**result.is_ambiguous: \n*# ask follow-up question*<br><br>As with contextual compression, you can fold the disambiguation LLM call into the \nrelevance computation LLM call. <br><br>**Personalization and conversation history**<br><br>Inserting a step between retrieval and generation provides you with the ability to add \ninformation to the context beyond what\u2019s retrieved. For example, you can look at the \nretrieved nodes and pull in relevant information from the user\u2019s context. <br><br>This could be as simple as inserting data from a user\u2019s past transactions, if it\u2019s relevant \nto the query. For example, a travel chatbot might incorporate your personal travel ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a865682-26ff-45b3-ade3-c6c1ff08a8c2": {"__data__": {"id_": "2a865682-26ff-45b3-ade3-c6c1ff08a8c2", "embedding": null, "metadata": {"bbox": [{"page": 187, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.95, "x1": 432.0, "y1": 308.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "854b0a35-d101-4c22-b6aa-44a9feb47bcc", "node_type": "1", "metadata": {"bbox": [{"page": 186, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 85.65, "x1": 432.0, "y1": 298.05}]}, "hash": "5e5db5b5b7d3e519994a736d5f03531e3dce0062afe16b4dc69053e670383faa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cf603d0-03b6-4735-9528-a1930bf6d404", "node_type": "1", "metadata": {"bbox": [{"page": 188, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.6, "x1": 432.0, "y1": 353.66}]}, "hash": "4f626ff9264ea316228643040f7e854f13b09fe886ad6cc56555848f378db354", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Limitations of semantic RAG**<br><br>Retrieve top_k=2 nodes for the following query: <br><br>Describe the geology of the Grand Canyon. <br><br>This yields, as the top result, a node with the table of contents: <br><br>Node ID: 7b635fb9-7b61-4508-ad6a-370f5cd42822 <br><br>Text: W. M. DAVIS HARVARD UNIVERSITY, CAMBRIDGE, MASS. JULY, <br><br>1905 CONTENTS INTRODUCTION.--THE SCOPE AND AIM OF <br><br>GEOLOGY PART I EXTERNAL GEOLOGICAL AGENCIES I. THE WORK <br><br>OF THE WEATHER <br><br>So, in essence, the result is generated from just the second node. Unfortunately, the \nsecond node is similar to the query (in that the Grand Canyon appears prominently) \nbut not really relevant to providing the answer to the question. The key sentence \nwithin the chunk starts with this: <br><br>The high plateaus of northern Arizona and southern Utah, north of the Grand Canyon \nof the Colorado River \u2026 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5cf603d0-03b6-4735-9528-a1930bf6d404": {"__data__": {"id_": "5cf603d0-03b6-4735-9528-a1930bf6d404", "embedding": null, "metadata": {"bbox": [{"page": 188, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.6, "x1": 432.0, "y1": 353.66}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2a865682-26ff-45b3-ade3-c6c1ff08a8c2", "node_type": "1", "metadata": {"bbox": [{"page": 187, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.95, "x1": 432.0, "y1": 308.82}]}, "hash": "0695be9e1912e2a0d91aa791ecd4a8b2f30993beaa7b9da6ffb4d0515a0d91c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40b09d16-0067-4608-a0e8-4b7afe9b8432", "node_type": "1", "metadata": {"bbox": [{"page": 189, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.12, "x1": 432.0, "y1": 318.63}]}, "hash": "e86319ea0778ef9c3b21cd067d138b89ae1e9094d6aedee44448132dc38be928", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just using more nodes is not enough, so what happens if you apply Node Postprocess\u2010 \ning? <br><br>**Impact of Node Postprocessing**<br><br>If you postprocess the retrieved nodes to compress the chunk text to the relevant bits \nand rank the chunks on relevance, then the top two nodes will have directly relevant \nanswers. The relevant text of the most relevant chunk starts with this: <br><br>Running water has gulched the walls, and weathering has everywhere attacked and \ndriven them back. The horizontal harder beds stand out in long lines of vertical cliffs, \noften hundreds of feet in height, at whose feet talus slopes conceal the outcrop of the \nweaker strata. <br><br>The second chunk is what used to be most relevant: <br><br>The high plateaus of northern Arizona and southern Utah, north of the Grand Canyon \nof the Colorado River, are composed of stratified rocks more than ten thousand feet \nthick and of very gentle inclination northward. <br><br>The generated response is now very pertinent because it synthesizes the two nodes: <br><br>The Grand Canyon of the Colorado River is located in the high plateaus of northern \nArizona and southern Utah. The canyon walls have been shaped by running water \nand weathering, with horizontal harder beds forming vertical cliffs and talus slopes \nconcealing weaker strata. The sides of the canyon descend to the river by gigantic ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40b09d16-0067-4608-a0e8-4b7afe9b8432": {"__data__": {"id_": "40b09d16-0067-4608-a0e8-4b7afe9b8432", "embedding": null, "metadata": {"bbox": [{"page": 189, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.12, "x1": 432.0, "y1": 318.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5cf603d0-03b6-4735-9528-a1930bf6d404", "node_type": "1", "metadata": {"bbox": [{"page": 188, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.6, "x1": 432.0, "y1": 353.66}]}, "hash": "4f626ff9264ea316228643040f7e854f13b09fe886ad6cc56555848f378db354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a747d8ec-47ef-42fe-b828-8fce713bd15c", "node_type": "1", "metadata": {"bbox": [{"page": 190, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 111.94, "x1": 432.0, "y1": 262.0}]}, "hash": "e8fa9c28ea752cff1c736dea4a1c93390731321a5aac37d8b23ac4545c92d761", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The more tasks you perform in Node Postprocessing, the more it makes sense to \nfold all the operations into a single LLM call. To ensure that all the operations are \nperformed, use a structured output (see Pattern 2, Grammar, in Chapter 2). If you do \nthis, you can\u2019t use a fine-tuned model like BGE\u2014you\u2019ll need a foundational model \nthat\u2019s capable of performing all the tasks. <br><br>**References**\nGuo et al. (2019) surveyed neural ranking models for information retrieval. Ma et \nal. (2023) showed how to fine-tune the LLaMA model, both as a dense retriever \n(RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval \nand document retrieval. Sourav Verma (2024) examined contextual compression \ntechniques and their benefits. Chen et al. (2021) pointed to the prevalence of ambi\u2010 \nguity as a source of error in RAG and proposed a benchmark to evaluate entity \ndisambiguation approaches. <br><br>Bench.io expands queries to better contextualize the results of generation. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a747d8ec-47ef-42fe-b828-8fce713bd15c": {"__data__": {"id_": "a747d8ec-47ef-42fe-b828-8fce713bd15c", "embedding": null, "metadata": {"bbox": [{"page": 190, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 111.94, "x1": 432.0, "y1": 262.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "40b09d16-0067-4608-a0e8-4b7afe9b8432", "node_type": "1", "metadata": {"bbox": [{"page": 189, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.12, "x1": 432.0, "y1": 318.63}]}, "hash": "e86319ea0778ef9c3b21cd067d138b89ae1e9094d6aedee44448132dc38be928", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adfddc27-2db8-48e8-89d7-51399d47bbe2", "node_type": "1", "metadata": {"bbox": [{"page": 191, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.36, "x1": 432.01, "y1": 320.66}]}, "hash": "9104b3e6be871f48825321540881c664c9ca07a6400bb71e2da2d1beceb39710", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is, currently, no way to completely avoid these issues. However, there are best \npractices that enable you to evaluate the trustworthiness of generated answers, or \neven parts of answers, and to pass that information on to consumers of your RAG \nanswers. In situations where the generated answers will be used to drive actions, it \nis important to portray this trustworthiness in a way that lets users decide whether \nor not to carry out such actions. For example, consider a RAG system for medical \napplications of which a doctor or patient could ask the following question: <br><br>What are the best treatment options for Type 1 diabetics? <br><br>It would be important to do everything you can to ensure that the answer is correct \nand portray to the user whether any of the treatment options might be outdated or \ncome from non-peer-reviewed sources. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "adfddc27-2db8-48e8-89d7-51399d47bbe2": {"__data__": {"id_": "adfddc27-2db8-48e8-89d7-51399d47bbe2", "embedding": null, "metadata": {"bbox": [{"page": 191, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.36, "x1": 432.01, "y1": 320.66}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a747d8ec-47ef-42fe-b828-8fce713bd15c", "node_type": "1", "metadata": {"bbox": [{"page": 190, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 111.94, "x1": 432.0, "y1": 262.0}]}, "hash": "e8fa9c28ea752cff1c736dea4a1c93390731321a5aac37d8b23ac4545c92d761", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4289e806-c93a-4d61-91a6-aad69899c868", "node_type": "1", "metadata": {"bbox": [{"page": 193, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "a48b1f16eb52bca8e42612c05d7595ecf1f825469a307cb445b50457025491cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zero-shot classifications to categorize queries <br><br>Use a pretrained language model, perhaps a small language model, that performs \nreally well for zero-shot classifications. Define categories for your queries that \nyou would expect. For example, in a medical RAG example, you could set the \ncategories to [\u201cMedical\u201d, \u201cNot Medical\u201d]. If the zero-shot classification model \nreturns \u201cNot Medical\u201d with a high likelihood, you could declare it an out-of- \ndomain detection. If the queries change in context, add examples to your query \nproblem and turn the zero-shot problem into a few-shot problem. If you ask the \nmodel to detect multiple categories, such as [\u201cFinance,\u201d \u201cHealth\u201d, \u201cTechnology\u201d, \n\u201cOther\u201d], you could require at least 30% confidence as a threshold before you \nanswer the question. <br><br>Requiring domain-specific keywords or terminology <br><br>One of the most restrictive approaches is to require a question (and/or the \nanswer) to use specific keywords or terminology. In the case of a medical RAG \nsystem, you could require the presence of at least one or more terms from a \nmedical dictionary. <br><br>Because each of the above methods has its benefits and disadvantages, you might \nchoose a hybrid approach (such a weighted sum of the three signals) to detect \nout-of-domain queries. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4289e806-c93a-4d61-91a6-aad69899c868": {"__data__": {"id_": "4289e806-c93a-4d61-91a6-aad69899c868", "embedding": null, "metadata": {"bbox": [{"page": 193, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "adfddc27-2db8-48e8-89d7-51399d47bbe2", "node_type": "1", "metadata": {"bbox": [{"page": 191, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.36, "x1": 432.01, "y1": 320.66}]}, "hash": "9104b3e6be871f48825321540881c664c9ca07a6400bb71e2da2d1beceb39710", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c7d7afd-5a7b-48c5-ab3c-c925372cfede", "node_type": "1", "metadata": {"bbox": [{"page": 194, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.02, "y1": 601.79}]}, "hash": "da4c906f4725123b87ce39a16407b491c163aa6a292a007aa9327a64d698dc9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "store. When you retrieve document chunks to generate a response, you pass the \ncorresponding metadata to a citation-aware prompt to generate the response. Then, \nyou can postprocess the citations, such as by formatting them to the correct citation \nstyle (see Figure 4-5). <br><br><br><br>Figure 4-5. Workflow to generate RAG responses with citations <br><br>You can find an example implementation of this approach in the GitHub repository \nof this book. It is also discussed in greater detail in the Example subsection later in \nthis section. <br><br>**176** **|** **Chapter 4: Adding Knowledge: Syncopation**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c7d7afd-5a7b-48c5-ab3c-c925372cfede": {"__data__": {"id_": "5c7d7afd-5a7b-48c5-ab3c-c925372cfede", "embedding": null, "metadata": {"bbox": [{"page": 194, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.02, "y1": 601.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4289e806-c93a-4d61-91a6-aad69899c868", "node_type": "1", "metadata": {"bbox": [{"page": 193, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "a48b1f16eb52bca8e42612c05d7595ecf1f825469a307cb445b50457025491cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d09a041-3686-4cbb-a4f8-d54cbaad88ad", "node_type": "1", "metadata": {"bbox": [{"page": 195, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249996}]}, "hash": "4cb11c1b1c05246b03aeec3cecd728e00567c292cfff73e425a9756797500de4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Classification-based citations.**If you want to have more control over the citations, \nusing the classification approach is a good option. A classifier can distinguish between \ncommon knowledge that doesn\u2019t require any citations and factual claims that require \ncitations. The latter can vary from domain to domain: academic papers require more \ncitations than general domain content. Figure 4-6 shows an example of a response \nthat differentiates between \u201ccommon\u201d knowledge and citation-requiring statements \nlike \u201che has been widely regarded as one of the greatest composers in the history of \nWestern music.\u201d. <br><br><br><br>Figure 4-6. Using a classifier to detect citation requirements <br><br>Using the classification-based approach increases the system\u2019s complexity, since you \nwill need to provide or fine-tune a citation classifier. However, the citations will be \nmore precise. The source-tracking approach is simpler, but it tends to over-cite. <br><br>**Token-level attribution.**When a RAG system generates a response, the generation can \nblur the boundaries of the original chunks or multiple chunks can contribute to a \nsingle statement. Some LLMs tend to paraphrase chunks in ways that will alter the \ndirect connection to the source chunk, and the paraphrasing can make direct text \nmatching difficult. <br><br>You can address those more complex citation scenarios by using token-based attribu\u2010 \ntion, in which you chunk your texts and tag them with the metadata as before. The \ndifference is that, during the generation process, the metadata is tracked through the \nLLM\u2019s attention mechanism, which allows you to attribute every generated token with \none or more sources (see Figure 4-7). You can then set a threshold above which you \nwant the LLM to report citations in the final generated response. <br><br>**Pattern 11: Trustworthy Generation** **|** **177**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d09a041-3686-4cbb-a4f8-d54cbaad88ad": {"__data__": {"id_": "8d09a041-3686-4cbb-a4f8-d54cbaad88ad", "embedding": null, "metadata": {"bbox": [{"page": 195, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249996}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5c7d7afd-5a7b-48c5-ab3c-c925372cfede", "node_type": "1", "metadata": {"bbox": [{"page": 194, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.02, "y1": 601.79}]}, "hash": "da4c906f4725123b87ce39a16407b491c163aa6a292a007aa9327a64d698dc9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c63ff9e2-3d0c-49d3-928a-305929bcca17", "node_type": "1", "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 254.9, "x1": 432.01, "y1": 607.79}]}, "hash": "198eb36f463c345c9586814453fdf871973ac41ba300cf89ab4520e5c0baaea9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 4-7. Token-level attribution tracking for citations <br><br>The token-level approach addresses paraphrasing by the LLM without losing the \nattribution; it also supports mixed-source attributions for your final responses. At the \ntime of writing, token-level attribution is an area of active research. No production- \nready open-source implementation has yet emerged. <br><br>**178** **|** **Chapter 4: Adding Knowledge: Syncopation**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c63ff9e2-3d0c-49d3-928a-305929bcca17": {"__data__": {"id_": "c63ff9e2-3d0c-49d3-928a-305929bcca17", "embedding": null, "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 254.9, "x1": 432.01, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8d09a041-3686-4cbb-a4f8-d54cbaad88ad", "node_type": "1", "metadata": {"bbox": [{"page": 195, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249996}]}, "hash": "4cb11c1b1c05246b03aeec3cecd728e00567c292cfff73e425a9756797500de4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd3aaca5-e2ab-440a-9b43-07097cac6d17", "node_type": "1", "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 242.8}]}, "hash": "418965d001d0df562e41de2c4d47c732a51fc8b0acd2643eea89809af36f5d09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Guardrails**<br><br>You can reduce the chance of the LLM \u201cgoing rogue\u201d by applying guardrails through\u2010 \nout your RAG pipeline (see Figure 4-8). <br><br>Before retrieving chunks from your document store, you can do the following: <br><br>\u2022 \n\u2022 Filter harmful or inappropriate queries through out-of-domain detection. <br><br>\u2022 \n\u2022 Sanitize user input to the RAG system to prevent any form of injection attacks via <br><br>prompts. <br><br>\u2022 \n\u2022 Filter document-store chunks to include only those with highly trusted sources. <br><br>Once you have retrieved relevant document chunks, you can do the following to \napply guardrails: <br><br>\u2022 Enforce tracking of chunk metadata like sources and other attributions. \n\u2022 <br><br>\u2022 \n\u2022 Prioritize document chunks based on the sources and relevance sources, filtering <br><br>out chunks below a minimum threshold. <br><br>\u2022 \n\u2022 Fact-check against trusted sources using reflective RAGs. <br><br>\u2022 \n\u2022 Check the retrieved chunks for privacy compliance before passing them on for <br><br>the response regeneration. <br><br>Ahead of the generation, you can do the following to apply more guardrails: <br><br>\u2022 \n\u2022 Check the chunks for data freshness, for example, by excluding chunks more <br><br>than six months old. <br><br>\u2022 \n\u2022 Enforce having chunks from multiple sources to promote source diversity. <br><br>\u2022 \n\u2022 Check chunks for harmful content. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd3aaca5-e2ab-440a-9b43-07097cac6d17": {"__data__": {"id_": "fd3aaca5-e2ab-440a-9b43-07097cac6d17", "embedding": null, "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 242.8}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c63ff9e2-3d0c-49d3-928a-305929bcca17", "node_type": "1", "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 254.9, "x1": 432.01, "y1": 607.79}]}, "hash": "198eb36f463c345c9586814453fdf871973ac41ba300cf89ab4520e5c0baaea9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94c1a00f-56f5-4c6f-836a-8ff080fc0517", "node_type": "1", "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 208.21, "x1": 432.01, "y1": 607.79}]}, "hash": "a84b8814cd720306e8cb6f9254843091619d83d281c5c7dc2395056a62d7f61a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dismiss any chunks that don\u2019t pass the checks\u2014do not pass them to the response \ngeneration stage. <br><br>Once your RAG generates a response, you can apply the following final guardrails \nbefore returning any response to your user: <br><br>\u2022 \n\u2022 Enforce citations and source attributions. <br><br>\u2022 \n\u2022 Fact check against trusted sources. <br><br>\u2022 \n\u2022 Check for data privacy. <br><br>\u2022 \n\u2022 Check response for accidental harmful generations. <br><br>If any guardrails fail, rewrite the user query and then trigger a new generation or even \na new retrieval of chunks. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94c1a00f-56f5-4c6f-836a-8ff080fc0517": {"__data__": {"id_": "94c1a00f-56f5-4c6f-836a-8ff080fc0517", "embedding": null, "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 208.21, "x1": 432.01, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fd3aaca5-e2ab-440a-9b43-07097cac6d17", "node_type": "1", "metadata": {"bbox": [{"page": 196, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 242.8}]}, "hash": "418965d001d0df562e41de2c4d47c732a51fc8b0acd2643eea89809af36f5d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "172524da-5ec4-474e-9f47-bbb4e856e49a", "node_type": "1", "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 101.41, "x1": 432.01, "y1": 200.11}]}, "hash": "9bd0ae64516219804940419518cb63620ed5702e0a0dafd3d9b5f30418e4a8e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Observability**<br><br>Another way of increasing user trust in RAG responses is by continuously generating \nobservability metrics around your LLM generations and RAG responses. Observabil\u2010 \nity tools allow you to track LLM input and outputs, and such tools automatically \ngenerate metrics like these: <br><br>\u2022 \n\u2022 Context relevance <br><br>\u2022 \n\u2022 Response relevance <br><br>\u2022 \n\u2022 Faithfulness <br><br>\u2022 \n\u2022 Context recall and precision <br><br>While you could implement your own observability tools, we recommend using one \nof the available libraries. Open-source solutions include Arize Phoenix, Comet Opik, \nLangfuse, and Langtrace. <br><br>We\u2019ve observed that implementing rigorous observability for query-response pairs in \nRAG systems consistently leads to measurable improvements in user trust and system \nadoption. When stakeholders can transparently evaluate the correlation between \ninputs and outputs, their confidence in the system\u2019s reliability and accuracy increases. \nThis empirical approach to RAG system evaluation not only enhances trust, but \nit also provides valuable data points for continuous system optimization and perfor\u2010 \nmance validation. <br><br>**Human Feedback**<br><br>You can incorporate human feedback into your RAG pipelines both online and \noffline (see Figure 4-9). If you prefer to have a human in the loop, you could add a \nhuman review step after document chunk retrieval, with a reviewer giving the chunks \nup or down votes to update the chunks\u2019 relevance before you use them for response \ngeneration. You can also send all generated responses, or those that fall below some \nconfidence threshold, to a review queue for domain experts to review before sending \nthem back to the requesting user. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "172524da-5ec4-474e-9f47-bbb4e856e49a": {"__data__": {"id_": "172524da-5ec4-474e-9f47-bbb4e856e49a", "embedding": null, "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 101.41, "x1": 432.01, "y1": 200.11}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "94c1a00f-56f5-4c6f-836a-8ff080fc0517", "node_type": "1", "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 208.21, "x1": 432.01, "y1": 607.79}]}, "hash": "a84b8814cd720306e8cb6f9254843091619d83d281c5c7dc2395056a62d7f61a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae310ad9-2bfe-4392-863b-81f307b9605d", "node_type": "1", "metadata": {"bbox": [{"page": 200, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 513.44, "x1": 432.0, "y1": 605.54}]}, "hash": "db280127de5a80b8318686a37140d243cef1c88ac2a2cc7ef003b26cb62ee8bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Humans who perform offline reviews can provide actionable reviews for your RAG \npipeline by using only a subset of all the information going through them. First, they \nshould review the ranking of retrieved chunks. The human ranking can be the basis \nfor a domain-specific embedding model for your user case. You can also use the \nreview ranking to fine-tune any LLM if you want to rewrite user queries to improve \nretrieval performance. Domain experts can also review a subset of all generated \nresponses for correctness and relevance; then, you can use the updated responses to \nfine-tune an LLM for your domain-specific use case. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae310ad9-2bfe-4392-863b-81f307b9605d": {"__data__": {"id_": "ae310ad9-2bfe-4392-863b-81f307b9605d", "embedding": null, "metadata": {"bbox": [{"page": 200, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 513.44, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "172524da-5ec4-474e-9f47-bbb4e856e49a", "node_type": "1", "metadata": {"bbox": [{"page": 198, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 101.41, "x1": 432.01, "y1": 200.11}]}, "hash": "9bd0ae64516219804940419518cb63620ed5702e0a0dafd3d9b5f30418e4a8e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3a96d31-d0bc-47bb-a47a-b9cf93cbaa75", "node_type": "1", "metadata": {"bbox": [{"page": 202, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 374.9, "x1": 432.0, "y1": 607.79}]}, "hash": "5dfe498817124b35c9e1cf16436e3b868c02d4878e6aa515341434ee655728bf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "helpful, since knowledge bases tend to be static and online content updates tend to \nbe faster. Second, with a \u201cdecompose-then-recompose\u201d strategy, CRAG can help the \nLLM focus on key information by filtering out apparently irrelevant content from \nretrieved documents. <br><br>CRAG can fully replace irrelevant content with search results while combining \nambiguous content with search results before passing it on for response generation \n(see Figure 4-10). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3a96d31-d0bc-47bb-a47a-b9cf93cbaa75": {"__data__": {"id_": "d3a96d31-d0bc-47bb-a47a-b9cf93cbaa75", "embedding": null, "metadata": {"bbox": [{"page": 202, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 374.9, "x1": 432.0, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ae310ad9-2bfe-4392-863b-81f307b9605d", "node_type": "1", "metadata": {"bbox": [{"page": 200, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 513.44, "x1": 432.0, "y1": 605.54}]}, "hash": "db280127de5a80b8318686a37140d243cef1c88ac2a2cc7ef003b26cb62ee8bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8685acb9-9433-4fac-b3e5-ad8edfaf158d", "node_type": "1", "metadata": {"bbox": [{"page": 204, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "1d8106ab53c5556fabefbf3270caf9a32e89e96bcbe617300e677663679bd1ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Self-RAG**<br><br>CRAG workflows try to review and correct retrieved document chunks before the \nresponse is generated, to prevent potential hallucinations. In contrast, self-RAG, or \nreflection in RAG (see Figure 4-11), critically examines the overall process, starting \nfrom the generated output or retrieval results. The goal is to refine retrieval and \ngeneration processes. Self-RAG consists of three elements: <br><br>Self-evaluation <br><br>The system critiques its retrieved documents for relevance and quality. <br><br>Adaptive retrieval <br><br>The system can decide whether to retrieve more information or proceed with \ngeneration. <br><br>Controlled generation <br><br>The system can modulate between relying on retrieved information and relying \non parametric knowledge from its underlying LLM. <br><br>You can accomplish all of these by using prompts (see Pattern 17, LLM-as-Judge, in \nChapter 6). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8685acb9-9433-4fac-b3e5-ad8edfaf158d": {"__data__": {"id_": "8685acb9-9433-4fac-b3e5-ad8edfaf158d", "embedding": null, "metadata": {"bbox": [{"page": 204, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d3a96d31-d0bc-47bb-a47a-b9cf93cbaa75", "node_type": "1", "metadata": {"bbox": [{"page": 202, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 374.9, "x1": 432.0, "y1": 607.79}]}, "hash": "5dfe498817124b35c9e1cf16436e3b868c02d4878e6aa515341434ee655728bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "077d3865-4b3a-40a1-96f3-f712713327fa", "node_type": "1", "metadata": {"bbox": [{"page": 205, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 330.93, "x1": 432.01, "y1": 605.54}]}, "hash": "008c9d2b577136fef9713d7c4670a8b71fd1facc9476610b0e68687d2043bce5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG system from a mysterious black box into a reliable research assistant whose \nwork can be verified and trusted. <br><br><br><br>Figure 4-12. Some of the ways in which Perplexity AI\u2019s search interface fosters trust in its \ngenerated answers <br><br>User trust is further enhanced through thoughtful interaction patterns that give users \nappropriate control over the RAG process. Allowing them to adjust search parame\u2010 \nters, filter results by date or source authority, and refine queries based on initial \nresponses creates a collaborative experience, rather than one-way information deliv\u2010 \nery. Using progressive disclosure techniques\u2014showing simplified results first, with \noptions to explore deeper\u2014helps to avoid overwhelming users while maintaining \ndepth. <br><br>**Pattern 11: Trustworthy Generation** **|** **187**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "077d3865-4b3a-40a1-96f3-f712713327fa": {"__data__": {"id_": "077d3865-4b3a-40a1-96f3-f712713327fa", "embedding": null, "metadata": {"bbox": [{"page": 205, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 330.93, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8685acb9-9433-4fac-b3e5-ad8edfaf158d", "node_type": "1", "metadata": {"bbox": [{"page": 204, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "1d8106ab53c5556fabefbf3270caf9a32e89e96bcbe617300e677663679bd1ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27459fde-b1b2-447d-b819-b2b23a9306c1", "node_type": "1", "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.84, "x1": 645.75, "y1": 601.79}]}, "hash": "877fcb717840a596348e0742ddd0d61fc25c9d57d20430d54452337779de3e75", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Feedback mechanisms\u2014such as thumbs-up/thumbs-down buttons, correction capa\u2010 \nbilities, and visible system improvements based on user input\u2014demonstrate that the \nsystem values accuracy and user experience (UX). When users see their feedback \nincorporated into improved responses, they develop confidence in the system\u2019s com\u2010 \nmitment to continuous improvement and reliability. <br><br>**Example**\nOf all the ideas discussed in the previous \u201cSolution\u201d section, the most effective one \nfor building users\u2019 trust in the answers your RAG system generates is adding citations \nto those answers. Here, we\u2019ll show you how to use a classifier to detect sections of \nthe response that need citation and then provide relevant citations. If you can\u2019t find \nany sources in your document store for a fact that needs a citation, you should alert \nthe user that the statement is untrustworthy, perhaps by highlighting the relevant \nsentence in red. <br><br>**Technique**<br><br>In this example, you\u2019re incorporating citations into a RAG response. As shown in \nFigure 4-13, you\u2019ll modify the response generation step of a RAG. Once the initial \nresponse is generated, you\u2019ll check the response for citation-worthy claims. Then, \nfor each chunk that needs a citation, you\u2019ll cross-reference the document store for \nreferences. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27459fde-b1b2-447d-b819-b2b23a9306c1": {"__data__": {"id_": "27459fde-b1b2-447d-b819-b2b23a9306c1", "embedding": null, "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.84, "x1": 645.75, "y1": 601.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "077d3865-4b3a-40a1-96f3-f712713327fa", "node_type": "1", "metadata": {"bbox": [{"page": 205, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 330.93, "x1": 432.01, "y1": 605.54}]}, "hash": "008c9d2b577136fef9713d7c4670a8b71fd1facc9476610b0e68687d2043bce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d857a3b-1a70-4694-8d3d-6ad70d63caf3", "node_type": "1", "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.22, "x1": 565.0, "y1": 319.28}]}, "hash": "e05dafcf5caee4c4718e9d27b73db87912613ac660ae4cb8691c0ff2088d2423", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 2: Chunk the response into smaller sections.**This example chunks the text down to \nthe sentence level. You can implement different strategies, depending on how fine- \ngrained you want the citations to be. <br><br>**Step 3: Check whether a chunk needs a citation.**Once a classifier marks a chunk as need\u2010 \ning a citation, cross-reference that chunk against the document store. <br><br>sentences = raw_response.split(\".\") \n... \n**for**sentence**in**sentences: \n review = needs_citation(sentence) \n chunks = check_sources(sentence)**if**review**else**[] <br><br>You can check whether a chunk needs a citation with a small language model, such as \na fine-tuned BERT, or with a zero-shot call to an LLM. Here, we use an LLM: <br><br>**def**needs_citation(content: str) -> bool: \n*\"\"\"Check if the content requires citations using OpenAI.\"\"\"*\n llm = ChatOpenAI(model_name=LLM_MODEL) \n prompt = PromptTemplate.from_template(\"\"\" \n Check if the content requires citations. The return should be true or false in this JSON format: {{\"requires_citations\": true}} <br><br> Content: {content} \n \"\"\") \n response = llm.invoke(prompt.format(content=content)) \n**return**json.loads(response.content)[\"requires_citations\"] ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d857a3b-1a70-4694-8d3d-6ad70d63caf3": {"__data__": {"id_": "0d857a3b-1a70-4694-8d3d-6ad70d63caf3", "embedding": null, "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.22, "x1": 565.0, "y1": 319.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "27459fde-b1b2-447d-b819-b2b23a9306c1", "node_type": "1", "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 331.84, "x1": 645.75, "y1": 601.79}]}, "hash": "877fcb717840a596348e0742ddd0d61fc25c9d57d20430d54452337779de3e75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e8f9156-ae2e-41d0-9480-fdb02230c6f9", "node_type": "1", "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 150.61, "x1": 414.0, "y1": 368.89}]}, "hash": "e9b0cffc52f4e7c7a381943beec74c0ecad5720efd7f24937e434185e4ea26db", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 4: Look up the sources.**Once you have identified the response chunks that need a \ncitation, you can look up the citation sources in your document store. <br><br>**def**check_sources(sentence: str) -> List[Any]: \n*\"\"\"Find similar documents in the document store\"\"\"*\n vectorstore = load_vector_store() <br><br> similar_chunks = vectorstore.similarity_search(sentence, k=5) \n**return**similar_chunks <br><br>This produces the following: <br><br>Chunk 1: \nContent: The Brandenburg Concertos (BWV 1046\u20131051) by Johann Sebastian Bach are a collection of six instrumen... \nMetadata: {'source': 'raw_texts/bach_brandenburg-concertos.txt'} \n\u2026 \nChunk 5: \nContent: Even more exceptional, for a Lutheran composer such as Bach, is that the composition is a Missa tota... \nMetadata: {'source': 'raw_texts/bach_mass-in-b-minor-bach.txt'} <br><br>**Step 5: Incorporate the citations into the response.**Here, we loop over and concatenate \nthe response chunks, and we also add the citations to the response. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e8f9156-ae2e-41d0-9480-fdb02230c6f9": {"__data__": {"id_": "5e8f9156-ae2e-41d0-9480-fdb02230c6f9", "embedding": null, "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 150.61, "x1": 414.0, "y1": 368.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0d857a3b-1a70-4694-8d3d-6ad70d63caf3", "node_type": "1", "metadata": {"bbox": [{"page": 207, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.22, "x1": 565.0, "y1": 319.28}]}, "hash": "e05dafcf5caee4c4718e9d27b73db87912613ac660ae4cb8691c0ff2088d2423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86a1ef50-bbb2-4176-874f-1b929cca61e3", "node_type": "1", "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.09, "x1": 432.0, "y1": 141.23}]}, "hash": "1fb89b4b7512a7a7996cb6720e4cc419aa4b38b281eb772c5594a7da4c5020e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here is an example response generated for the following original question: <br><br>What are the Brandenburg Concertos? <br><br>Various claims refer back to the document store: <br><br>The Brandenburg Concertos (BWV 1046\u20131051) are a collection of six instrumen\u2010 \ntal works by Johann Sebastian Bach, presented to Christian Ludwig, Margrave of \nBrandenburg-Schwedt, in 1721. [1] These concertos are highly regarded as some of \nthe greatest orchestral compositions of the Baroque era. [1] The concertos feature \nvarious combinations of solo instruments and are written for different instrumental \nensembles. Recent research has indicated that some of the material for the concertos \nmay have been based on earlier music composed by Bach for other purposes. [1, 2] The \ninstrumentation for the concertos includes a variety of instruments, such as violins, \nviolas, cellos, oboes, horns, bassoon, harpsichord, flute, and trumpet. <br><br>References: <br><br>[1] raw_texts/bach_brandenburg-concertos.txt <br><br>[2] raw_texts/bach_mass-in-b-minor-bach.txt <br><br>The example reference [1] contains this sentence: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86a1ef50-bbb2-4176-874f-1b929cca61e3": {"__data__": {"id_": "86a1ef50-bbb2-4176-874f-1b929cca61e3", "embedding": null, "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.09, "x1": 432.0, "y1": 141.23}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5e8f9156-ae2e-41d0-9480-fdb02230c6f9", "node_type": "1", "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 150.61, "x1": 414.0, "y1": 368.89}]}, "hash": "e9b0cffc52f4e7c7a381943beec74c0ecad5720efd7f24937e434185e4ea26db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04d95066-0641-4d32-b0a5-8feed4471b2d", "node_type": "1", "metadata": {"bbox": [{"page": 209, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 87.47, "x1": 432.01, "y1": 297.46}]}, "hash": "16e49eee10fb26a8201551d01a02cbc1d1c1ede7a430e5e8e95028a8cc8b576c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They are widely regarded as some of the greatest orchestral compositions of the \nBaroque era. <br><br>This could be seen as a good reference. In a production implementation, the response \nshould refer to proper sources (from the metadata of the chunks), not to the example ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04d95066-0641-4d32-b0a5-8feed4471b2d": {"__data__": {"id_": "04d95066-0641-4d32-b0a5-8feed4471b2d", "embedding": null, "metadata": {"bbox": [{"page": 209, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 87.47, "x1": 432.01, "y1": 297.46}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "86a1ef50-bbb2-4176-874f-1b929cca61e3", "node_type": "1", "metadata": {"bbox": [{"page": 208, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.09, "x1": 432.0, "y1": 141.23}]}, "hash": "1fb89b4b7512a7a7996cb6720e4cc419aa4b38b281eb772c5594a7da4c5020e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "355e8c96-8f05-4083-b62c-2f5dc03418c1", "node_type": "1", "metadata": {"bbox": [{"page": 210, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.63, "x1": 432.01, "y1": 308.73}]}, "hash": "bcb21b0d3e38767605b9dfbef22f9594b0385c19a594dfc1bdcfb2e1b2d3463e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\u2019s important to balance safeguards with information loss, too. Imagine a very strict \nRAG that filters most of the document chunks it retrieves. Such strict guardrails \ncan remove valuable information, make the LLM generate incomplete answers, and \nnegatively affect the UX. <br><br>Finally, verification steps, much like a human in the loop, only scale so far. At some \npoint, even human verification isn\u2019t possible anymore. Growing knowledge bases can \nalso produce more false positives over time as conflicting information is added to \nthem. Verification requires constantly monitoring the RAG system and adjusting its \nthresholds. <br><br>**Alternatives**<br><br>One alternative to self-RAG is to integrate multiple knowledge sources. RAG pipe\u2010 \nlines that retrieve information via multiple sources other than a single one-shot \napproach tend to return better overall responses. This can mean combining para\u2010 \nmetric information from the LLM and nonparametric information via the retrieval \nprocess, or it can mean using knowledge graphs to further improve response quality. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "355e8c96-8f05-4083-b62c-2f5dc03418c1": {"__data__": {"id_": "355e8c96-8f05-4083-b62c-2f5dc03418c1", "embedding": null, "metadata": {"bbox": [{"page": 210, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.63, "x1": 432.01, "y1": 308.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "04d95066-0641-4d32-b0a5-8feed4471b2d", "node_type": "1", "metadata": {"bbox": [{"page": 209, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 87.47, "x1": 432.01, "y1": 297.46}]}, "hash": "16e49eee10fb26a8201551d01a02cbc1d1c1ede7a430e5e8e95028a8cc8b576c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cf9ddfc-db42-4711-be78-08d746d8cab9", "node_type": "1", "metadata": {"bbox": [{"page": 211, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.3, "x1": 432.0, "y1": 274.25}]}, "hash": "364edd6b54b28388953389012abf97700ffc29bb18fc025c28c8239ce5fcf284", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The foundational work on CRAG is detailed in \u201cCorrective Retrieval Augmented \nGeneration\u201d (Yan et al., 2024). For practical implementation, we recommend the \nLangGraph tutorial on corrective RAG and the DataCamp implementation guide for \nCRAG with LangGraph. <br><br>The core research for Self-RAG was presented in \u201cSelf-RAG: Learning to Retrieve, \nGenerate, and Critique through Self-Reflection\u201d (Asai et al., 2023). The official Self- \nRAG GitHub repository provides complete implementation code, while LangChain\u2019s \nSelf-Reflective RAG with LangGraph page demonstrates practical deployment strate\u2010 \ngies. <br><br>Google NotebookLM provides in-line citations to source material that\u2019s used to gen\u2010 \nerate answers. <br><br>**Pattern 12: Deep Search**\nThe Deep Search pattern employs an iterative process of searching, reading, and \nreasoning to provide comprehensive answers to complex queries. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cf9ddfc-db42-4711-be78-08d746d8cab9": {"__data__": {"id_": "4cf9ddfc-db42-4711-be78-08d746d8cab9", "embedding": null, "metadata": {"bbox": [{"page": 211, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.3, "x1": 432.0, "y1": 274.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "355e8c96-8f05-4083-b62c-2f5dc03418c1", "node_type": "1", "metadata": {"bbox": [{"page": 210, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.63, "x1": 432.01, "y1": 308.73}]}, "hash": "bcb21b0d3e38767605b9dfbef22f9594b0385c19a594dfc1bdcfb2e1b2d3463e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9933f1ce-e945-4647-87f9-ab8e290a0a6c", "node_type": "1", "metadata": {"bbox": [{"page": 212, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 145.36, "x1": 432.01, "y1": 305.2}]}, "hash": "82112e48f558e38635743ff7b1ab528dc4e59065e0515ca44804b8b125f01e60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, take a look at this query: <br><br>Name the characteristics of coal-bearing strata in Newcastle. <br><br>When answering an ambiguous query like this, a simple RAG system might note that \nthe nodes being retrieved are about two different Newcastles. On the other hand, a \nRAG system with better reasoning capability would be able to infer that a query about \ncoal and Newcastle is more likely to be about the English city. <br><br>**Information staleness and verification**<br><br>Simple RAG systems rely on preindexed information that may become outdated. \nThey lack mechanisms to verify the currency or accuracy of retrieved information. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9933f1ce-e945-4647-87f9-ab8e290a0a6c": {"__data__": {"id_": "9933f1ce-e945-4647-87f9-ab8e290a0a6c", "embedding": null, "metadata": {"bbox": [{"page": 212, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 145.36, "x1": 432.01, "y1": 305.2}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4cf9ddfc-db42-4711-be78-08d746d8cab9", "node_type": "1", "metadata": {"bbox": [{"page": 211, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.3, "x1": 432.0, "y1": 274.25}]}, "hash": "364edd6b54b28388953389012abf97700ffc29bb18fc025c28c8239ce5fcf284", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4202199-f228-492a-b333-659bb23290cd", "node_type": "1", "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 284.24, "x1": 432.01, "y1": 605.54}]}, "hash": "9741465a51b4043861c1ffba3d71f73db1dbf8444b85c92bfecfeac9b1a9e055", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nThe Deep Search pattern involves an iterative loop that consists of retrieval and \nthinking until either a good enough answer is found or the time/cost budget is \nexhausted (see Figure 4-14). It goes beyond traditional RAG systems in three ways. \nFirst, it adds a thinking step between retrieval and generation to determine what \nadditional information, if any, needs to be gathered. Second, instead of a single \nretrieval step, Deep Search involves iterating through multiple stages of retrieval and \ngeneration. Finally, instead of just relying on adding knowledge from an enterprise \nknowledge base, Deep Search employs tools such as search engines and enterprise \nAPIs to gather the knowledge it needs proactively. Also note that you can implement \nthe steps in blue in Figure 4-14 by prompting foundational models, as we\u2019ll discuss \nlater in this section. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4202199-f228-492a-b333-659bb23290cd": {"__data__": {"id_": "b4202199-f228-492a-b333-659bb23290cd", "embedding": null, "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 284.24, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9933f1ce-e945-4647-87f9-ab8e290a0a6c", "node_type": "1", "metadata": {"bbox": [{"page": 212, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 145.36, "x1": 432.01, "y1": 305.2}]}, "hash": "82112e48f558e38635743ff7b1ab528dc4e59065e0515ca44804b8b125f01e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a2eab7b-dcc8-4192-9905-71a4fad24cc2", "node_type": "1", "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.24, "x1": 432.01, "y1": 275.14}]}, "hash": "84783c0756f4df04d28b363ddeaf2fcf2f6ba2b6208087e93ad78e943c447ee6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Context window constraints <br><br>The iterative approach allows Deep Search to process information across multi\u2010 \nple context windows. By maintaining state across iterations, Deep Search can \naccumulate knowledge progressively and thus effectively expand the functional \ncontext beyond the model\u2019s context window limit. The system can also retain key \ninsights from previous iterations while discarding less relevant information, thus \nenabling more comprehensive analysis of complex topics. <br><br>Query ambiguity <br><br>Deep Search can employ query decomposition and refinement mechanisms to \nidentify ambiguities in the original query and generate clarifying subqueries. \nThrough its reasoning component, it can interpret initial search results to iden\u2010 \ntify gaps in the retrieved results and determine what additional information is \nneeded. <br><br>Information staleness and verification <br><br>The architecture allows you to integrate fact-checking mechanisms and confi\u2010 \ndence scores for retrieved information into the evaluation and reflection steps \nthat are part of the thinking stage. You can cross-check information across multi\u2010 \nple sources, including real-time data sources. You can also redact unsupported \ninformation from the synthesized responses that are saved in the context. <br><br>Reasoning depth limitations <br><br>Iterating through retrieval and thinking means that you can perform multistep \ninference, connect information across different retrieval results, and generate \nintermediate conclusions that inform subsequent search operations. This enables \nDeep Search to go beyond the text that is present in any single document. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a2eab7b-dcc8-4192-9905-71a4fad24cc2": {"__data__": {"id_": "9a2eab7b-dcc8-4192-9905-71a4fad24cc2", "embedding": null, "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.24, "x1": 432.01, "y1": 275.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b4202199-f228-492a-b333-659bb23290cd", "node_type": "1", "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 284.24, "x1": 432.01, "y1": 605.54}]}, "hash": "9741465a51b4043861c1ffba3d71f73db1dbf8444b85c92bfecfeac9b1a9e055", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2b61d9b-68f3-44b3-a0ee-031dbc2de038", "node_type": "1", "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 315.33, "x1": 432.0, "y1": 607.79}]}, "hash": "78c5587013a065565989cec48bc550ae0434c593c48edf595e00ae174fd00126", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multihop query challenges <br><br>The iterative architecture naturally supports multihop information retrieval. You \ncan use information discovered in early iterations to inform and direct later \nsearches. Because you\u2019re carrying out the search across multiple knowledge \nsources, this also enables the synthesis step to connect the dots across multiple \nknowledge domains. <br><br>Instead of repeating the original query each time, most Deep Search implementations \nmaintain state across iterations. This allows for updated queries to query for addi\u2010 \ntional information; the responses are appended to the state. The entire context is used \nto synthesize a final response for evaluation. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2b61d9b-68f3-44b3-a0ee-031dbc2de038": {"__data__": {"id_": "e2b61d9b-68f3-44b3-a0ee-031dbc2de038", "embedding": null, "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 315.33, "x1": 432.0, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9a2eab7b-dcc8-4192-9905-71a4fad24cc2", "node_type": "1", "metadata": {"bbox": [{"page": 214, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 145.24, "x1": 432.01, "y1": 275.14}]}, "hash": "84783c0756f4df04d28b363ddeaf2fcf2f6ba2b6208087e93ad78e943c447ee6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e619d21e-904b-407b-96ec-c682035aa7bd", "node_type": "1", "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.93, "x1": 432.01, "y1": 307.23}]}, "hash": "3645cb0a52e80c8198c8a6fd71ed220b787417ec63f2c0a4285c4e1eea316b70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Iterative refinement**<br><br>Identifying gaps as shown previously gives you the ability to employ iterative refine\u2010 \nment throughout the execution flow. This is where you start very simply and then use \nthe answer to determine the next step in the process. For example, you could start out \nby decomposing a complex query such as this one: <br><br>Compare the economic impacts of climate change mitigation strategies across develop\u2010 \ning and developed nations, considering both short-term costs and long-term benefits. <br><br>But a better way would be to start out by answering a simpler question: <br><br>What is the economic impact of climate change mitigation strategies? <br><br>Then, in each iteration, you can progressively fill in the gaps in the answer to the \nsimpler question. <br><br>**Evaluation metrics**<br><br>The evaluation step and the reflection step (which is tied to the evaluation metric) \nare extremely important in building a good Deep Search system. The quality of the \nresponses you get is often directly tied to how well you can evaluate the iterations and \nprovide feedback to the model for the next iteration. <br><br>Instead of devising your own metric to evaluate content, we recommend that you \nemploy a widely used open source framework like Ragas. Such frameworks are well \ntested against the popular foundational models and a broad range of documents. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e619d21e-904b-407b-96ec-c682035aa7bd": {"__data__": {"id_": "e619d21e-904b-407b-96ec-c682035aa7bd", "embedding": null, "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.93, "x1": 432.01, "y1": 307.23}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e2b61d9b-68f3-44b3-a0ee-031dbc2de038", "node_type": "1", "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 315.33, "x1": 432.0, "y1": 607.79}]}, "hash": "78c5587013a065565989cec48bc550ae0434c593c48edf595e00ae174fd00126", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7475945-cf92-48fb-af99-b8869ad225d7", "node_type": "1", "metadata": {"bbox": [{"page": 217, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 281.14, "x1": 432.01, "y1": 340.17}]}, "hash": "a1fce6b1927c4f110faa21003b7d8974bbaaf59dd77548809b0c394bf2f111a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ragas provides a number of out-of-the-box metrics that you can use. Typically, you\u2019ll \ncreate a weighted average of the evaluation metrics and choose weights that reflect \nthe extent to which each metric matters for your problem. This allows you to have a \ncustomized evaluation score that is also robust. <br><br>At a minimum, make sure that your combined evaluation score includes these aspects \nof the report: <br><br>\u2022 \n\u2022 Relevance <br><br>\u2022 \n\u2022 Comprehensiveness <br><br>\u2022 \n\u2022 Accuracy and factual correctness <br><br>\u2022 \n\u2022 Coherence, logical flow, and organization <br><br>\u2022 \n\u2022 Citation quality <br><br>\u2022 \n\u2022 Efficiency <br><br>If you\u2019re using specific external tools (such as a SQL database), make sure to include \nmetrics that are customized for that purpose. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7475945-cf92-48fb-af99-b8869ad225d7": {"__data__": {"id_": "b7475945-cf92-48fb-af99-b8869ad225d7", "embedding": null, "metadata": {"bbox": [{"page": 217, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 281.14, "x1": 432.01, "y1": 340.17}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e619d21e-904b-407b-96ec-c682035aa7bd", "node_type": "1", "metadata": {"bbox": [{"page": 216, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.93, "x1": 432.01, "y1": 307.23}]}, "hash": "3645cb0a52e80c8198c8a6fd71ed220b787417ec63f2c0a4285c4e1eea316b70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f015568-5076-46d4-bf6c-dcdd4fe34083", "node_type": "1", "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 231.32, "x1": 432.0, "y1": 606.65}]}, "hash": "73153977c5c7cf9e69b3a8372ef307f36d4a65dfd553b07472137ad5536f64d7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Example**\nTo demonstrate Deep Search, we\u2019ll build a simplified version that carries out iterative \nretrieval, thinking, and generation based on Wikipedia articles (see Figure 4-15). The \nfull code is in the GitHub repository of this book. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f015568-5076-46d4-bf6c-dcdd4fe34083": {"__data__": {"id_": "9f015568-5076-46d4-bf6c-dcdd4fe34083", "embedding": null, "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 231.32, "x1": 432.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b7475945-cf92-48fb-af99-b8869ad225d7", "node_type": "1", "metadata": {"bbox": [{"page": 217, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 281.14, "x1": 432.01, "y1": 340.17}]}, "hash": "a1fce6b1927c4f110faa21003b7d8974bbaaf59dd77548809b0c394bf2f111a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0038430a-2909-42db-bdb2-91e57ae0a8e0", "node_type": "1", "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.6, "x1": 414.01, "y1": 221.94}]}, "hash": "cf2e13dabe015d226347464951dc273ea1d0032db0558aad27ff31d3ec3b0664", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " pages = [] \n**for**title**in**results: \n page = wikipedia.page(title) \n pages.append(WikipediaPage(title=page.title, \n url=page.url)) \n**return**pages <br><br>We did this for the following query: <br><br>What were the causes of the Liberian civil war? <br><br>We got Wikipedia pages with the following titles and a few others: <br><br>First Liberian Civil War <br><br>Liberians United for Reconciliation and Democracy <br><br>Americo-Liberian people <br><br>The next step is to rank the resulting pages on their relevance to the query. You can \ndo this by using an LLM: <br><br>**def**rank_pages(query: str, pages: list[WikipediaPage]) -> list[WikipediaPage]: \n agent = Agent(model=MODEL_ID, result_type=list[WikipediaPage]) \n prompt = f\"\"\"Rank these Wikipedia pages by relevance to the query: \"{query}\". \n Pages: {pages}\"\"\" \n response = agent.run_sync(prompt) \n**return**response.data <br><br>The result for the Liberia page was the following titles: <br><br>First Liberian Civil War <br><br>History of Liberia <br><br>Americo-Liberian people <br><br>The third step of retrieval is to extract relevant text. Again, this can be done with an \nLLM call (see the notebook in GitHub for actual code). The relevant text from the \nfirst page, for example, starts with this: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0038430a-2909-42db-bdb2-91e57ae0a8e0": {"__data__": {"id_": "0038430a-2909-42db-bdb2-91e57ae0a8e0", "embedding": null, "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.6, "x1": 414.01, "y1": 221.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9f015568-5076-46d4-bf6c-dcdd4fe34083", "node_type": "1", "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 231.32, "x1": 432.0, "y1": 606.65}]}, "hash": "73153977c5c7cf9e69b3a8372ef307f36d4a65dfd553b07472137ad5536f64d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14070fd8-f8b9-49ef-9123-3cdc3fe6f18a", "node_type": "1", "metadata": {"bbox": [{"page": 220, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 69.7, "x1": 432.0, "y1": 204.09}]}, "hash": "1de0c877df9216b586e7af9e75ac4152fef1eb8b4977e264b36995b59f9043f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The causes of the First Liberian Civil War are complex and multifaceted.\\n\\n**His\u2010 \ntorical Factors:**\\n\\n***Socio-economic disparities:**The Americo-Liberian elite, \ndescendants of freed American slaves, had historically dominated the country\u2019s polit\u2010 \nical and economic life, marginalizing the indigenous population. This created deep \nresentment and inequality.\\n***Political exclusion:**The True Whig Party held power \nfor over a century, effectively creating a one-party state that suppressed dissent and \nlimited political participation for non-Americo-Liberians.\\n*<br><br>This looks quite relevant indeed. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14070fd8-f8b9-49ef-9123-3cdc3fe6f18a": {"__data__": {"id_": "14070fd8-f8b9-49ef-9123-3cdc3fe6f18a", "embedding": null, "metadata": {"bbox": [{"page": 220, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 69.7, "x1": 432.0, "y1": 204.09}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0038430a-2909-42db-bdb2-91e57ae0a8e0", "node_type": "1", "metadata": {"bbox": [{"page": 219, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.6, "x1": 414.01, "y1": 221.94}]}, "hash": "cf2e13dabe015d226347464951dc273ea1d0032db0558aad27ff31d3ec3b0664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a63262c4-e2dc-4cd1-b54d-44786baaf38d", "node_type": "1", "metadata": {"bbox": [{"page": 221, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 514.0, "y1": 252.14}]}, "hash": "877fa451a0dc67be2f25c756e98a73be942336628349d143a2ded102b5395d2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each of these questions can be sent to the retrieval step and used to generate an \nanswer. <br><br>Note that this technique of identifying likely follow-ups is commonly used in chatbot \ndesign. In that process, the visual interface typically includes follow-up questions as \nchips, or clickable buttons, that the user can employ to drill more deeply into the \nanswer provided. <br><br>**Orchestration**<br><br>Putting it all together, the system uses each query and its answer to create a section of \nthe report: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a63262c4-e2dc-4cd1-b54d-44786baaf38d": {"__data__": {"id_": "a63262c4-e2dc-4cd1-b54d-44786baaf38d", "embedding": null, "metadata": {"bbox": [{"page": 221, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 514.0, "y1": 252.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "14070fd8-f8b9-49ef-9123-3cdc3fe6f18a", "node_type": "1", "metadata": {"bbox": [{"page": 220, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 69.7, "x1": 432.0, "y1": 204.09}]}, "hash": "1de0c877df9216b586e7af9e75ac4152fef1eb8b4977e264b36995b59f9043f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac0d2846-201f-4b2e-b97e-124493b5f2f4", "node_type": "1", "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 277.23, "x1": 427.16, "y1": 482.38}]}, "hash": "c936a7500f64a0506016c219208495256f9946dd82fa027a52659427de5b1c73", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can invoke this Deep Search implementation as follows: <br><br>report = deep_search(query=\"What were some of the famous victories of Napoleon Bonaparte?\", depth=1) <br><br>The resulting report reads in part as follows: <br><br><h1>What were some of the famous victories of Napoleon Bonaparte?<h1> <br><br>Based on the provided information, some of Napoleon Bonaparte\u2019s famous victories \ninclude: <br><br>*Siege of Toulon (1793) <br><br>*13 Vend\u00e9miaire (1795) <br><br>\u2026 <br><br>*Battle of Austerlitz (1805) <br><br>.. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac0d2846-201f-4b2e-b97e-124493b5f2f4": {"__data__": {"id_": "ac0d2846-201f-4b2e-b97e-124493b5f2f4", "embedding": null, "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 277.23, "x1": 427.16, "y1": 482.38}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a63262c4-e2dc-4cd1-b54d-44786baaf38d", "node_type": "1", "metadata": {"bbox": [{"page": 221, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.04, "x1": 514.0, "y1": 252.14}]}, "hash": "877fa451a0dc67be2f25c756e98a73be942336628349d143a2ded102b5395d2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7841c3f1-44b0-4c32-994f-d84c6b161c5d", "node_type": "1", "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 140.63, "x1": 426.07, "y1": 499.41}]}, "hash": "387defb5557b650e3cd7cf081eeb67a1a3f09c3d41628905aecf75a3d6f442a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Usage scenarios**\nYou can use RAG systems to do the following: <br><br>\u2022 Enhance customer service chatbots by providing \n\u2022 \naccurate, context-aware responses based on a \ncompany\u2019s knowledge base. \n\u2022 \n\u2022 Build an intelligent internal search engine to <br><br>synthesize answers to employee questions from \ninternal documents, databases, and function- \nspecific reports.  **Semantic Indexing**\n**(Pattern 7)**<br><br>\u2022 \n\u2022 Assist analysts in quickly accessing and <br><br>synthesizing information from large volumes of \nresearch reports and publicity materials. \n\u2022 \n\u2022 Efficiently retrieve relevant case law and <br><br>statutes to ensure that lawyers have up-to-date \nand accurate information for their cases. <br><br>\u2022 \n\u2022 Synthesize publicly available information about \ncompetitors\u2014such as news articles, financial \nreports, and product announcements\u2014into \ncompetitive intelligence reports. \n**Indexing at Scale**\n**(Pattern 8)**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7841c3f1-44b0-4c32-994f-d84c6b161c5d": {"__data__": {"id_": "7841c3f1-44b0-4c32-994f-d84c6b161c5d", "embedding": null, "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 140.63, "x1": 426.07, "y1": 499.41}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ac0d2846-201f-4b2e-b97e-124493b5f2f4", "node_type": "1", "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 277.23, "x1": 427.16, "y1": 482.38}]}, "hash": "c936a7500f64a0506016c219208495256f9946dd82fa027a52659427de5b1c73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14e6a16b-dd1d-45d1-8fda-e0e0c955cdad", "node_type": "1", "metadata": {"bbox": [{"page": 226, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.87, "x1": 432.01, "y1": 582.33}]}, "hash": "e19d0d0cd07b81f72f405ff80cfcdad46fdb8b91c39da8f90a1fee69844db60f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4-1. Patterns for adding knowledge <br><br>**Patterns**\n**Basic RAG (Pattern**\n**6)**<br><br>**Problems**\nKnowledge cutoff, \nconfidential data, and \nhallucinations pose \nproblems for zero- \nshot generation by \nLLMs. \nTraditional keyword \nindexing/lookup \napproaches fail when \ndocuments get more \ncomplex, contain \ndifferent media types \nlike images or tables, \nor bridge multiple \ndomains. \nYou must deal with \noutdated or \ncontradictory \ninformation in your \nknowledge base. \nComparing questions \nto chunks is \nproblematic because \na question itself will \nnot appear in the \nknowledge base, may \nuse synonyms or \njargon, or may \nrequire holistic \ninterpretation. <br><br>**Solutions**\nGround the response \ngenerated by the LLM \nby adding relevant \ninformation from a \nknowledge base into \nthe prompt context. \nUse embeddings to \ncapture the meaning \nof texts, images, and \nother media types. \nFind relevant chunks \nby comparing the \nembedding of the \nchunk to that of the \nquery. \nUse metadata, query \nfiltering, and result \nreranking. \n\u2022 \n\u2022 Speed up literature review and discovery <br><br>by retrieving relevant information based on \nresearch questions and generating comparisons \nof different findings. \n**Index-Aware**\n**Retrieval (Pattern 9)**<br><br>Use hypothetical \nanswers, query \nexpansion, hybrid \nsearch, and GraphRAG. \n\u2022 Create and update personalized summaries of \n\u2022 \ntopics and questions of interest from the latest \ninformation. <br><br>The applications of RAG are constantly \nexpanding as the technology evolves. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14e6a16b-dd1d-45d1-8fda-e0e0c955cdad": {"__data__": {"id_": "14e6a16b-dd1d-45d1-8fda-e0e0c955cdad", "embedding": null, "metadata": {"bbox": [{"page": 226, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.87, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7841c3f1-44b0-4c32-994f-d84c6b161c5d", "node_type": "1", "metadata": {"bbox": [{"page": 223, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 140.63, "x1": 426.07, "y1": 499.41}]}, "hash": "387defb5557b650e3cd7cf081eeb67a1a3f09c3d41628905aecf75a3d6f442a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce8f04ed-2802-455c-b20a-bba93a1a8ca1", "node_type": "1", "metadata": {"bbox": [{"page": 227, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.23, "x1": 432.01, "y1": 173.73}]}, "hash": "da76efade6e0c401c91d25ce553d3a7e10577c2a690006dbb705bc1b36804f07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 5**\n**Extending Model Capabilities**<br><br>LLMs excel at recognizing patterns in vast amounts of data. They can generate coher\u2010 \nent and contextually relevant responses by statistically predicting the next token. \nHowever, this doesn\u2019t necessarily mean they can genuinely understand the underlying \nconcepts. If you have a task that is not well covered by the training regimen of \nyour foundational model, then the model may not be able to perform that task. This \nchapter discusses four patterns you can use to teach foundational models tasks that \nthey were not trained to perform. <br><br>**The Limits of LLM Reasoning**\nFoundational models can manipulate symbols and words effectively, but this may be \nbecause they are generalizing from the manipulation they have encountered in the \ntraining data, not because they grasp semantic meanings and the logical relationships \nbetween them in the way humans do. While you can use foundational models to \nperform many tasks, that\u2019s because those tasks are similar to the tasks that the \nfoundational model was trained to do. <br><br>It\u2019s difficult to describe tasks that foundational models can\u2019t do well\u2014and that\u2019s \nbecause they\u2019ll be esoteric or industry-specific tasks that a more general audience, \nsuch as the readership of this book, will not understand. Tasks that aren\u2019t well \ncaptured by the training data of LLMs include writing a memo to the investment \ncommittee of a mutual fund or adjudicating an internal investigation because such \nmemos are internal records and such investigations are very company specific and \nnot public knowledge. By definition, any task that is described in widely available \nsources will be captured by LLMs. Given this, we will use math and reasoning \nproblems to support the idea that there are limits to LLMs\u2019 ability to reason. However, \nnote that the patterns in this chapter are not solutions to math or reasoning prob\u2010 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce8f04ed-2802-455c-b20a-bba93a1a8ca1": {"__data__": {"id_": "ce8f04ed-2802-455c-b20a-bba93a1a8ca1", "embedding": null, "metadata": {"bbox": [{"page": 227, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.23, "x1": 432.01, "y1": 173.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "14e6a16b-dd1d-45d1-8fda-e0e0c955cdad", "node_type": "1", "metadata": {"bbox": [{"page": 226, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.87, "x1": 432.01, "y1": 582.33}]}, "hash": "e19d0d0cd07b81f72f405ff80cfcdad46fdb8b91c39da8f90a1fee69844db60f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4c58635-b805-4951-b8e5-d0880c4765c1", "node_type": "1", "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 234.47, "x1": 432.01, "y1": 367.4}]}, "hash": "ed6f9bdab67490f9251e6637c150a70430c27da667159bd8e47d3f3cf2e41e9f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, while it\u2019s unlikely that ChatGPT encountered the specific word problem \n(about apartment size) or the number (84) in training, it can generalize from similar \nproblems that it has encountered to follow the correct approach. <br><br>The result of the multiplication is actually 904.1676, however, so the multiplication \nresult is hallucinated. To address this issue, you\u2019d need to provide the model access to \na calculator tool (see Pattern 21, Tool Calling, in Chapter 7). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4c58635-b805-4951-b8e5-d0880c4765c1": {"__data__": {"id_": "c4c58635-b805-4951-b8e5-d0880c4765c1", "embedding": null, "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 234.47, "x1": 432.01, "y1": 367.4}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ce8f04ed-2802-455c-b20a-bba93a1a8ca1", "node_type": "1", "metadata": {"bbox": [{"page": 227, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.23, "x1": 432.01, "y1": 173.73}]}, "hash": "da76efade6e0c401c91d25ce553d3a7e10577c2a690006dbb705bc1b36804f07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cff4e886-1745-4fbd-9709-a36f809f02bb", "node_type": "1", "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.29, "y1": 186.95}]}, "hash": "ad1ce5fe99e65f739e7befe1065c441c5d2c94d16c10aa760d0847a41506b2de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In bridge, holding AKJxx opposite four small, how should you play the suit for no \nlosers? <br><br>And it incorrectly told us that the best line was to finesse: <br><br> Best line of play (standard finesse and safety play): <br><br>Play a small card from the hand with the four small (dummy) toward the AKJxx. <br><br>If RHO plays low, insert the jack (J). <br><br>So, just because a model can reproduce a standard piece of advice, it does not mean \nthat the model can apply that advice. This is not limited to ChatGPT\u2014Gemini and \nAnthropic also knew the maxim but didn\u2019t apply it.1 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cff4e886-1745-4fbd-9709-a36f809f02bb": {"__data__": {"id_": "cff4e886-1745-4fbd-9709-a36f809f02bb", "embedding": null, "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.29, "y1": 186.95}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c4c58635-b805-4951-b8e5-d0880c4765c1", "node_type": "1", "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 234.47, "x1": 432.01, "y1": 367.4}]}, "hash": "ed6f9bdab67490f9251e6637c150a70430c27da667159bd8e47d3f3cf2e41e9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57eb5f31-cec4-41f3-80a8-c0dd2685e16f", "node_type": "1", "metadata": {"bbox": [{"page": 229, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.58, "x1": 432.01, "y1": 428.79}]}, "hash": "7e4b3e091d1829788ef61a2deef02f3caf18c4f1a5a26ff4e87f150bdd23c4b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 Model providers frequently fix errors like this as they are pointed out, so don\u2019t be surprised if you can\u2019t <br><br>reproduce this exact problem by the time you read this. The results of the API calls in April 2025 are in our \nGitHub repository. \nThere are two potentially correct answers. The expert line is to play the ace and then the king if the ten doesn\u2019t \nfall on the right. If the ten falls, then you should come to hand in another suit and take the finesse. This takes \ninto account the probability table of holdings involving five outstanding cards and a principle called restricted \nchoice. A reasonably correct answer, and one that many intermediate players would take, is to cash the ace \nand the king because the probability that the queen will fall is more than 50% and that of a finesse is 50%. \nIn April 2025, this particular error happened even in the more capable conversational interfaces with access \nto tools such as web search. Here are the links to our ChatGPT, Gemini, and Claude sessions. Note that we \ndid the queries in the opposite order because these tools maintain conversational state and we didn\u2019t want to \nprime the models with a hint about how to solve the problem. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57eb5f31-cec4-41f3-80a8-c0dd2685e16f": {"__data__": {"id_": "57eb5f31-cec4-41f3-80a8-c0dd2685e16f", "embedding": null, "metadata": {"bbox": [{"page": 229, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.58, "x1": 432.01, "y1": 428.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cff4e886-1745-4fbd-9709-a36f809f02bb", "node_type": "1", "metadata": {"bbox": [{"page": 228, "page_height": 661.5, "page_width": 504.0, "x0": 73.14, "y0": 68.95, "x1": 431.29, "y1": 186.95}]}, "hash": "ad1ce5fe99e65f739e7befe1065c441c5d2c94d16c10aa760d0847a41506b2de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dd5ec7d-a5fd-4dbd-8f00-b325316eab5c", "node_type": "1", "metadata": {"bbox": [{"page": 230, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.67, "x1": 432.01, "y1": 300.98}]}, "hash": "0f41376f9df184bb0efe067b1c2c97f55034e918c9e1b738ac242339f42f1fc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Doubtless, in your industry and business, you have to perform hard or obscure \nanalytical tasks (such as writing an investment committee memo, carrying out an \ninternal investigation, determining how to select a vendor, and so on) that are simi\u2010 \nlarly not well covered by the training datasets of foundational models. This chapter \ndiscusses four patterns that you can use to teach foundational models tasks that they \nwere not trained to perform. <br><br>**Pattern 13: Chain of Thought**\nThe Chain of Thought pattern uses chain-of-thought (CoT) reasoning, which is a \nprompting technique that enhances LLMs\u2019 problem-solving capabilities by encourag\u2010 \ning them to break down complex problems into intermediate reasoning steps before \narriving at a final answer. <br><br>**Problem**\nWhen it comes to problems that involve mathematical calculations, logical deduc\u2010 \ntions, or sequential reasoning, foundational models suffer from some critical limi\u2010 \ntations in their training data coverage. This is particularly obvious in problems \ninvolving multistep reasoning or when the model directly provides an answer for \nquestions that require some deliberation. <br><br>**Training data coverage**<br><br>As the trick-taking example in the introduction illustrates, foundational models can\u2019t \nalways generalize to tasks that are unknown or poorly covered by the training dataset. \nIt\u2019s likely that while grade school mathematics and physics problems were part of the \nmodels\u2019 training datasets, industry-specific calculations were not. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dd5ec7d-a5fd-4dbd-8f00-b325316eab5c": {"__data__": {"id_": "5dd5ec7d-a5fd-4dbd-8f00-b325316eab5c", "embedding": null, "metadata": {"bbox": [{"page": 230, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.67, "x1": 432.01, "y1": 300.98}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "57eb5f31-cec4-41f3-80a8-c0dd2685e16f", "node_type": "1", "metadata": {"bbox": [{"page": 229, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.58, "x1": 432.01, "y1": 428.79}]}, "hash": "7e4b3e091d1829788ef61a2deef02f3caf18c4f1a5a26ff4e87f150bdd23c4b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1641e7d7-feb3-4e9e-8bf5-0d2d544282a5", "node_type": "1", "metadata": {"bbox": [{"page": 232, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 607.89}]}, "hash": "0e243f638d71096f3a89d2f67fb3ccb2d15044eb9af57afe5d9219cedf8d73b7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model knows the equations in question, and its objections that the pipe could be \nbent or slanted are just filibustering. If you were to anthropomorphize the model, you \nmight claim that it\u2019s being lazy! <br><br>**Multistep reasoning**<br><br>A model\u2019s zero-shot capability often fails when problems require multistep reasoning, \nsince models tend to jump directly to answers without the intermediate steps of \nproper analysis. When they do, they often fabricate information or provide incorrect \nanswers. <br><br>For example, we asked Gemini this question: <br><br>You are allowed to check in 50 kg if your final destination is the United States or if \nyou have special needs. Otherwise, you are allowed only 40 kg. What is the carry-on \nallowance for the following itinerary: SIN-DFW-YYZ? <br><br>And we got an incorrect answer: <br><br>Since at least one of the legs involves the United States as the final destination, you are \nallowed to check in**50kg**. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1641e7d7-feb3-4e9e-8bf5-0d2d544282a5": {"__data__": {"id_": "1641e7d7-feb3-4e9e-8bf5-0d2d544282a5", "embedding": null, "metadata": {"bbox": [{"page": 232, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5dd5ec7d-a5fd-4dbd-8f00-b325316eab5c", "node_type": "1", "metadata": {"bbox": [{"page": 230, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.67, "x1": 432.01, "y1": 300.98}]}, "hash": "0f41376f9df184bb0efe067b1c2c97f55034e918c9e1b738ac242339f42f1fc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7db68fbf-92f1-469d-abd8-876090769e98", "node_type": "1", "metadata": {"bbox": [{"page": 233, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "c454aa9ee0d41bd241f08ad19e8f3219d0ddb287573f627888ffd1547d1b4053", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nCoT involves modifying the prompting strategy\u2014instead of asking for a direct \nanswer, the prompt explicitly requests or demonstrates a step-by-step reasoning \nprocess. There are three variants: Zero-shot CoT, Few-shot CoT, and Auto-CoT. Let\u2019s \nlook at them one by one. <br><br>**Variant 1: Zero-shot CoT**<br><br>In Zero-shot CoT, you request a step-by-step reasoning by simply adding the phrase \nthink step-by-step to the prompt (see Figure 5-2). For example, you might say this: <br><br>A 2 kg object is sliding down a frictionless inclined plane with an angle of 30\u00b0 to the \nhorizontal. What is the acceleration of the object? Think step-by-step. <br><br><br><br>Figure 5-2. Zero-shot CoT. <br><br>The response, from OpenAI\u2019s GPT 3.5 Turbo, includes step-by-step reasoning: <br><br>Step 1: Draw a free body diagram of the object. <br><br>The forces acting on the object are its weight (mg) acting straight downwards and the \nnormal force (N) acting perpendicular to the inclined plane. Since the inclined plane is \nfrictionless, there is no frictional force acting on the object. <br><br>Step 2: Resolve the weight force into components parallel and perpendicular to the \ninclined plane. <br><br>The weight force can be resolved into two components: one parallel to the inclined \nplane (mg*sin(30\u00b0)) and one perpendicular to the inclined plane (mg*cos(30\u00b0)). <br><br>These steps are drawn from the model\u2019s pretraining and limited to what it has \nencountered in its pretraining. Try it when the model is being \u201clazy,\u201d either by not \nusing the information it has or by directly jumping to an answer. <br><br>**Variant 2: Few-shot CoT**<br><br>In cases where the model\u2019s pretrained capabilities do not include the specific logic \nneeded, you can try Few-shot CoT, which involves providing the model with exam\u2010 <br><br>**Pattern 13: Chain of Thought** **|** **215**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7db68fbf-92f1-469d-abd8-876090769e98": {"__data__": {"id_": "7db68fbf-92f1-469d-abd8-876090769e98", "embedding": null, "metadata": {"bbox": [{"page": 233, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1641e7d7-feb3-4e9e-8bf5-0d2d544282a5", "node_type": "1", "metadata": {"bbox": [{"page": 232, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 607.89}]}, "hash": "0e243f638d71096f3a89d2f67fb3ccb2d15044eb9af57afe5d9219cedf8d73b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2f49a73-a41b-4cc0-94bc-43ca5e191c0e", "node_type": "1", "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 310.32, "x1": 414.0, "y1": 585.66}]}, "hash": "a824cf6e8a017284e08d861855cbff90b27e672b92755a30a561fbfffa20de3e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ples of step-by-step reasoning for similar problems before asking it to solve a new \nproblem. In Few-shot CoT, you demonstrate the desired reasoning pattern through \nexamples (see Figure 5-3). <br><br><br><br>Figure 5-3. Few-shot CoT involves providing examples of reasoning <br><br>**Few-shot CoT versus RAG**<br><br>Both Pattern 6, Basic RAG (from Chapter 3) and Few-shot CoT \ninvolve adding information to the context of the prompt. The \ndifference is that in RAG, you add knowledge (data), whereas in \nCoT, you demonstrate logic. In RAG, you want the answer to be \ngrounded precisely on the knowledge added, whereas in CoT, you \nwant the model to generalize from the examples you provide. <br><br>Think of it this way: RAG gives the model a (few) fish, while \nFew-shot CoT shows the model how to fish. <br><br>**216** **|** **Chapter 5: Extending Model Capabilities**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2f49a73-a41b-4cc0-94bc-43ca5e191c0e": {"__data__": {"id_": "e2f49a73-a41b-4cc0-94bc-43ca5e191c0e", "embedding": null, "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 310.32, "x1": 414.0, "y1": 585.66}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7db68fbf-92f1-469d-abd8-876090769e98", "node_type": "1", "metadata": {"bbox": [{"page": 233, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "c454aa9ee0d41bd241f08ad19e8f3219d0ddb287573f627888ffd1547d1b4053", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a85bb2a1-01d2-46f9-ad3c-dc30b602f1f0", "node_type": "1", "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.88, "x1": 432.0, "y1": 305.46}]}, "hash": "655b7edec90368bee0f861f4fd68ea31f59802d7ed78e6851b426d166f1310e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Solve the following physics problem using the following example as a template. <br><br>Example: <br><br>Q: A ball is dropped from a height of 200 m. How long will it take to reach the ground? <br><br>A: <br><br>Step 1: Identify equation <br><br>The equation that relates displacement, initial velocity, acceleration, and time is: \u0394y = \nv\u2080t + (1/2)at\u00b2 <br><br>Step 2: Identify known values <br><br>\u0394y = final height - initial height = 0 m - 200 m = -200 m <br><br>v\u2080 = initial velocity = 0 <br><br>a = Acceleration due to gravity (g) \u2248 9.8 m/s\u00b2 <br><br>Step 3: Substitute known values into the equation <br><br>-200 m = (0 m/s)*t + (1/2)*(-9.8 m/s\u00b2)*t\u00b2 <br><br>Step 4: Solve <br><br>-200 m = 0 + (-4.9 m/s\u00b2)*t\u00b2 <br><br>t\u00b2 = (-200 m) / (-4.9 m/s\u00b2) <br><br>t\u00b2 \u2248 40.816 s\u00b2 <br><br>t = \u221a(40.816 s\u00b2) <br><br>t \u2248 6.39 s <br><br>Step 5: Report answer ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a85bb2a1-01d2-46f9-ad3c-dc30b602f1f0": {"__data__": {"id_": "a85bb2a1-01d2-46f9-ad3c-dc30b602f1f0", "embedding": null, "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.88, "x1": 432.0, "y1": 305.46}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e2f49a73-a41b-4cc0-94bc-43ca5e191c0e", "node_type": "1", "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 310.32, "x1": 414.0, "y1": 585.66}]}, "hash": "a824cf6e8a017284e08d861855cbff90b27e672b92755a30a561fbfffa20de3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e7629e4-8afe-451a-89c7-393f3c2af179", "node_type": "1", "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 207.26, "x1": 432.0, "y1": 605.54}]}, "hash": "a13cfcbbccc64538fd0db65f577d41649231c9f146da5880649e03210ea48a05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A ball that is dropped from a height of 200 m will take 6.39 s to reach the ground. <br><br>Q: A 2 kg object is sliding down a frictionless inclined plane with an angle of 30\u00b0 to the \nhorizontal. What is the acceleration of the object? <br><br>The response now follows the templated example in having exactly five steps, as \ndemonstrated here: <br><br>A: <br><br>Step 1: Identify equation <br><br>The equation that relates acceleration, gravity, and the angle of the inclined plane is: a \n= g*sin(\u03b8) <br><br>Step 2: Identify known values <br><br>m = mass = 2 kg <br><br>\u03b8 = angle of the inclined plane = 30\u00b0 <br><br>g = Acceleration due to gravity = 9.8 m/s\u00b2 <br><br>Step 3: Substitute known values into the equation <br><br>a = (9.8 m/s\u00b2)*sin(30\u00b0) <br><br>Step 4: Solve ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e7629e4-8afe-451a-89c7-393f3c2af179": {"__data__": {"id_": "7e7629e4-8afe-451a-89c7-393f3c2af179", "embedding": null, "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 207.26, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a85bb2a1-01d2-46f9-ad3c-dc30b602f1f0", "node_type": "1", "metadata": {"bbox": [{"page": 234, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.88, "x1": 432.0, "y1": 305.46}]}, "hash": "655b7edec90368bee0f861f4fd68ea31f59802d7ed78e6851b426d166f1310e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02992b45-84ef-4cfc-a7b4-d2a8a6a939ce", "node_type": "1", "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.88, "x1": 432.01, "y1": 197.68}]}, "hash": "aa2eccf4f1974095a3e647665f5ee5087131a55f56720fa6fc057192cba516c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "trawling, bait casting, and so on\u2014and you choose the appropriate example for your \nscenario. <br><br>**Examples**\nLet\u2019s look at a couple of examples of CoT in action. <br><br>**Zero-shot CoT**<br><br>By instructing the model to \u201cthink step-by-step\u201d before providing an answer, Zero- \nshot CoT often unlocks its pretrained capabilities. Recall that the model was \u201clazy\u201d \nwhen we asked it the oil-and-gas question. What happens if you add the Zero-shot \nCoT catchphrase? <br><br>If I have a pipe of diameter 25 cm and a hose that is 100 m long, <br><br>what\u2019s the flow-rate of Texas Sweet with a 7-bar pressure differential? <br><br>Think about it step-by-step. <br><br>The abridged answer from Claude is this. (The full answer is on GitHub.): <br><br>... <br><br>## Step 1: Determine the properties of Texas Sweet crude oil <br><br>Texas Sweet crude oil typically has these properties: <br><br>- Density (\u03c1): approximately 850 kg/m\u00b3 <br><br>- Dynamic viscosity (\u03bc): approximately 0.005-0.010 Pa\u00b7s (I\u2019ll use 0.008 Pa\u00b7s) <br><br>... <br><br>## Step 3: Substitute values and calculate <br><br>Q = (\u03c0 \u00d7 (0.25 m)\u2074 \u00d7 700,000 Pa) / (128 \u00d7 0.008 Pa\u00b7s \u00d7 100 m) <br><br>... <br><br>The calculated flow rate of approximately 84 m\u00b3/s (or 5,040 <br><br>m\u00b3/min) for Texas Sweet crude oil with the given parameters seems very high, indicat\u2010 \ning the flow would be turbulent in practice and might be limited by other factors in a \nreal pipeline system. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02992b45-84ef-4cfc-a7b4-d2a8a6a939ce": {"__data__": {"id_": "02992b45-84ef-4cfc-a7b4-d2a8a6a939ce", "embedding": null, "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.88, "x1": 432.01, "y1": 197.68}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7e7629e4-8afe-451a-89c7-393f3c2af179", "node_type": "1", "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 207.26, "x1": 432.0, "y1": 605.54}]}, "hash": "a13cfcbbccc64538fd0db65f577d41649231c9f146da5880649e03210ea48a05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60786c79-d6bb-482e-8805-5d3a2b2ce9b3", "node_type": "1", "metadata": {"bbox": [{"page": 238, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 116.28, "x1": 432.0, "y1": 347.18}]}, "hash": "977a76995a4990b6aff692a2c6310d5848d4e2f153436de1acaf02c780dfa0e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We didn\u2019t have to specify the exact steps\u2014the model knew that it needed to look \nup the viscosity of the type of crude oil and plug the appropriate numbers into the \nequation to calculate flow. Just asking the model to think step-by-step was enough to \ntrigger the right behavior. <br><br>**Few-shot CoT**<br><br>Recall that the model gave erroneous results when we asked it the question about \nbaggage. This was because it had mistakenly identified a layover as a \u201cfinal destina\u2010 \ntion.\u201d However, you can address mistaken or hallucinated logic by using Few-shot ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60786c79-d6bb-482e-8805-5d3a2b2ce9b3": {"__data__": {"id_": "60786c79-d6bb-482e-8805-5d3a2b2ce9b3", "embedding": null, "metadata": {"bbox": [{"page": 238, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 116.28, "x1": 432.0, "y1": 347.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "02992b45-84ef-4cfc-a7b4-d2a8a6a939ce", "node_type": "1", "metadata": {"bbox": [{"page": 237, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.88, "x1": 432.01, "y1": 197.68}]}, "hash": "aa2eccf4f1974095a3e647665f5ee5087131a55f56720fa6fc057192cba516c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "065e47e4-d960-4c68-8efa-5d5451f0f813", "node_type": "1", "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 209.77, "x1": 414.01, "y1": 414.69}]}, "hash": "c8f629b014da78aadd378e4c81db5e3c0250ea3164d21926b96b569543f99527", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Few-shot CoT is also effective at solving the trick-taking problem we described in \nthe introductory section of this chapter. Providing a couple of examples of the most \nlikely split of the opponent\u2019s cards is enough to trigger an acceptable response from \nthe model: <br><br>You are a bridge teacher. Guide a student on how to play the given suit combination. <br><br>Example 1: <br><br>Q: Holding AKJx opposite xxxx. <br><br>A: You have 8 cards between the two hands, and the opponents have 5 cards, including \nthe queen. <br><br>The most likely split is 3-2. You can pick up the suit only if the queen is onside. <br><br>Finesse the jack. <br><br>Example 2: <br><br>Q: Holding AKJx opposite 5 small. <br><br>A: You have 9 cards between the two hands, and the opponents have 4 cards, including \nthe queen. <br><br>The most likely split is 2-2. Therefore, play for the queen to drop. <br><br>Q: Holding AKJxx opposite four small. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "065e47e4-d960-4c68-8efa-5d5451f0f813": {"__data__": {"id_": "065e47e4-d960-4c68-8efa-5d5451f0f813", "embedding": null, "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 209.77, "x1": 414.01, "y1": 414.69}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "60786c79-d6bb-482e-8805-5d3a2b2ce9b3", "node_type": "1", "metadata": {"bbox": [{"page": 238, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 116.28, "x1": 432.0, "y1": 347.18}]}, "hash": "977a76995a4990b6aff692a2c6310d5848d4e2f153436de1acaf02c780dfa0e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70c661ea-4616-46df-97fe-87fc1dca73a3", "node_type": "1", "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.52, "x1": 432.0, "y1": 204.91}]}, "hash": "22d22f61b09e292157583de6042fc38f2c1e3a986f3e360df8a8fc3fc3c9d7de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The response indicates that the model is following the correct logic: <br><br>If you drive 300 km west of Hyderabad, you can expect to end up in or near a few \nnotable areas in the state of Maharashtra. Here\u2019s a step-by-step breakdown: <br><br>1.**Starting Point**: You begin in Hyderabad, which is located in the southern part of \nIndia, in the state of Telangana. <br><br>2.**Direction**: Driving west means you\u2019ll be heading toward Maharashtra, which is \ndirectly west of Telangana. <br><br>3.**Estimated Distance**: A distance of 300 km takes you through various terrain and \ncould put you near several towns or cities in Maharashtra. <br><br>4.**Potential Destinations**: <br><br>-**Aurangabad**: A significant city known for its historical sites, including the Ajanta \nand Ellora Caves. It is about 300 km from Hyderabad. <br><br>-**Nanded**: Another city that is approximately within this distance and is known for \nits Gurudwara and historical significance in Sikhism. <br><br>5.**Route Options**: Depending on the specific route taken (e.g., via NH65), you \nmight pass through smaller towns as well, like Zaheerabad or Bidar. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70c661ea-4616-46df-97fe-87fc1dca73a3": {"__data__": {"id_": "70c661ea-4616-46df-97fe-87fc1dca73a3", "embedding": null, "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.52, "x1": 432.0, "y1": 204.91}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "065e47e4-d960-4c68-8efa-5d5451f0f813", "node_type": "1", "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 209.77, "x1": 414.01, "y1": 414.69}]}, "hash": "c8f629b014da78aadd378e4c81db5e3c0250ea3164d21926b96b569543f99527", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57406f0a-2af1-4f36-b03a-4c25e5f7435b", "node_type": "1", "metadata": {"bbox": [{"page": 240, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.87, "x1": 432.0, "y1": 247.07}]}, "hash": "4f83f7f1e6cc0b87086915a17368c0c78055fa0029c438c4eb705fac4df9919d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In conclusion, after traveling west for 300 km from Hyderabad, you would likely \nend up in Aurangabad or Nanded, among other possible smaller towns or cities in \nMaharashtra. <br><br>The logic is correct, but the destination is hallucinated. Nanded is northwest of \nHyderabad, not west. <br><br>The way to fix a data gap is to add knowledge. In this case, if the model is multimo\u2010 \ndal, adding a map will do nicely: <br><br>**def**answer_based_on_map(model_id, prompt: str) -> str: \n**from pydantic_ai import**Agent, ImageUrl \n agent = Agent(model_id) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57406f0a-2af1-4f36-b03a-4c25e5f7435b": {"__data__": {"id_": "57406f0a-2af1-4f36-b03a-4c25e5f7435b", "embedding": null, "metadata": {"bbox": [{"page": 240, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.87, "x1": 432.0, "y1": 247.07}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "70c661ea-4616-46df-97fe-87fc1dca73a3", "node_type": "1", "metadata": {"bbox": [{"page": 239, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.52, "x1": 432.0, "y1": 204.91}]}, "hash": "22d22f61b09e292157583de6042fc38f2c1e3a986f3e360df8a8fc3fc3c9d7de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2406dbb1-a8f1-4980-84a2-d94c24978c60", "node_type": "1", "metadata": {"bbox": [{"page": 241, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.03, "x1": 432.01, "y1": 343.56}]}, "hash": "6353e16ca364f1e19958604b9c208195c922a12f8ff010c74990952120ef134a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Nonsequential logic.**If the logic involves cyclical loops, CoT tends to not work. In \nthe bridge problem, making decisions based purely on the most likely split of the \nopponents\u2019 cards is a shortcut. An expert would update the relevant likelihoods based \non what the opponents play to the first few tricks, such as if they show out of a suit. \nAn expert would also take an unlikely line if that were the only possibility to take the \ndesired number of tricks. <br><br>Demonstrating expert logic in the CoT doesn\u2019t work because the expert lines of play \ninvolve optimizing for multiple scenarios and are therefore hard for the model to \nsimply mimic: <br><br>You are a bridge expert. Guide me on how to play the given suit combination. <br><br>Example 1: <br><br>Q: Holding AKJx opposite xxx for no losers. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2406dbb1-a8f1-4980-84a2-d94c24978c60": {"__data__": {"id_": "2406dbb1-a8f1-4980-84a2-d94c24978c60", "embedding": null, "metadata": {"bbox": [{"page": 241, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.03, "x1": 432.01, "y1": 343.56}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "57406f0a-2af1-4f36-b03a-4c25e5f7435b", "node_type": "1", "metadata": {"bbox": [{"page": 240, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.87, "x1": 432.0, "y1": 247.07}]}, "hash": "4f83f7f1e6cc0b87086915a17368c0c78055fa0029c438c4eb705fac4df9919d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d1a7a89-887a-4225-9b8c-88871ca54ea0", "node_type": "1", "metadata": {"bbox": [{"page": 243, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "585e1e7bb32420850e8bd623f5e9f2457e70021c1e496b565c0bc3ae3a04b480", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Q: Holding AKJxx opposite four small, for no losers. <br><br>The resulting response (see the GitHub repository) is incorrect. The model can\u2019t pick \nup the desired behavior when the demonstration examples contain nonsequential \nlogic. <br><br>**Alternatives**<br><br>If you are considering CoT, consider whether you can use a more capable model or \nan agentic approach instead. <br><br>**Test-time compute in models.**If doing CoT is as simple as adding \u201cthink step-by-step\u201d \nto the prompt, why don\u2019t model providers add the phrase automatically to user \nprompts? In effect, they do: modern foundational model APIs classify questions and \ndetermine whether to use Zero-shot CoT. Generated demonstrations, as in Auto-CoT, \nare part of the pretraining regimen of these models\u2014and in some situations, they \nmay also use an example store of demonstrations in Few-shot CoT. <br><br>For this reason, you\u2019ll find Zero-shot CoT more helpful for small, local models than \nfor frontier models. You will find Few-shot CoT helpful only in domains that are \noutside the pretraining of the model you are using. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d1a7a89-887a-4225-9b8c-88871ca54ea0": {"__data__": {"id_": "1d1a7a89-887a-4225-9b8c-88871ca54ea0", "embedding": null, "metadata": {"bbox": [{"page": 243, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2406dbb1-a8f1-4980-84a2-d94c24978c60", "node_type": "1", "metadata": {"bbox": [{"page": 241, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.03, "x1": 432.01, "y1": 343.56}]}, "hash": "6353e16ca364f1e19958604b9c208195c922a12f8ff010c74990952120ef134a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75ec9b66-e6b8-417e-9d18-f40724f79acc", "node_type": "1", "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.26, "x1": 432.01, "y1": 607.89}]}, "hash": "fc5bed0d08de69c7ccb1d4acd9cb25847fa5f45480d14fd38e7fa1452d54dd0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "showing that these reasoning steps could sometimes be elicited by simply asking. \nThen, a few months after that (and still in 2022!), Zhang et al. introduced Auto-CoT \nwith the two key ideas that allow CoT to be done without manual writing of answers: \nsampling questions with diversity and generating reasoning chains using Zero-shot \nCoT to populate the example store. <br><br>Then, in 2025, Invisible taught CoT to LLMs in a project for Allen & Overy (A&O), \na legal application. K2view implemented CoT reasoning in its GenAI Data Fusion \nplatform, breaking down complex customer service queries into manageable steps for \nenterprise chatbots with real-time data integration. <br><br>**Pattern 14: Tree of Thoughts (ToT)**\nThe ToT pattern approaches complex tasks by generating multiple reasoning paths, \nranking them, exploring the most promising ones, and backtracking and trying new \nand different paths if necessary (see Figure 5-5). Recall that CoT can handle only \ntasks that can be broken down into a set of linear steps that can be processed sequen\u2010 \ntially. In contrast, ToT requires multiple LLM calls to evaluate multiple solution \npaths in a nonlinear way\u2014for example, it can discard pursued paths and \u201crestart\u201d its \nreasoning process. ToT can be employed on a broader spectrum of tasks than CoT \ncan, especially ones that require reasoning and planning. <br><br><br><br>Figure 5-5. Tree of Thoughts showing an LLM searching multiple paths and backtracking \nif a path turns out to be unsuccessful <br><br>**226** **|** **Chapter 5: Extending Model Capabilities**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75ec9b66-e6b8-417e-9d18-f40724f79acc": {"__data__": {"id_": "75ec9b66-e6b8-417e-9d18-f40724f79acc", "embedding": null, "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.26, "x1": 432.01, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1d1a7a89-887a-4225-9b8c-88871ca54ea0", "node_type": "1", "metadata": {"bbox": [{"page": 243, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "585e1e7bb32420850e8bd623f5e9f2457e70021c1e496b565c0bc3ae3a04b480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df7b52d8-25a3-4f8f-9fdf-37a75a844c26", "node_type": "1", "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 122.57, "x1": 432.0, "y1": 311.67}]}, "hash": "57082d06d204bae652fb1a6aa63b23df6a4d5f13355eddba113b9416dc21b040", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Problem**\nMany tasks that demand strategic thinking or logical reasoning can\u2019t be solved by \npursuing a single multistep reasoning path. These problems require exploring multi\u2010 \nple reasoning directions, backtracking on solutions that haven\u2019t proven successful, \nand continuous self-evaluation of the options. <br><br>Suppose that you are given four random sentences and told to construct a coherent \npassage with four paragraphs, each of which ends with one of the four sentences, \nrespectively. The Princeton and Google DeepMind researchers who wrote the ToT \npaper describe this as a problem that requires both creative thinking and high-level \nplanning. <br><br>It\u2019s difficult to construct a CoT step-by-step set of instructions for this task, and it\u2019s \nunclear that Few-shot CoT would help. So, we are left with Zero-shot CoT: <br><br>Write a 4 paragraph essay that ends with the following 4 sentences respectively. <br><br>1. To be or not to be, that is the question. <br><br>2. Take me to your leader. <br><br>3. It is a truth universally acknowledged, that a single man in possession of a good \nfortune, must be in want of a wife. <br><br>4. The only thing we have to fear is fear itself. <br><br>Each paragraph should be 3-4 sentences. <br><br>Think step-by-step. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df7b52d8-25a3-4f8f-9fdf-37a75a844c26": {"__data__": {"id_": "df7b52d8-25a3-4f8f-9fdf-37a75a844c26", "embedding": null, "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 122.57, "x1": 432.0, "y1": 311.67}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "75ec9b66-e6b8-417e-9d18-f40724f79acc", "node_type": "1", "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.26, "x1": 432.01, "y1": 607.89}]}, "hash": "fc5bed0d08de69c7ccb1d4acd9cb25847fa5f45480d14fd38e7fa1452d54dd0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67bc9c67-3466-4f5e-afb8-b7157eea5813", "node_type": "1", "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 363.51, "x1": 1559.5, "y1": 607.79}]}, "hash": "160e789a6fdd8efe8123ea5877b83531312dfdad9afbd231355d7322abc0a880", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Try out the example in this book\u2019s Github repository. The result, when we tried it on \nClaude, met the conditions, with each paragraph ending with the sentences stated. \nBut the paragraphs don\u2019t quite make sense. For example, the second paragraph reads \nas follows: <br><br>Throughout human history, we have sought guidance when facing difficult choices, \nlooking to those with wisdom or authority to show us the way. Ancient civilizations \nconsulted oracles and shamans, while modern societies turn to mentors, therapists, or \nspiritual advisors. When lost in the wilderness of indecision, we naturally seek those \nwho might possess a map through the uncertain terrain. Take me to your leader. <br><br>The essay as a whole doesn\u2019t gel, either. Here are some of the reasons: <br><br>It\u2019s stuck on its initial path <br><br>The first quote, from Hamlet, is quite philosophical, and this leads the essay in a \nphilosophical direction that doesn\u2019t quite fit the next two quotes. The initial path \nmight be flawed and result in no solution. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67bc9c67-3466-4f5e-afb8-b7157eea5813": {"__data__": {"id_": "67bc9c67-3466-4f5e-afb8-b7157eea5813", "embedding": null, "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 363.51, "x1": 1559.5, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "df7b52d8-25a3-4f8f-9fdf-37a75a844c26", "node_type": "1", "metadata": {"bbox": [{"page": 244, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 122.57, "x1": 432.0, "y1": 311.67}]}, "hash": "57082d06d204bae652fb1a6aa63b23df6a4d5f13355eddba113b9416dc21b040", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a0d545f-5a0c-4ad9-a5e6-f7dc58b7b3c0", "node_type": "1", "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.55, "x1": 432.0, "y1": 354.14}]}, "hash": "9605fbc23becfeea00634741aa3000050dfd08c94db8afb1d230e8287d16ee82", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Thought generation**<br><br>Each iteration step in the ToT pattern starts with generating several thoughts by \nusing an LLM. The model is prompted to generate diverse next steps and returns the \n\u201cthoughts\u201d as a JSON-formatted list. <br><br>Each time you call generate_thoughts, you present the LLM with the current state of \nyour iterations: your top-K solutions for the current state of your ToT generation. <br><br>The following example shows the thought-generation prompt from our example \ngeneration. <br><br>**def**generate_thoughts(self, state: str, step: int) -> List[str]: \n*\"\"\" Generate multiple possible next thoughts. \"\"\"*\n self.call_count += 1 <br><br> prompt = f\"\"\"{state} \n You are solving a problem step-by-step using the Tree of Thoughts method. Think about the problem state above and generate {self.num_thoughts_per_step} distinct and diverse next steps. This is step {step} of up to {self.max_steps} steps. Generate {self.num_thoughts_per_step} different possible next thoughts to make progress on this problem. \n Make each thought meaningfully different to explore diverse approaches. \"\"\" \n ... \n thoughts = json.loads(content) \n**return**thoughts <br><br>For the essay problem, here are the thoughts it generated for step 1: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a0d545f-5a0c-4ad9-a5e6-f7dc58b7b3c0": {"__data__": {"id_": "7a0d545f-5a0c-4ad9-a5e6-f7dc58b7b3c0", "embedding": null, "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.55, "x1": 432.0, "y1": 354.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "67bc9c67-3466-4f5e-afb8-b7157eea5813", "node_type": "1", "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 363.51, "x1": 1559.5, "y1": 607.79}]}, "hash": "160e789a6fdd8efe8123ea5877b83531312dfdad9afbd231355d7322abc0a880", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97ffb17a-eb78-4d15-9db5-982a8c34dd71", "node_type": "1", "metadata": {"bbox": [{"page": 248, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.43, "x1": 432.0, "y1": 324.5}]}, "hash": "05acd3c80a5d8f096de10133d12026d3c1f9239ae2bd65e27a8c9dc4d9c0a419", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generated 3 thoughts for step 1: <br><br>1. I need to write a 4-paragraph essay where each paragraph ends with one of the \nprovided sentences. I could structure it around the theme of making decisions, since \nmany of these quotes relate to choices. <br><br>2. I could create a creative narrative that somehow connects these very different quotes \n- perhaps about an alien encounter that leads to philosophical reflections on life and \nrelationships. <br><br>3. I should first identify the themes of each quote (existential questioning, alien con\u2010 \ntact, marriage/social expectations, and overcoming fear) and then craft a cohesive essay \nthat naturally leads to each conclusion. <br><br>The first thought notes that \u201cmany\u201d of the quotes relate to choices, the second is an \ninspired narrative about an \u201calien encounter,\u201d and the third is a strategic approach. All \nthree could work, so which one is best and should be explored first? <br><br>**Path evaluation**<br><br>As the next step, you need to loop over all of the generated thoughts and evaluate \nthem in the context of the current state. Here, you\u2019ll ask the LLM to judge the \nproposed reasoning path and give it a score between 0 and 100, applying evalu \nate_state to all thoughts. This code divides the 0\u2013100 score by 100 to get a score on \nthe 0\u20131 scale: <br><br>**def**evaluate_state(self, state: str, problem: str) -> float: \n*\"\"\" Evaluate the promise of a reasoning path using Claude. \"\"\"*", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97ffb17a-eb78-4d15-9db5-982a8c34dd71": {"__data__": {"id_": "97ffb17a-eb78-4d15-9db5-982a8c34dd71", "embedding": null, "metadata": {"bbox": [{"page": 248, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.43, "x1": 432.0, "y1": 324.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7a0d545f-5a0c-4ad9-a5e6-f7dc58b7b3c0", "node_type": "1", "metadata": {"bbox": [{"page": 247, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.55, "x1": 432.0, "y1": 354.14}]}, "hash": "9605fbc23becfeea00634741aa3000050dfd08c94db8afb1d230e8287d16ee82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22a098e8-2ed7-44ee-98a7-4e8f1b9befec", "node_type": "1", "metadata": {"bbox": [{"page": 249, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.13, "x1": 432.01, "y1": 370.35}]}, "hash": "8a354b45361d424ac6e4ad420f28b4162a02bd310e070da03f6e1d80335f55b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This tells the LLM that the most promising approach is to identify the themes of each \nquote first. <br><br>**Tree search**<br><br>At this point in the workflow, you have several thoughts regarding the current state \nof the tree, each of which has been evaluated and scored. The scores will help you \ndetermine the best next paths or know whether you have found the perfect path and \ncan terminate the search early. <br><br>The following code snippet uses the previously introduced generate_thoughts and \nevaluate_state functions. We store the candidates with the negative score so that we \ncan pick the best by selecting the lowest ones. <br><br>candidates = [] \n... \nthoughts = self.generate_thoughts(current_state, step) \n**for**thought**in**thoughts: \n new_state = f\"{current_state}**\\n**Step {step}: {thought}\" \n new_path = reasoning_path + [f\"Step {step}: {thought}\"] \n new_score = self.evaluate_state(new_state, problem) \n candidates.append((-new_score, new_state, new_path, step)) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22a098e8-2ed7-44ee-98a7-4e8f1b9befec": {"__data__": {"id_": "22a098e8-2ed7-44ee-98a7-4e8f1b9befec", "embedding": null, "metadata": {"bbox": [{"page": 249, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.13, "x1": 432.01, "y1": 370.35}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "97ffb17a-eb78-4d15-9db5-982a8c34dd71", "node_type": "1", "metadata": {"bbox": [{"page": 248, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.43, "x1": 432.0, "y1": 324.5}]}, "hash": "05acd3c80a5d8f096de10133d12026d3c1f9239ae2bd65e27a8c9dc4d9c0a419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3b8af68-5d26-450a-85e5-19d1b5281914", "node_type": "1", "metadata": {"bbox": [{"page": 250, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.71, "x1": 432.01, "y1": 318.86}]}, "hash": "e8b4e06712b901f987cd042d8184c3c983c5ce5fd0977a8b4b96a1efbacf8e42", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Based on this approach, the model chose the following reasoning path for the essay \nproblem: <br><br>Step 1: I could create a creative narrative that somehow connects these very different \nquotes - perhaps about an alien encounter that leads to philosophical reflections on life \nand relationships. <br><br>Step 2: I could create an essay about human exploration, with paragraph 1 about phil\u2010 \nosophical questions of our purpose, paragraph 2 about encountering extraterrestrial \nlife, paragraph 3 about human connection and relationships, and paragraph 4 about \novercoming fear to progress. <br><br>Step 3: I could frame this as an essay about literature itself, with each paragraph discus\u2010 \nsing how different literary works have shaped human thought: Shakespeare\u2019s existential \nquestions, science fiction\u2019s exploration of the unknown, Austen\u2019s commentary on social \nrelationships, and Roosevelt\u2019s inspiring words about courage. <br><br>Step 4: I\u2019ll write an essay about human nature through literature, connecting Shake\u2010 \nspeare\u2019s existential questions, science fiction\u2019s exploration of the unknown, Austen\u2019s \nsocial commentary, and Roosevelt\u2019s wisdom about courage. <br><br>**Summary generation**<br><br>The final response generation is straightforward: we present the LLM with the initial \nproblem statement and the final reasoning state and then ask it to respond concisely \nto the initial problem statement. <br><br>**def**generate_solution(self, problem: str, final_state: str) -> str: \n*\"\"\" Generate a concise summary of the solution.\"\"\"*\n self.call_count += 1 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3b8af68-5d26-450a-85e5-19d1b5281914": {"__data__": {"id_": "c3b8af68-5d26-450a-85e5-19d1b5281914", "embedding": null, "metadata": {"bbox": [{"page": 250, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.71, "x1": 432.01, "y1": 318.86}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "22a098e8-2ed7-44ee-98a7-4e8f1b9befec", "node_type": "1", "metadata": {"bbox": [{"page": 249, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.13, "x1": 432.01, "y1": 370.35}]}, "hash": "8a354b45361d424ac6e4ad420f28b4162a02bd310e070da03f6e1d80335f55b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed5b3b98-d131-4aa6-a3e9-9732e5faf531", "node_type": "1", "metadata": {"bbox": [{"page": 251, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 178.56, "x1": 383.82, "y1": 250.22}]}, "hash": "464c28cb3159c99df86fcb05c6c4e24971665ce8422617b059249c055e71e0c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isn\u2019t this much better? A little planning and evaluation go a long way! <br><br>We only show an abbreviated implementation here. Please check out the full ToT \nimplementation and the essay problem results in our Github repository. <br><br>**Example**\nToT has shown strong results when used on strategic problems. For our next exam\u2010 \nple, let\u2019s consider a realistic supply chain problem with multiple possible solutions. \nWe\u2019ll see the benefits of the LLM exploring different avenues on the way to the final \nsolution. <br><br>We start with an initial problem statement: <br><br>I need to optimize our supply chain using Tree of Thoughts: <br><br>Current situation: <br><br>- 3 potential manufacturing locations (Mexico, Vietnam, Poland) <br><br>- 4 distribution centers (Atlanta, Chicago, Dallas, Seattle) <br><br>- 2 primary shipping methods (air, sea) <br><br>- Historical demand fluctuations of \u00b120% <br><br>- Recent disruptions in Asian shipping routes ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed5b3b98-d131-4aa6-a3e9-9732e5faf531": {"__data__": {"id_": "ed5b3b98-d131-4aa6-a3e9-9732e5faf531", "embedding": null, "metadata": {"bbox": [{"page": 251, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 178.56, "x1": 383.82, "y1": 250.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c3b8af68-5d26-450a-85e5-19d1b5281914", "node_type": "1", "metadata": {"bbox": [{"page": 250, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.71, "x1": 432.01, "y1": 318.86}]}, "hash": "e8b4e06712b901f987cd042d8184c3c983c5ce5fd0977a8b4b96a1efbacf8e42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f22c7fc-99a3-475b-8b95-da4db7e46a60", "node_type": "1", "metadata": {"bbox": [{"page": 253, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.76}]}, "hash": "c626eee5f068c9ff3df08c7ac275ec8d6c3a5a5e9afbd7d37d6686790f9d53f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then score the initial thoughts (also see Figure 5-7). <br><br>Top 3 states after step 1: <br><br>1. Score: 0.65 | Step 1: Let me begin by mapping the transportation networks ... <br><br>2. Score: 0.35 | Step 1: I\u2019ll first define the key attributes of each manufacturing... <br><br>3. Score: 0.35 | Step 1: Instead of looking at all possible combinations, I\u2019ll... ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f22c7fc-99a3-475b-8b95-da4db7e46a60": {"__data__": {"id_": "5f22c7fc-99a3-475b-8b95-da4db7e46a60", "embedding": null, "metadata": {"bbox": [{"page": 253, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ed5b3b98-d131-4aa6-a3e9-9732e5faf531", "node_type": "1", "metadata": {"bbox": [{"page": 251, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 178.56, "x1": 383.82, "y1": 250.22}]}, "hash": "464c28cb3159c99df86fcb05c6c4e24971665ce8422617b059249c055e71e0c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff4df780-3288-41a6-b0dd-cd3aca9006b6", "node_type": "1", "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 376.22, "x1": 432.0, "y1": 605.76}]}, "hash": "3ad1da684375abd450c4d7b76c0a92dcfe3158f9c1855778f7593898aafe0665", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(30%), then stress test each configuration against the three scenarios to generate \nquantitative performance data. (score: 0.65) <br><br>For brevity\u2019s sake, we\u2019ll skip the third set here. <br><br>Now, we score all available thoughts and preserve the top three states: <br><br>Top 3 states after step 2: <br><br>1. Score: 0.75 | Step 2: Before designing full configurations, I should analyze... <br><br>2. Score: 0.75 | Step 2: Let me develop three distinct supply chain configurations... <br><br>3. Score: 0.65 | Step 2: I\u2019ll create a simulation model with weighted scoring... <br><br>We\u2019ll continue walking down the tree (see Figure 5-8) until we either reach a max\u2010 \nimum tree depth or come across a great solution (one with a score higher than \n0.95). At that point, we can stop the tree search and terminate the process early. \nFor brevity\u2019s sake, we won\u2019t print all of the steps here, but you can try the example \nyourself in our Github repository. <br><br><br><br>Figure 5-8. The next step in the tree search <br><br>We\u2019ve reached the maximum tree depth of four layers, and the path with the most \npromising scores contained the following thoughts: <br><br>Reasoning Path: <br><br>**236** **|** **Chapter 5: Extending Model Capabilities**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff4df780-3288-41a6-b0dd-cd3aca9006b6": {"__data__": {"id_": "ff4df780-3288-41a6-b0dd-cd3aca9006b6", "embedding": null, "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 376.22, "x1": 432.0, "y1": 605.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5f22c7fc-99a3-475b-8b95-da4db7e46a60", "node_type": "1", "metadata": {"bbox": [{"page": 253, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.76}]}, "hash": "c626eee5f068c9ff3df08c7ac275ec8d6c3a5a5e9afbd7d37d6686790f9d53f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "452e0739-b055-4c62-8616-a5b5522bfeff", "node_type": "1", "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.22, "x1": 432.0, "y1": 366.84}]}, "hash": "ddfe4da7d9016e2fa888038b1d7eac4785630a19bffe8c105accf2d713d800a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Step 1: Let me begin by mapping the transportation networks between all locations, \ncalculating baseline costs and transit times for both air and sea options, so I can \nidentify potential bottlenecks and vulnerabilities. <br><br>Step 2: Let me develop three distinct supply chain configurations, each with different \nmanufacturing/distribution combinations optimized for specific priorities: Configura\u2010 \ntion A focused on cost minimization (Mexico + slower shipping), Configuration B \nfocused on speed (balanced locations + air shipping), and Configuration C focused on \nresilience (distributed manufacturing across all locations). <br><br>Step 3: Let me analyze each of the three configurations (A, B, and C) under the three \ndifferent scenarios (normal operations, major shipping disruption, and 30% demand \nincrease) to evaluate their performance on total cost, delivery time reliability, and \ndisruption vulnerability. I\u2019ll create a comprehensive performance matrix to visualize \nhow each configuration handles each scenario. <br><br>Step 4: Let me conduct a detailed sensitivity analysis by varying the probability of each \nscenario (normal operations, shipping disruption, demand increase) and seeing how \nthat affects the optimal configuration. This will reveal which configuration is most \nrobust across different probability distributions of future events. <br><br>We can now take the reasoning path and prompt an LLM to generate a final response \nto the initial supply chain question. Here is the generated answer: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "452e0739-b055-4c62-8616-a5b5522bfeff": {"__data__": {"id_": "452e0739-b055-4c62-8616-a5b5522bfeff", "embedding": null, "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.22, "x1": 432.0, "y1": 366.84}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ff4df780-3288-41a6-b0dd-cd3aca9006b6", "node_type": "1", "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 376.22, "x1": 432.0, "y1": 605.76}]}, "hash": "3ad1da684375abd450c4d7b76c0a92dcfe3158f9c1855778f7593898aafe0665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79d620e3-2d04-4185-b8e2-bf67f83e43c0", "node_type": "1", "metadata": {"bbox": [{"page": 255, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 282.64}]}, "hash": "96b9584ccf14a363e3705336905c99f731232b54b55da566f2b57e2156001174", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Solution Summary: <br><br># Supply Chain Optimization Summary <br><br>Based on the Tree of Thoughts analysis, the optimal supply chain configuration is Con\u2010 \nfiguration C, which focuses on resilience through distributed manufacturing across \nMexico, Poland, and Vietnam with a mix of shipping methods. <br><br>## Key Insights: <br><br>- Configuration A (cost-focused with Mexico manufacturing) performed well under \nnormal conditions but suffered severely during disruptions. <br><br>- Configuration B (speed-focused with air shipping) offered reliability but at signifi\u2010 \ncantly higher costs. <br><br>- Configuration C (resilience-focused with distributed manufacturing) provided the \nbest balance of cost, speed, and resilience across all scenarios. <br><br>The distributed manufacturing approach in Configuration C creates built-in redun\u2010 \ndancy that allows for rapid adaptation to shipping disruptions and demand fluctua\u2010 \ntions. While slightly more expensive than Configuration A under normal conditions, \nConfiguration C\u2019s ability to maintain operations during disruptions and scale during \ndemand increases provides the best risk-adjusted performance. <br><br>This solution addresses the recent Asian shipping route disruptions by reducing \ndependency on any single manufacturing location, while maintaining cost efficiency \nthrough strategic use of both air and sea shipping based on urgency and conditions. <br><br>To arrive at this response and evaluate all nodes and generate scores, we had to make \n41 model API calls. The entire process took 93 seconds, which is a substantial time \nfor real-world applications. You can speed this up by doing some of the generations ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79d620e3-2d04-4185-b8e2-bf67f83e43c0": {"__data__": {"id_": "79d620e3-2d04-4185-b8e2-bf67f83e43c0", "embedding": null, "metadata": {"bbox": [{"page": 255, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 282.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "452e0739-b055-4c62-8616-a5b5522bfeff", "node_type": "1", "metadata": {"bbox": [{"page": 254, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.22, "x1": 432.0, "y1": 366.84}]}, "hash": "ddfe4da7d9016e2fa888038b1d7eac4785630a19bffe8c105accf2d713d800a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4a7d7bb-1028-45c9-9138-161a50bf929a", "node_type": "1", "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 226.13, "x1": 432.0, "y1": 336.48}]}, "hash": "5c834e6b19371cadff0c7f4d1f9efff2c52efd06631640de3a48b83e6fc6ff9a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Complexity <br><br>Implementing ToT requires various aspects from thought generation, scoring, \nbeam search, and summary generation. Between the tasks, you need to track the \nstate, explored paths, and number of steps. You can implement the tree search in \nvarious ways\u2014such as beam search, breadth-first search, and depth-first search \n\u2014but each implementation should be a small project on its own. The increased \ncomplexity also comes with more room for implementation bugs. <br><br>**Alternatives**<br><br>Even if CoT doesn\u2019t work, you may be able to avoid ToT by using reasoning models, \nleast-to-most prompting, reflection, or wait-injection. These are all less complex than \nToT. <br><br>**Reasoning models.**ToT implementations come with major complexities: the thought \ngeneration, tree search, and tuning of different parameters, like depth, width, and \nthresholds for good solutions. You need to call the LLM several times, and each step ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4a7d7bb-1028-45c9-9138-161a50bf929a": {"__data__": {"id_": "c4a7d7bb-1028-45c9-9138-161a50bf929a", "embedding": null, "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 226.13, "x1": 432.0, "y1": 336.48}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "79d620e3-2d04-4185-b8e2-bf67f83e43c0", "node_type": "1", "metadata": {"bbox": [{"page": 255, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 282.64}]}, "hash": "96b9584ccf14a363e3705336905c99f731232b54b55da566f2b57e2156001174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d056a8af-24f3-4620-9035-dafb9cef11b7", "node_type": "1", "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 78.62, "x1": 423.75, "y1": 206.12}]}, "hash": "0a8b17b9ac122069350565a8e20d81cc681a0b97c2ea12d4e8455e5a31b7a1fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In some cases, you can even combine these techniques: you might use LtM to break \na problem down into subproblems and then apply ToT to explore multiple solutions \nfor particularly complex subproblems. <br><br>**Reflection.**Reflection is an agentic technique that uses LLMs to evaluate and critique \ntheir own outputs. LLMs can \u201creflect\u201d on their generation and potentially even \ndiscard it if it doesn\u2019t match an expectation. This helps them move toward more \nmethodical reasoning. However, reflection also addresses the problem linearly. See \nthe section on Reflection (Pattern 18) in Chapter 6 for more on this technique. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d056a8af-24f3-4620-9035-dafb9cef11b7": {"__data__": {"id_": "d056a8af-24f3-4620-9035-dafb9cef11b7", "embedding": null, "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 78.62, "x1": 423.75, "y1": 206.12}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c4a7d7bb-1028-45c9-9138-161a50bf929a", "node_type": "1", "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 226.13, "x1": 432.0, "y1": 336.48}]}, "hash": "5c834e6b19371cadff0c7f4d1f9efff2c52efd06631640de3a48b83e6fc6ff9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d23736d-94dd-4c2b-ba5f-7f2bba878494", "node_type": "1", "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 226.91, "x1": 432.01, "y1": 556.54}]}, "hash": "fc48f05d1feb0b4dc2384943714d9bed16009517abfccd3c9f69cf0644086205", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Andrew Ng\u2019s Four Design Pattern Categories**\nAndrew Ng\u2014the prominent AI researcher, educator, and founder of DeepLearn\u2010 \ning.ai\u2014has identified four categories of design patterns: Reflection, Tool Use, Plan\u2010 \nning, and Multiagent Collaboration. In this book, we treat Reflection (Pattern \n18), Tool Use (which we call Tool Calling [Pattern 21]), and Multiagent Collabora\u2010 \ntion (Pattern 23) as standalone patterns, while Deep Search (Pattern 12), Chain-of- \nThought (Pattern 13), and ToT are examples of Ng\u2019s Planning patterns. Many of \nthe patterns in this book involve all four of these patterns as ingredients and would \ntherefore fall into several of Ng\u2019s categories. The ToT pattern falls into all four\u2014it \nplans before jumping into generation, the thought generation phase is an example ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d23736d-94dd-4c2b-ba5f-7f2bba878494": {"__data__": {"id_": "2d23736d-94dd-4c2b-ba5f-7f2bba878494", "embedding": null, "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 226.91, "x1": 432.01, "y1": 556.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d056a8af-24f3-4620-9035-dafb9cef11b7", "node_type": "1", "metadata": {"bbox": [{"page": 256, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 78.62, "x1": 423.75, "y1": 206.12}]}, "hash": "0a8b17b9ac122069350565a8e20d81cc681a0b97c2ea12d4e8455e5a31b7a1fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79789cd0-501a-45cb-a33a-951d943c2dda", "node_type": "1", "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.2, "x1": 432.0, "y1": 218.81}]}, "hash": "07a1b88ad11aa903c1e62352925d76a534cd941d0fb6594c3b3d1fee2fa06e35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Wait-injection.**The idea of wait-injection, which is also called budget forcing, is as \ningenious as it is simple. The core idea is to inject the word Wait when a model wants \nto terminate its output generation. During the generation, you would overwrite the \ntermination token with the word Wait. The missing end-of-sequence token triggers \nthe model to continue generating and effectively reevaluate the previous statement. \nThe final generated response is often way more reflective than the initial response. <br><br>**References**\nThe ToT concept was described in great detail in the paper \u201cTree of Thoughts: \nDeliberate Problem Solving with Large Language Models\u201d by Yao et al. (2023). \nWait-injection was proposed by Muennighoff et al. (2025), and the authors offer \nan open-source example. <br><br>**Pattern 15: Adapter Tuning**\nThe Adapter Tuning pattern fine-tunes a foundational model to perform a specialized \ntask by efficiently training a few add-on neural network layers on a small dataset of \nexamples. <br><br>**Problem**\nPretrained foundational models like GPT-4, Gemini, and Claude are very powerful \nand capable of a wide variety of tasks. The canonical way to unlock their capabilities \nis through prompts. Prompts work because these models have been trained to follow \ninstructions, have encountered millions of examples, and are extremely good writers. \nBut what if the model\u2019s response isn\u2019t quite what you want? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79789cd0-501a-45cb-a33a-951d943c2dda": {"__data__": {"id_": "79789cd0-501a-45cb-a33a-951d943c2dda", "embedding": null, "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.2, "x1": 432.0, "y1": 218.81}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2d23736d-94dd-4c2b-ba5f-7f2bba878494", "node_type": "1", "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 226.91, "x1": 432.01, "y1": 556.54}]}, "hash": "fc48f05d1feb0b4dc2384943714d9bed16009517abfccd3c9f69cf0644086205", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "309a46fc-8551-4566-9d88-4e4692b5db37", "node_type": "1", "metadata": {"bbox": [{"page": 258, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.12, "x1": 432.01, "y1": 307.32}]}, "hash": "7b00074c2c6e566f788513b1cc3643811f1fe64c731e7e2e4395262c1c0768bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, we asked Gemini to do this: <br><br>Suggest 3 ways to improve the flavor of ice cream. <br><br>And we got this response: <br><br>1. Use high-quality, fresh ingredients. <br><br>2. Enhance flavor depth with additions like salt or roasted elements. <br><br>3. Chill base thoroughly and freeze rapidly for optimal texture and flavor. <br><br>These are fine, but they are principles. Suppose you don\u2019t like these responses and \nwould prefer a more concrete set of ideas, such as these: <br><br>1. Infuse the base with fresh ingredients like mint or citrus zest. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "309a46fc-8551-4566-9d88-4e4692b5db37": {"__data__": {"id_": "309a46fc-8551-4566-9d88-4e4692b5db37", "embedding": null, "metadata": {"bbox": [{"page": 258, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.12, "x1": 432.01, "y1": 307.32}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "79789cd0-501a-45cb-a33a-951d943c2dda", "node_type": "1", "metadata": {"bbox": [{"page": 257, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.2, "x1": 432.0, "y1": 218.81}]}, "hash": "07a1b88ad11aa903c1e62352925d76a534cd941d0fb6594c3b3d1fee2fa06e35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfe2bce9-f5bf-4c3c-9658-3f64e3fed016", "node_type": "1", "metadata": {"bbox": [{"page": 260, "page_height": 661.5, "page_width": 504.0, "x0": 80.65, "y0": 431.84, "x1": 432.0, "y1": 601.54}, {"page": 262, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 384.04, "x1": 432.01, "y1": 605.54}]}, "hash": "64f3210a8d03455a477b6fe11cd04877127e32ccdaaae523ab85af76e68ab458", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Adapter Tuning provides a way to efficiently post-train a foundational model based \non a training dataset that consists of a few hundred to a few thousand example \ninput-output pairs. Adapter Tuning is more principled than prompt engineering, \nscales to more numerous and more complex examples than few-shot learning, and is \nless cumbersome and trouble prone than full fine-tuning. (See Chapter 1 for a deeper \nintroduction to post-training.) <br><br>**Solution**\nAdapter Tuning is an efficient way of fine-tuning a foundational model to perform a \nspecialized task. This is because of its special architecture that\u2019s parameter efficient to \ntrain, convenient to deploy, and fast at inference. <br><br>**Architecture**<br><br>In Adapter Tuning, you need to train only a few add-on neural network layers (see \nthe green boxes in Figure 5-9, which are marked \u201cAdapter\u201d). You insert the adapter \nlayers into the transformer block, which forms the building block of foundational \nmodels. Where you insert the adapters varies. You also need only a few hundred to ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfe2bce9-f5bf-4c3c-9658-3f64e3fed016": {"__data__": {"id_": "cfe2bce9-f5bf-4c3c-9658-3f64e3fed016", "embedding": null, "metadata": {"bbox": [{"page": 260, "page_height": 661.5, "page_width": 504.0, "x0": 80.65, "y0": 431.84, "x1": 432.0, "y1": 601.54}, {"page": 262, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 384.04, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "309a46fc-8551-4566-9d88-4e4692b5db37", "node_type": "1", "metadata": {"bbox": [{"page": 258, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.12, "x1": 432.01, "y1": 307.32}]}, "hash": "7b00074c2c6e566f788513b1cc3643811f1fe64c731e7e2e4395262c1c0768bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28e75883-e6ce-47da-afdd-74c3505b7b86", "node_type": "1", "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.26, "x1": 432.01, "y1": 605.54}]}, "hash": "cbbd668fc463f7aae95c4dd78b6a24beeb5a84346d8ea05af2a1de56c2ca099e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Adapter Tuning teaches a pretrained foundational model to do a specialized task. \n\u2022 \nIt is not for industry jargon or new facts. Common specialized tasks for which \nAdapter Tuning is employed include classification, summarization, extractive \nquestion answering, and creating chatbots whose responses align with a compa\u2010 \nny\u2019s brand. <br><br>\u2022 The foundational model weights are frozen, and the adapter layers provide small \n\u2022 \nupdates to a few of the layers (see Figure 5-9). Therefore, adapter tuning is \nvery efficient\u2014it can often be accomplished on a single GPU in under an hour \n(depending on the size of the model and capability of the GPU, of course). <br><br>\u2022 The training dataset can be smaller than is typical in deep learning because \n\u2022 \nyou don\u2019t retrain the full model (which requires millions of tokens of training \ndata) or continue pretraining the full model weights (which runs a high risk of \ncatastrophic forgetting). Here, you need to train only the adapter weights. <br><br>\u2022 Another dense layer that re-creates the original dimensionality. In our example, \n\u2022 \nthis dense layer would compute 768 different weighted averages of the 64 inputs. <br><br>In our example, we\u2019d have 768 \u00d7 64 \u00d7 2 total parameters in the adapter layer. \nBecause the number of parameters to be trained is much smaller than the billions \nof weights (or parameters) in the foundational model, Adapter Tuning is a form of \nparameter-efficient fine-tuning (PeFT). <br><br>Because the nonlinearity in the adapter layers is at a lower dimension than the \ntransformer blocks (64, in our example), Adapter Tuning is also colloquially referred \nto as low-rank adaptation (LoRA)\u2014even though it\u2019s not strictly what researchers \nthink of as a LoRA architecture. <br><br>The adapter architecture is parameter efficient in terms of training. The foundational \nmodel is used as-is, and only the adapter layer weights (shown in green in Fig\u2010 \nure 5-10) need to be adjusted. Because there aren\u2019t that many weights in the adapter \nlayers, training doesn\u2019t require much data\u2014in practice, as few as 100 examples might \nsuffice. It is also quite fast, with most of the time it takes coming from the fact that the \nfoundational model is still required to do inference. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28e75883-e6ce-47da-afdd-74c3505b7b86": {"__data__": {"id_": "28e75883-e6ce-47da-afdd-74c3505b7b86", "embedding": null, "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.26, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cfe2bce9-f5bf-4c3c-9658-3f64e3fed016", "node_type": "1", "metadata": {"bbox": [{"page": 260, "page_height": 661.5, "page_width": 504.0, "x0": 80.65, "y0": 431.84, "x1": 432.0, "y1": 601.54}, {"page": 262, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 384.04, "x1": 432.01, "y1": 605.54}]}, "hash": "64f3210a8d03455a477b6fe11cd04877127e32ccdaaae523ab85af76e68ab458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af10fc44-b6ac-4fa7-8046-55c382575c1b", "node_type": "1", "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.86, "x1": 432.0, "y1": 195.44}]}, "hash": "4453df0ee2387a1f9be4463946fbfb5776672d9bfaee0cdbcf3ee52fd7838973", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "modified. It\u2019s possible to recreate the insertion of adapter layers into the foundational \nmodel during inference, which minimizes extra storage requirements. <br><br>A further optimization is possible: instead of training a foundational model, you can \ntrain a quantized version of the foundational model. You might see this technique \nreferred to as quantization-aware low-rank adaptation (QLoRA), which is what we \ndemonstrate in the \u201cExample\u201d section. <br><br>**Training**<br><br>At the time of writing (April 2025), the most popular open source framework for \nfine-tuning is the Transformer Reinforcement Learning (TRL) package from Hugging \nFace. To fine-tune a foundational model, you need access to its layers so that you \ncan insert adapter layers in between them. Closed-weights models get around this \nby providing fully managed fine-tuning services. However, Adapter Tuning is usually \ncarried out on open-weights models for two reasons. First, you can fine-tune smaller \nmodels for specialized tasks\u2014you don\u2019t need a massive model like GPT-4 or Gemini. \nSecond, this allows you to run the model on local hardware that\u2019s disconnected from \nthe internet. <br><br>The first step is to load the foundational model you want to fine-tune. Quantized \n4-bit versions of many open-weights models (including Llama, Mistral, Phi, Gemma, \nand DeepSeek) are available from Unsloth. We showed you an example of fine-tuning \nLlama using the Unsloth framework in Chapter 1. Here, we\u2019ll illustrate how you can \nload the floating-point version of the model and its tokenizer from Hugging Face and \nthen quantize it into four bits. (See the full code in this book\u2019s GitHub repository.): <br><br>model_kwargs[\"quantization_config\"] = BitsAndBytesConfig( \n load_in_4bit=**True**, \n bnb_4bit_use_double_quant=**True**, \n bnb_4bit_quant_type=\"nf4\", \n bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"], \n bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"], \n) \nmodel**=**AutoModelForImageTextToText**.**from_pretrained(model_id,** ****model_kwargs) \nprocessor**=**AutoProcessor**.**from_pretrained(\"google/gemma-3-4b-it\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af10fc44-b6ac-4fa7-8046-55c382575c1b": {"__data__": {"id_": "af10fc44-b6ac-4fa7-8046-55c382575c1b", "embedding": null, "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.86, "x1": 432.0, "y1": 195.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "28e75883-e6ce-47da-afdd-74c3505b7b86", "node_type": "1", "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.26, "x1": 432.01, "y1": 605.54}]}, "hash": "cbbd668fc463f7aae95c4dd78b6a24beeb5a84346d8ea05af2a1de56c2ca099e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51b830db-6831-4867-a45a-5084e8ee1892", "node_type": "1", "metadata": {"bbox": [{"page": 265, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.4, "x1": 956.0, "y1": 329.79}]}, "hash": "9db8f4d4262c8b09a2300a8da4edd3f930d433d2b4d97e7420acf84b7414d29e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, set up the adapter layers. Setting the rank (r) to be 16 means that inputs are \nreduced down to 16 dimensions. The larger the r, the more weights you will be \ntuning, so set this based on the size of your dataset: <br><br>peft_config = LoraConfig( \n lora_alpha=16, \n lora_dropout=0.05, \n r=16, \n bias=\"none\", \n target_modules=\"all-linear\", \n task_type=\"CAUSAL_LM\", \n modules_to_save=[ ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51b830db-6831-4867-a45a-5084e8ee1892": {"__data__": {"id_": "51b830db-6831-4867-a45a-5084e8ee1892", "embedding": null, "metadata": {"bbox": [{"page": 265, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.4, "x1": 956.0, "y1": 329.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "af10fc44-b6ac-4fa7-8046-55c382575c1b", "node_type": "1", "metadata": {"bbox": [{"page": 264, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.86, "x1": 432.0, "y1": 195.44}]}, "hash": "4453df0ee2387a1f9be4463946fbfb5776672d9bfaee0cdbcf3ee52fd7838973", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da7d7b5c-545e-47f0-8409-f70075169a27", "node_type": "1", "metadata": {"bbox": [{"page": 266, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.85, "x1": 432.01, "y1": 250.18}]}, "hash": "d31e6de41d50d350132909fda92aef7ad5173da0175c4e8cf41870c75964672d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the model in question expects to see messages during inference, make sure that the \ntraining data consists of messages. Given an input prompt and output text, you can \nformat a single input-output pair as a set of messages: <br><br>[ \n{'role': 'system', \n 'content': [{'type': 'text', \n 'text': 'You are a food influencer.'}]}, \n{'role': 'user', \n 'content': [{'type': 'text', \n 'text': 'Suggest 3 ways to improve the flavor of ice cream.'}, \n{'role': 'assistant', \n 'content': [{'type': 'text', \n 'text': '1. Infuse the base with fresh ingredients like mint or citrus zest.\\n2. Add mix-ins such as roasted nuts or crushed cookies.\\n3. Add flaky salt on top before serving to intensify the flavor.'}, \n] <br><br>The input prompt is assigned to a message from the user, while the output response \nis assigned to a message from the assistant. These messages are then passed to the \nmodel, which compares its response to the desired assistant output in order to update \ngradients during the training process: <br><br>trainer = SFTTrainer( \n model=model, \n args=sft_config, \n train_dataset=messages, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da7d7b5c-545e-47f0-8409-f70075169a27": {"__data__": {"id_": "da7d7b5c-545e-47f0-8409-f70075169a27", "embedding": null, "metadata": {"bbox": [{"page": 266, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.85, "x1": 432.01, "y1": 250.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "51b830db-6831-4867-a45a-5084e8ee1892", "node_type": "1", "metadata": {"bbox": [{"page": 265, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.4, "x1": 956.0, "y1": 329.79}]}, "hash": "9db8f4d4262c8b09a2300a8da4edd3f930d433d2b4d97e7420acf84b7414d29e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dc6344c-f7bd-4c7e-a2ba-ad698c813263", "node_type": "1", "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 371.63, "x1": 423.75, "y1": 605.63}]}, "hash": "245943b4df46416684783556fc972e03f95963af98f97477868aed84c0c99c34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Example**\nFine-tuning isn\u2019t limited to text models\u2014you can apply it to other image, audio, and \nvideo models, too. To emphasize this point, we\u2019ll illustrate Adapter Tuning end to end \non a multimodal model, teaching the model to take an image as input and produce \ntext as output. <br><br>**Anatomical descriptions**<br><br>What we want to do is to teach this model to caption a set of radiology images. \nTo make things interesting, we want the caption to explain the anatomy depicted \nin the images and concisely state any problems, but not to provide a full diagnosis \nor explanation. For example, given the image shown in Figure 5-11, we\u2019d want the \ncaption to be as follows: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dc6344c-f7bd-4c7e-a2ba-ad698c813263": {"__data__": {"id_": "5dc6344c-f7bd-4c7e-a2ba-ad698c813263", "embedding": null, "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 371.63, "x1": 423.75, "y1": 605.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "da7d7b5c-545e-47f0-8409-f70075169a27", "node_type": "1", "metadata": {"bbox": [{"page": 266, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 85.85, "x1": 432.01, "y1": 250.18}]}, "hash": "d31e6de41d50d350132909fda92aef7ad5173da0175c4e8cf41870c75964672d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aac59e3b-3be8-4fcd-ac8b-0c6de5f424a3", "node_type": "1", "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.81, "x1": 432.0, "y1": 347.58}]}, "hash": "727a302dd8162152475b04f8fd5e3fab1fd3a55a94fecdf5ec4782074bd6e70f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "instructions. The complex system instructions required to achieve that level of \naccuracy incur a large token count, and that makes the application too expensive \nto scale. (See also the discussion on prompt compression in Pattern 26, Optimiz\u2010 \ning Inference, in Chapter 8.) <br><br>Location <br><br>You may need to run your production model on premises for regulatory reasons \nor on the edge for connectivity reasons. If using cloud-hosted foundational \nmodels isn\u2019t an option, you\u2019ll need to use a smaller model (see Pattern 24, Small \nLanguage Models, in Chapter 8) with more limited instruction-following capa\u2010 \nbility. This means your detailed prompts may not work as well on the smaller \nmodel. <br><br>Maintainability <br><br>As your prompts become more detailed, maintenance overhead increases \nbecause the overall system is extremely sensitive to the specific wording of \nprompts. You\u2019ll need to do integration testing every time the foundational model \nchanges, and often even when one of your tools changes. <br><br>In these situations, you might consider post-training a smaller model for your \nspecific task. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aac59e3b-3be8-4fcd-ac8b-0c6de5f424a3": {"__data__": {"id_": "aac59e3b-3be8-4fcd-ac8b-0c6de5f424a3", "embedding": null, "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.81, "x1": 432.0, "y1": 347.58}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5dc6344c-f7bd-4c7e-a2ba-ad698c813263", "node_type": "1", "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 371.63, "x1": 423.75, "y1": 605.63}]}, "hash": "245943b4df46416684783556fc972e03f95963af98f97477868aed84c0c99c34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e099e39b-7a86-4642-83ea-5ca3dc4833b5", "node_type": "1", "metadata": {"bbox": [{"page": 269, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.57, "x1": 432.0, "y1": 285.44}]}, "hash": "268c89e84db893cdf4cf67df740caf9564fc598368dec5beecbe25849ffd14b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Dataset preparation**<br><br>Suppose you have a training dataset consisting of radiology images and the sorts of \ncaptions that you\u2019d like. You can use Adapter Tuning to teach a small open source \nmodel how to properly caption radiology images. <br><br>Modern GenAI models take messages as input. This allows for the inputs to be \nmultiturn and multimodal. In our case, each training example will consist of three \nmessages: <br><br>1. \n1. A system message telling the model to act like an expert researcher <br><br>2. \n2. A user prompt that consists of two parts: <br><br>a. \na. A part that instructs the model to write a caption for the image <br><br>b. \nb. A part that sends the image to caption <br><br>3. \n3. The assistant response consisting of the desired caption <br><br>Putting these together, here\u2019s one of the training examples. (The numbers refer to the \nmessages we just described.): <br><br>{'messages': [ \n {'role': 'system', \n 'content': [{'type': 'text', \n 'text': 'You are an expert researcher in radiology.'}]},***# 1***", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e099e39b-7a86-4642-83ea-5ca3dc4833b5": {"__data__": {"id_": "e099e39b-7a86-4642-83ea-5ca3dc4833b5", "embedding": null, "metadata": {"bbox": [{"page": 269, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.57, "x1": 432.0, "y1": 285.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "aac59e3b-3be8-4fcd-ac8b-0c6de5f424a3", "node_type": "1", "metadata": {"bbox": [{"page": 268, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.81, "x1": 432.0, "y1": 347.58}]}, "hash": "727a302dd8162152475b04f8fd5e3fab1fd3a55a94fecdf5ec4782074bd6e70f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68f74068-b909-4195-9449-28ef14636f5c", "node_type": "1", "metadata": {"bbox": [{"page": 270, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.68, "x1": 432.0, "y1": 247.94}]}, "hash": "e05295e3b742e4219f40b8302b37b70b54aeecce2901c0686852514c84e7236a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We fine-tuned the model on 500 image-text pairs, with a batch size of 4. The training \nloss started out at 14.8 and started dropping, it reached the 4.0 level at around the \n95th batch, and it hovered there for the last 30 batches. This indicates that five \nhundred images were sufficient for our problem. Had the loss kept dropping, we \nwould have tried to use more images. <br><br>We trained on only one epoch, so overfitting was not a concern for us, but if you \nrepeatedly iterate over the training data over multiple epochs, you should evaluate on \na validation set to catch overfitting. <br><br>**Inference**<br><br>On a machine with a GPU (we used an L4 GPU with 32 GB of RAM), you read in the \nsaved adapter model and tokenizer by using the following code: <br><br>SFT_OUTDIR=\"gemma-radiology\"*# output dir in 3_adapter_tuning.ipynb*<br><br>*# Load Model with PEFT adapter (not the merged model)*\nmodel**=**AutoModelForImageTextToText**.**from_pretrained( \n SFT_OUTDIR, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68f74068-b909-4195-9449-28ef14636f5c": {"__data__": {"id_": "68f74068-b909-4195-9449-28ef14636f5c", "embedding": null, "metadata": {"bbox": [{"page": 270, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.68, "x1": 432.0, "y1": 247.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e099e39b-7a86-4642-83ea-5ca3dc4833b5", "node_type": "1", "metadata": {"bbox": [{"page": 269, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.57, "x1": 432.0, "y1": 285.44}]}, "hash": "268c89e84db893cdf4cf67df740caf9564fc598368dec5beecbe25849ffd14b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2046c89-be1f-473b-b7d8-00cc75a02b6e", "node_type": "1", "metadata": {"bbox": [{"page": 272, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 596.45}]}, "hash": "0938cf9ab85349ce48d21c00978af37fe0d9bb78ae3f8f4e8f4229ec0f3cca90", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that there is no assistant message\u2014the assistant message is the output the model \nwill generate. <br><br>Doing this on the test image shown in Figure 5-12 (an image not in the training \ndataset) produced the following caption: <br><br>CT scan of the abdomen showing the size and density of the intra-abdominal mass. <br><br>This caption is concise, and it focuses on the anatomical details depicted in the image \n\u2014exactly as you\u2019d expect, based on the dataset of captions on which we fine-tuned the \nmodel. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2046c89-be1f-473b-b7d8-00cc75a02b6e": {"__data__": {"id_": "f2046c89-be1f-473b-b7d8-00cc75a02b6e", "embedding": null, "metadata": {"bbox": [{"page": 272, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 596.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "68f74068-b909-4195-9449-28ef14636f5c", "node_type": "1", "metadata": {"bbox": [{"page": 270, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.68, "x1": 432.0, "y1": 247.94}]}, "hash": "e05295e3b742e4219f40b8302b37b70b54aeecce2901c0686852514c84e7236a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61fbcc4a-2ff3-4434-a2b1-def4e50448c6", "node_type": "1", "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.94, "x1": 432.01, "y1": 605.54}]}, "hash": "afbcb184505230837f342a49bede87dda61b56cb2e4718b207b60c016b2cd286", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "processor = AutoProcessor.from_pretrained(args.output_dir) \nprocessor.save_pretrained(\"merged_model\") <br><br>**Closed-weights models.**We illustrated fine-tuning on an open-weights model. Provid\u2010 \ners of proprietary models offer fully managed services that will fine-tune their models \nand deploy the fine-tuned models to an endpoint. All you need to do is point the \nfine-tuning service to a training dataset in the format that the service specifies. <br><br>In Chapter 1, we demonstrated fine-tuning on OpenAI, so here, we\u2019ll demonstrate \nfine-tuning on Vertex AI. In Vertex AI, the dataset needs to be organized as messages, \nin the same format we used for the open source model fine-tuning in the \u201cExample\u201d \nsubsection of this pattern section. You can launch a fine-tuning job by using a REST \nAPI call, by using a Python API, or directly from the Google Cloud web console. <br><br>On the web console, you can set up the fine-tuning job to create a tuned model, as \nshown in Figure 5-13. You specify the base model and the number of epochs. For \nsimplicity, the learning rate and adapters are configured in terms of multipliers\u2014in \nthe figure, we use the recommended learning rate and train an adapter that\u2019s twice the \nsize of what\u2019s recommended. You\u2019d typically use larger adapters only if you have more \ntraining examples. <br><br><br><br>Figure 5-13. Fine-tuning on Vertex AI <br><br>**Pattern 15: Adapter Tuning** **|** **255**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61fbcc4a-2ff3-4434-a2b1-def4e50448c6": {"__data__": {"id_": "61fbcc4a-2ff3-4434-a2b1-def4e50448c6", "embedding": null, "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.94, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f2046c89-be1f-473b-b7d8-00cc75a02b6e", "node_type": "1", "metadata": {"bbox": [{"page": 272, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 596.45}]}, "hash": "0938cf9ab85349ce48d21c00978af37fe0d9bb78ae3f8f4e8f4229ec0f3cca90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7ea0ea-a670-46dd-bace-1f24075e9cf5", "node_type": "1", "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 117.94, "x1": 432.01, "y1": 296.37}]}, "hash": "5fa6a899b1d7fc571945f0511b26dbd45382227c6e0a85bd7c2afa21d49712fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once training is finished, the tuned model will be available from an endpoint, and \nyou\u2019ll be able to use it in lieu of the base model to perform the specialized task. <br><br>**Alternatives**<br><br>All the considerations of fine-tuning covered in Chapter 1 apply here\u2014so make \nsure that the benefits you derive outweigh the additional complexity that fine-tuning \nintroduces. If you\u2019re considering using Adapter Tuning, also consider whether few- \nshot learning, Chain of Thought (Pattern 13), or Content Optimization (Pattern 5) \nwould better fit your needs. <br><br>Few-shot learning often suffices if the task is straightforward. A few in-context \nexamples may be enough for the model to mimic what\u2019s required, but this pattern\u2019s \ndata requirements are much lower. Even though the cost of a query might be higher \nbecause of the increased number of tokens, you don\u2019t have to maintain or manage \nmodel versions. <br><br>The Chain of Thought pattern (from Chapter 5) often suffices if the task requires \nlogical steps that can be demonstrated. As with few-shot learning, the cost might be \nhigher, but not having to train or manage models might more than compensate for \nthis additional expense. <br><br>If you don\u2019t have a ready-made training dataset available to demonstrate good \nresponses for a diverse set of inputs, the Content Optimization pattern (from Chap\u2010 \nter 2), might be a good option. This pattern allows you to bootstrap based on natural \nvariation in LLM responses and learn the type of content that performs the best. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d7ea0ea-a670-46dd-bace-1f24075e9cf5": {"__data__": {"id_": "3d7ea0ea-a670-46dd-bace-1f24075e9cf5", "embedding": null, "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 117.94, "x1": 432.01, "y1": 296.37}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "61fbcc4a-2ff3-4434-a2b1-def4e50448c6", "node_type": "1", "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 307.94, "x1": 432.01, "y1": 605.54}]}, "hash": "afbcb184505230837f342a49bede87dda61b56cb2e4718b207b60c016b2cd286", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f4fa1e0-3c12-4608-a88c-32dde4d9f89a", "node_type": "1", "metadata": {"bbox": [{"page": 274, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 292.01}]}, "hash": "fcc81a5e3c6796034cf41573f96e5e21f6e2d2fe6b52493370f1084d34d035ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nWei et al. (2021) showed crucial improvements in developing assistant models with \nresearch, demonstrating improved zero-shot performance across unseen tasks. Li \nand Liang (2021) showed performance comparable to full fine-tuning on generation \ntasks with a small fraction of trainable parameters. Lester et al. (2021) showed that \nadapter tuning in the prompt embedding layer matches full fine-tuning for billion- \nparameter models while requiring storage of only small task-specific prompts. Xu et \nal. (2023) carried out a review of these and other PeFT methods, comparing them \non both parameter efficiency and memory efficiency. Today, the most commonly \nused Adapter Tuning technique is QLoRA, which was introduced by Dettmers et al. \n(2023). <br><br>Fractional AI used Adapter Tuning to fine-tune GPT-3.5 to perform content modera\u2010 \ntion at Change.org. The training dataset consisted of human moderation decisions. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f4fa1e0-3c12-4608-a88c-32dde4d9f89a": {"__data__": {"id_": "0f4fa1e0-3c12-4608-a88c-32dde4d9f89a", "embedding": null, "metadata": {"bbox": [{"page": 274, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 292.01}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3d7ea0ea-a670-46dd-bace-1f24075e9cf5", "node_type": "1", "metadata": {"bbox": [{"page": 273, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 117.94, "x1": 432.01, "y1": 296.37}]}, "hash": "5fa6a899b1d7fc571945f0511b26dbd45382227c6e0a85bd7c2afa21d49712fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21b52473-5a0b-4639-9f60-744512e66eea", "node_type": "1", "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.28, "x1": 556.5, "y1": 607.89}]}, "hash": "2bccdebcfa9a6213d458e8cea7469a8ce21760d2e4d48381639bf87848669f09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Enterprise versions of foundational models are usually made available under data- \nprivacy policies in which the model provider agrees to not use the prompts and other \ndata provided to the model for training or improving the model. For example, Azure \nOpenAI makes these promises (among others): <br><br>Your prompts (inputs) and completions (outputs), your embeddings, and your training \ndata: <br><br>\u2022 \n\u2022 are NOT available to other customers. <br><br>\u2022 \n\u2022 are NOT available to OpenAI. <br><br>\u2022 \n\u2022 are NOT used to improve OpenAI models. <br><br>\u2022 \n\u2022 are NOT used to train, retrain, or improve Azure OpenAI Service foundation <br><br>models. <br><br>Similar policies exist at Anthropic and Gemini when used as part of enterprise agree\u2010 \nments. Many enterprises also use foundational models in air-gapped on-premises \nenvironments, like Google Distributed Cloud and Azure Government. All this means ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21b52473-5a0b-4639-9f60-744512e66eea": {"__data__": {"id_": "21b52473-5a0b-4639-9f60-744512e66eea", "embedding": null, "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.28, "x1": 556.5, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0f4fa1e0-3c12-4608-a88c-32dde4d9f89a", "node_type": "1", "metadata": {"bbox": [{"page": 274, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.59, "x1": 432.0, "y1": 292.01}]}, "hash": "fcc81a5e3c6796034cf41573f96e5e21f6e2d2fe6b52493370f1084d34d035ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3abc1999-a0e9-4f88-83d1-065b75b6c629", "node_type": "1", "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.48, "x1": 637.25, "y1": 348.46}]}, "hash": "22b95fafb012b5be667263810a4fb2cd3a403b8290b6514dd8563bab015c1e20", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 4: Instruction Tuning**\nModels are trained to follow instructions by performing SFT. They are shown dem\u2010 \nonstration examples, which consist of inputs (the prompt and any necessary context) \nand outputs (the ideal response of the model). To teach the model to carry out a new \ntype of task, you\u2019ll create a dataset of such input-output pairs and do SFT. <br><br>**SFT on open-weights models**<br><br>Hugging Face\u2019s Transformers library enables you to do SFT on open-weights models \nsuch as Llama. An example in a dataset used for SFT might look like this: <br><br>{ \n \"instruction\": \"Explain the concept of supervised learning.\", \n \"response\": \"Supervised learning is a machine learning paradigm where models learn from labeled examples...\" \n} <br><br>To do SFT, load in the model that you are tuning and its tokenizer. For example, in \nHugging Face, to load Llama-3, you would do the following: <br><br>model_name = \"meta-llama/Llama-3-8b-hf\" \nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n torch_dtype=torch.bfloat16) \ntokenizer = AutoTokenizer.from_pretrained(model_name) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3abc1999-a0e9-4f88-83d1-065b75b6c629": {"__data__": {"id_": "3abc1999-a0e9-4f88-83d1-065b75b6c629", "embedding": null, "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.48, "x1": 637.25, "y1": 348.46}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "21b52473-5a0b-4639-9f60-744512e66eea", "node_type": "1", "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.28, "x1": 556.5, "y1": 607.89}]}, "hash": "2bccdebcfa9a6213d458e8cea7469a8ce21760d2e4d48381639bf87848669f09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "405af04b-ca6e-48fc-b55b-8af18842885e", "node_type": "1", "metadata": {"bbox": [{"page": 278, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.28, "x1": 432.01, "y1": 337.64}]}, "hash": "b0baafffc7366ef7daf427614024815b9c5bef804e0cc3094d5826a64816c754", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, convert your dataset into the format that the model needs. A common require\u2010 \nment for models is that the training input must be a single text string that ends with a \nspecial token that tells the model to stop generation: <br><br>**def**format_and_tokenize(example): \n**return**tokenizer(f\"### Instruction:**\\n**{example['instruction']}**\\n\\n**### Response:**\\n**{example['response']}\" + tokenizer.eos_token) <br><br>tokenized_dataset = dataset.map(format_and_tokenize) <br><br>Then, set up a trainer from the Transformers library, train the model, and save it for \ninference: <br><br>training_args = TrainingArguments( \n output_dir=\"./trained\", \n ... \n learning_rate=2e-5, \n num_train_epochs=3, \n ... \n) \ntrainer = Trainer( \n model=model, \n args=training_args, \n train_dataset=tokenized_dataset['train'], \n eval_dataset=tokenized_dataset['valid'] \n) \ntrainer.train() \ntrainer.save_model() ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "405af04b-ca6e-48fc-b55b-8af18842885e": {"__data__": {"id_": "405af04b-ca6e-48fc-b55b-8af18842885e", "embedding": null, "metadata": {"bbox": [{"page": 278, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.28, "x1": 432.01, "y1": 337.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3abc1999-a0e9-4f88-83d1-065b75b6c629", "node_type": "1", "metadata": {"bbox": [{"page": 277, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.48, "x1": 637.25, "y1": 348.46}]}, "hash": "22b95fafb012b5be667263810a4fb2cd3a403b8290b6514dd8563bab015c1e20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5abe54a2-1121-488f-8432-d5e8a4639a11", "node_type": "1", "metadata": {"bbox": [{"page": 279, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.14, "x1": 432.0, "y1": 291.45}]}, "hash": "8e31f2cac266ccbe431b4cb6ab20556ec92e3fbdb64e26c5a3b5c8d60a16deec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To use this parameter-efficient approach, start with a four-bit quantized model from \nUnsloth (see model_name in the following code): <br><br>**from unsloth import**FastLanguageModel \nmodel, tokenizer = FastLanguageModel.from_pretrained( \n model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\", \n max_seq_length = 2048, \n load_in_4bit =**True**, \n full_finetuning =**False**,*# only a few layers*\n) <br><br>Then, specify the modules to optimize. The key here is that the modules should \ninclude the gate projection (gate_proj), the embedding tokens (embed_tokens), and \nthe attention head (lm_head): <br><br>model = FastLanguageModel.get_peft_model( \n model, \n r = 16, \n target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n \"gate_proj\", \n \"up_proj\", \"down_proj\", \n \"embed_tokens\", \"lm_head\",], \n lora_alpha = 32, \n use_rslora =**True**,*# rank stabilized LoRA*\n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5abe54a2-1121-488f-8432-d5e8a4639a11": {"__data__": {"id_": "5abe54a2-1121-488f-8432-d5e8a4639a11", "embedding": null, "metadata": {"bbox": [{"page": 279, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.14, "x1": 432.0, "y1": 291.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "405af04b-ca6e-48fc-b55b-8af18842885e", "node_type": "1", "metadata": {"bbox": [{"page": 278, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.28, "x1": 432.01, "y1": 337.64}]}, "hash": "b0baafffc7366ef7daf427614024815b9c5bef804e0cc3094d5826a64816c754", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79f4d535-870d-4f3e-b884-4d1e73519918", "node_type": "1", "metadata": {"bbox": [{"page": 280, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.73, "x1": 432.0, "y1": 269.89}]}, "hash": "e14020cfbccf524f305c0544b42bcf6a2758827d19a335a30e5417b4ee04378e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use it by sending it a prompt that\u2019s similar to what you used for fine-tuning: <br><br>**from transformers import**TextStreamer \nFastLanguageModel.for_inference(model)*# Enable native 2x faster inference*<br><br>inputs = tokenizer( \n [ \n f\"### Instruction: ...**\\n\\n**### Response:**\\n**\", \n ], \n return_tensors=\"pt\", \n).to(\"cuda\") \nresult = model.generate(**inputs, max_new_tokens = 128) <br><br>**Preparing an Instruction-Tuning Dataset**\nInstruction tuning requires a much larger dataset than the one for the Adapter \nTuning pattern. While Adapter Tuning can be done with hundreds of examples, \ninstruction tuning typically requires thousands of examples. <br><br>How can you create a large dataset efficiently? The Evol-Instruct approach is to start \nfrom an initial dataset of instructions and evolve the instructions in that dataset to ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79f4d535-870d-4f3e-b884-4d1e73519918": {"__data__": {"id_": "79f4d535-870d-4f3e-b884-4d1e73519918", "embedding": null, "metadata": {"bbox": [{"page": 280, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.73, "x1": 432.0, "y1": 269.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5abe54a2-1121-488f-8432-d5e8a4639a11", "node_type": "1", "metadata": {"bbox": [{"page": 279, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.14, "x1": 432.0, "y1": 291.45}]}, "hash": "8e31f2cac266ccbe431b4cb6ab20556ec92e3fbdb64e26c5a3b5c8d60a16deec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98a84edb-f88d-40d2-8713-9282aed6c882", "node_type": "1", "metadata": {"bbox": [{"page": 281, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.84, "x1": 432.01, "y1": 300.34}]}, "hash": "005af5274afd27eca15c91d0ec9e65749abc5a4122123c691c5cbe14281fd066", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Replace a commonly used requirement in the programming task with a less <br><br>common and more specific one. <br><br>They next made the problem more complex: <br><br>Provide a piece of erroneous code as a reference to increase <br><br>misdirection. <br><br>With the evolution process, you can use a foundational model to expand the set of \ninstructions in your dataset. Given a single instruction, you can quite easily (as you \nwill see in the \u201cExample\u201d section) generate 10 times as many instructions. <br><br>**Step 2: Generate answers**<br><br>To do instruction training, instructions are not enough. You need instructions and \ncorrect responses to those instructions too. So, where will you find correct responses \nto these made-up questions? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98a84edb-f88d-40d2-8713-9282aed6c882": {"__data__": {"id_": "98a84edb-f88d-40d2-8713-9282aed6c882", "embedding": null, "metadata": {"bbox": [{"page": 281, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.84, "x1": 432.01, "y1": 300.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "79f4d535-870d-4f3e-b884-4d1e73519918", "node_type": "1", "metadata": {"bbox": [{"page": 280, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 80.73, "x1": 432.0, "y1": 269.89}]}, "hash": "e14020cfbccf524f305c0544b42bcf6a2758827d19a335a30e5417b4ee04378e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2e2869e-0646-4451-83cd-af5d240fe323", "node_type": "1", "metadata": {"bbox": [{"page": 282, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 98.34, "x1": 432.01, "y1": 375.8}]}, "hash": "0ad14f633fb25fd49951a3f680d3ec30ac337381fd4d77aa24e59ddcee1747d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG <br><br>Sometimes, you can generate the answer by employing a RAG approach (see \nChapter 3 and Chapter 4) on data in enterprise databases. But if you have a RAG \nsystem that\u2019s capable of answering the question, why would you train a model \nto do that task? If, for some reason, the RAG approach is not feasible during \ninference, you can use instruction tuning to essentially \u201cstore\u201d that knowledge \ndirectly in the model. This may be the case if the model needs to run on the edge \nor in an environment where the data is inaccessible. <br><br>Teacher-student training <br><br>You could have a powerful model, perhaps a \u201cthinking\u201d model, generate the \nanswers and then fine-tune a more cost-effective, low-latency model to do the \nsame task. This is akin to distillation (see Pattern 24, Small Language Models in \nChapter 8), although in distillation, you typically try to replicate all the capabili\u2010 \nties of the teacher model. <br><br>At this point, you have a large dataset of instructions (evolved from a small dataset of \ninstructions) and corresponding answers. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2e2869e-0646-4451-83cd-af5d240fe323": {"__data__": {"id_": "c2e2869e-0646-4451-83cd-af5d240fe323", "embedding": null, "metadata": {"bbox": [{"page": 282, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 98.34, "x1": 432.01, "y1": 375.8}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98a84edb-f88d-40d2-8713-9282aed6c882", "node_type": "1", "metadata": {"bbox": [{"page": 281, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.84, "x1": 432.01, "y1": 300.34}]}, "hash": "005af5274afd27eca15c91d0ec9e65749abc5a4122123c691c5cbe14281fd066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5f51c0-2503-47a5-9fa2-7c990f83ecaf", "node_type": "1", "metadata": {"bbox": [{"page": 283, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.97, "x1": 432.0, "y1": 291.68}]}, "hash": "7ed08006d59ddbe5fdc0f680604830e5ae8c60369bd9d14f685cc64c88e9edc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, let\u2019s look at an end-to-end example of Evol-Instruct. <br><br>**Example**\nIn reality, the reason you\u2019d want to perform instruction tuning is that you want your \nLLM to be highly skilled at performing tasks in a way that is tailored to your industry, \nyour customers, your supply chain, your talent market, and your competition. For \nthis, you\u2019d train the LLM on confidential data, business constraints, and threats. This \nis a book, though, and we want you to be able to replicate our work, so we\u2019ll have to \nuse public data. <br><br>**The Goal: Business Strategy**\nTo illustrate Evol-Instruct, we\u2019ll teach an LLM to function as a business strategy \nconsultant who\u2019s knowledgeable about large American companies that are part of the \nS&P 500 index. Obviously, because we\u2019re going to use public data, the frontier models \nare probably already quite good at this. <br><br>First, we tried asking Claude a strategy question: <br><br>Assuming a major competitor aggressively expands its wealth management services \ntargeting ultra-high-net-worth individuals, what potential strategic advantages and \ndisadvantages might Morgan Stanley have and how should they adapt their client \nacquisition and retention strategies? Answer in 2-3 sentences. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e5f51c0-2503-47a5-9fa2-7c990f83ecaf": {"__data__": {"id_": "6e5f51c0-2503-47a5-9fa2-7c990f83ecaf", "embedding": null, "metadata": {"bbox": [{"page": 283, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.97, "x1": 432.0, "y1": 291.68}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c2e2869e-0646-4451-83cd-af5d240fe323", "node_type": "1", "metadata": {"bbox": [{"page": 282, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 98.34, "x1": 432.01, "y1": 375.8}]}, "hash": "0ad14f633fb25fd49951a3f680d3ec30ac337381fd4d77aa24e59ddcee1747d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1c08777-5fad-4bf8-8452-00565091df05", "node_type": "1", "metadata": {"bbox": [{"page": 284, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.92, "x1": 432.01, "y1": 302.5}]}, "hash": "c626f97e5487d3b3d6b6b6f8c63be02e0fb49b704e50d2926c2f927ead3f2eaa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is not as good. It\u2019s just words strung together with no real insight into either \nthe firm or the market. If you doubt us, replace Morgan Stanley in the answer with \nMcDonald\u2019s and wealth management services with hamburgers. You might find that the \nanswer mostly applies! That\u2019s the mark of a generic answer. <br><br>Our goal is to teach Gemma 3-1b to function as a business analyst and produce \nanswers on par with Claude Sonnet\u2019s. We\u2019ll use the Evol-Instruct pattern. <br><br>**Bootstrapping Initial Questions**\nIn a typical enterprise setting, you\u2019ll already have a few business strategy demos and \ndocuments to start from. Here, we\u2019ll start from companies\u2019 annual filings with the \nSEC and use them to bootstrap an initial set of strategy questions and answers. <br><br>These filings are available in an online database called the SEC\u2019s Electronic Data \nGathering, Analysis, and Retrieval (EDGAR) system. The specific part we are interes\u2010 \nted in is item_7, which is the management discussion. This is where executives of \neach large US company discuss their company\u2019s financial condition and the results \nof operations and also provide a business overview and outlook. The data wrangling ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1c08777-5fad-4bf8-8452-00565091df05": {"__data__": {"id_": "d1c08777-5fad-4bf8-8452-00565091df05", "embedding": null, "metadata": {"bbox": [{"page": 284, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.92, "x1": 432.01, "y1": 302.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6e5f51c0-2503-47a5-9fa2-7c990f83ecaf", "node_type": "1", "metadata": {"bbox": [{"page": 283, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.97, "x1": 432.0, "y1": 291.68}]}, "hash": "7ed08006d59ddbe5fdc0f680604830e5ae8c60369bd9d14f685cc64c88e9edc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a119085-03e4-45e2-90e4-7f29b1f32c15", "node_type": "1", "metadata": {"bbox": [{"page": 285, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.02, "x1": 432.0, "y1": 208.19}]}, "hash": "6fe8ea33ae13fc827d95e23e81a667d5d6b38ffd371b256de011c03d2208035c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example question: How might Google\u2019s (GOOG) reorganization of its hardware divi\u2010 \nsions affect its ability to grow Pixel phones\u2019 market share in 2023?\u201d <br><br>In this prompt, we ask the model to take on the role of a business school professor \nsetting exam questions based on their understanding of the management discussion. \nWe also explain what makes a good question and provide an example (this is few-shot \nlearning, which we discussed in Chapter 1). <br><br>The result includes this question for a chemical company called Air Products and \nwhose stock symbol is APD: <br><br>Air Products (APD) is investing heavily in gasification, carbon capture, and hydrogen \nprojects. How might the cyclical nature of the energy market impact the long-term \nprofitability and strategic viability of these capital-intensive projects, particularly given \nthe company\u2019s reliance on long-term contracts and customer relationships as of 2021? <br><br>This is a good question! I have no idea what the answer is, but it\u2019s great that the \nmanagers of Air Products have thought about it and have written down their answer \nin their SEC filing. <br><br>The resulting questions (three, in our case) will serve as the seed for Evol-Instruct. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a119085-03e4-45e2-90e4-7f29b1f32c15": {"__data__": {"id_": "4a119085-03e4-45e2-90e4-7f29b1f32c15", "embedding": null, "metadata": {"bbox": [{"page": 285, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.02, "x1": 432.0, "y1": 208.19}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d1c08777-5fad-4bf8-8452-00565091df05", "node_type": "1", "metadata": {"bbox": [{"page": 284, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.92, "x1": 432.01, "y1": 302.5}]}, "hash": "c626f97e5487d3b3d6b6b6f8c63be02e0fb49b704e50d2926c2f927ead3f2eaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fed134e9-f268-433e-8ba2-c9c06ff67c01", "node_type": "1", "metadata": {"bbox": [{"page": 286, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.59, "x1": 414.01, "y1": 344.89}]}, "hash": "dfd7d8e6db1b3f1e396e695ef6435f0ee727ec180f3451547c704554d164e403", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To make a question require more reasoning, combine questions: <br><br>Combine two of the questions so that both questions have to be answered implicitly in \norder to answer the given question. <br><br>Here is one of the new questions generated for Air Products. This is more concrete; it \nasks for specific ways profitability could be undermined: <br><br>Air Products (APD) is investing heavily in gasification, carbon capture, and hydrogen \nprojects. What are three specific ways the cyclical nature of the energy market could \nundermine the profitability of these projects, considering their reliance on long-term \ncontracts as of 2021? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fed134e9-f268-433e-8ba2-c9c06ff67c01": {"__data__": {"id_": "fed134e9-f268-433e-8ba2-c9c06ff67c01", "embedding": null, "metadata": {"bbox": [{"page": 286, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.59, "x1": 414.01, "y1": 344.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4a119085-03e4-45e2-90e4-7f29b1f32c15", "node_type": "1", "metadata": {"bbox": [{"page": 285, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 83.02, "x1": 432.0, "y1": 208.19}]}, "hash": "6fe8ea33ae13fc827d95e23e81a667d5d6b38ffd371b256de011c03d2208035c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31d4f3b6-d791-49cb-aff1-2bc4e875c24a", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 336.13, "x1": 1589.25, "y1": 375.23}]}, "hash": "4dc268e6547e9005dc44c40b4da72bb6d868dc8ec4b72d644125c4e6962f4a55", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You are a top student in a highly-ranked MBA program. <br><br>You are given an SEC filing from {filing['company']} made on {filing['filing_date']} <br><br>Use that filing to answer the following questions, but if some information is not in \nthe filing, answer based on your general market insights and knowledge of business \nstrategy. <br><br>Do not refuse to answer as that will give you zero points on the exam. <br><br>Each answer should be 2-3 sentences. <br><br>Here\u2019s an example of a question-answer pair for Air Products:3 <br><br>Q: Air Products (APD) reorganized its industrial gases segments effective October 1, \n2021. How could this reorganization affect APD\u2019s ability to respond to regional market \ndifferences, and what are the potential benefits and risks of this change in structure \nregarding operational efficiency and strategic focus? <br><br>A: The reorganization of APD\u2019s industrial gases segments could both help and hurt the \ncompany\u2019s regional market response. By specializing, APD may be able to address indi\u2010 \nvidual needs better. However, this could also create inefficiencies and slow decision- \nmaking, particularly if there is a need for cross-regional coordination, and APD will \nneed to balance focus with agility. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31d4f3b6-d791-49cb-aff1-2bc4e875c24a": {"__data__": {"id_": "31d4f3b6-d791-49cb-aff1-2bc4e875c24a", "embedding": null, "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 336.13, "x1": 1589.25, "y1": 375.23}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fed134e9-f268-433e-8ba2-c9c06ff67c01", "node_type": "1", "metadata": {"bbox": [{"page": 286, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 125.59, "x1": 414.01, "y1": 344.89}]}, "hash": "dfd7d8e6db1b3f1e396e695ef6435f0ee727ec180f3451547c704554d164e403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b85b82f2-6e83-436a-b41e-7e745f1ba34f", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 155.61, "x1": 1712.5, "y1": 327.31}]}, "hash": "cd0828a02c02ad051717ffc24c323a9141e90f2279d70922a1ac6e4a72ff02d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "{'ques**t**io**n**': \"Why might O'Reilly Automotive's 'good,' 'better,' 'best' product assortment not maximize profitability?\", \n 'a**ns**wer': 'The \"good, better, best\" produc**t**assor**t**me**nt**s**trate**gy may**n**o**t**maximize pro**f**i**ta**bili**t**y i**f t**he cos**t**o**f**ma**na**gi**n**g**t**he complexi**t**y o**f**such a wide ra**n**ge o**f**produc**ts**is**t**oo high. For example, i**t**requires more complex i**n**ve**nt**ory ma**na**geme**nt**, more skilled sales s**taff**, a**n**d more**fl**oor space, which may reduce pro**f**i**ta**bili**t**y.', \n 'score': 5, \n 'expla**nat**io**n**': 'This ques**t**io**n**delves i**nt**o**t**he po**tent**ial dow**ns**ides o**f**a commo**n**re**ta**il s**trate**gy, explori**n**g**t**he bala**n**ce be**t**wee**n**o**ffer**i**n**g cus**t**omer choice a**n**d ma**na**gi**n**g opera**t**io**nal**complexi**t**y. The a**ns**wer provides a co**n**cre**te**expla**nat**io**n**, maki**n**g i**t**i**ns**igh**tful f**or readers i**ntereste**d i**n**busi**ness**s**trate**gy a**n**d re**ta**il ma**na**geme**nt**. Thus, i**t**deserves a high score.'} ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b85b82f2-6e83-436a-b41e-7e745f1ba34f": {"__data__": {"id_": "b85b82f2-6e83-436a-b41e-7e745f1ba34f", "embedding": null, "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 155.61, "x1": 1712.5, "y1": 327.31}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "31d4f3b6-d791-49cb-aff1-2bc4e875c24a", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 336.13, "x1": 1589.25, "y1": 375.23}]}, "hash": "4dc268e6547e9005dc44c40b4da72bb6d868dc8ec4b72d644125c4e6962f4a55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b29defad-b5ef-4013-9dbe-22fa54f5723a", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.52, "x1": 393.83, "y1": 147.51}]}, "hash": "349d8e57153840168a91fcd56e4c40f26bbf9a0f85eb34b801cd3ff84958ddd8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here\u2019s a lower-scoring example: <br><br>{**\"question\"**: \"With revenue declines expected in Tommy Hilfiger and Calvin Klein International, how can PVH adjust marketing and distribution while balancing direct-to-consumer investments with wholesale partner relationships?\", \n**\"answer\"**: \"PVH can adjust marketing through promotional deals and distribution by streamlining logistics to balance direct sales with wholesale partnerships amid revenue challenges.\", \n**\"score\"**: 2, \n**\"explanation\"**: \"The answer is very general and lacks specific insights into PVH's strategy. It mentions promotional deals and streamlining logistics, which are quite generic approaches. It doesn't delve into the complexities of balancing direct-to-consumer investments with wholesale relationships in the context of declining revenues. Therefore, it's not particularly insightful.\"} <br><br>To ensure a high-quality dataset, we limit our training data to only those examples \nthat receive scores of 4 or 5. We then split the data 90:10, keeping 10% of the data for \nvalidation. This leaves us with about 11,000 training examples. <br><br>**Training Gemma**\nNow that we\u2019ve created a training dataset by using Evol-Instruct, we can train the \nGemma 3 1B model. Again, the full code is on GitHub, but we show a few highlights \nhere. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b29defad-b5ef-4013-9dbe-22fa54f5723a": {"__data__": {"id_": "b29defad-b5ef-4013-9dbe-22fa54f5723a", "embedding": null, "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.52, "x1": 393.83, "y1": 147.51}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b85b82f2-6e83-436a-b41e-7e745f1ba34f", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 155.61, "x1": 1712.5, "y1": 327.31}]}, "hash": "cd0828a02c02ad051717ffc24c323a9141e90f2279d70922a1ac6e4a72ff02d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5e742da-cb52-431b-902a-3c9499254c6f", "node_type": "1", "metadata": {"bbox": [{"page": 288, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.2, "x1": 432.0, "y1": 267.14}]}, "hash": "b3732b690e1f1415d13f715b04579a9aaca34c2604b57b8e42766f10b9bc49a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We load in the Gemma model and convert it into a parameter-efficient form: <br><br>model, tokenizer = FastLanguageModel.from_pretrained( \n model_name = \"unsloth/gemma-3-1b-it-bnb-4bit\", \n max_seq_length = 4096, \n load_in_4bit =**True**) \nmodel = FastLanguageModel.get_peft_model( \n model, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5e742da-cb52-431b-902a-3c9499254c6f": {"__data__": {"id_": "f5e742da-cb52-431b-902a-3c9499254c6f", "embedding": null, "metadata": {"bbox": [{"page": 288, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.2, "x1": 432.0, "y1": 267.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b29defad-b5ef-4013-9dbe-22fa54f5723a", "node_type": "1", "metadata": {"bbox": [{"page": 287, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.52, "x1": 393.83, "y1": 147.51}]}, "hash": "349d8e57153840168a91fcd56e4c40f26bbf9a0f85eb34b801cd3ff84958ddd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79228d4f-13fa-4555-896c-ce4206dea5d5", "node_type": "1", "metadata": {"bbox": [{"page": 289, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.04, "x1": 432.07, "y1": 209.94}]}, "hash": "d0583378e7d754d64c2e8585b3ef8d77450c71b67e336c1a87eb33537c6ca526", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It took us about three hours on an L4 GPU to train the model on about 11,000 \nexamples for three epochs. We then asked the trained Gemma model the same \nquestion about Morgan Stanley\u2019s wealth management, and we got back the following \nresponse: <br><br>Morgan Stanley will gain advantages by leveraging its existing global footprint, rep\u2010 \nutation as a long-term investment firm, and expertise in managing complex finan\u2010 \ncial situations. However, they\u2019ll face significant disadvantages, including potentially \nincreased competition from new entrants, shifting client preferences towards alterna\u2010 \ntive investment vehicles, and regulatory scrutiny. Adapting their client acquisition and \nretention strategies requires a multipronged approach: increasing focus on bespoke, \nrelationship-driven strategies alongside fostering strategic partnerships with comple\u2010 \nmentary wealth management firms and exploring new digital platforms and omni\u2010 \nchannel approaches. <br><br>This is very good. As with Claude\u2019s answer, there\u2019s a clear focus on strengths \nand weaknesses, insight into the firm, and its market. Unlike Claude, it also ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79228d4f-13fa-4555-896c-ce4206dea5d5": {"__data__": {"id_": "79228d4f-13fa-4555-896c-ce4206dea5d5", "embedding": null, "metadata": {"bbox": [{"page": 289, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.04, "x1": 432.07, "y1": 209.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f5e742da-cb52-431b-902a-3c9499254c6f", "node_type": "1", "metadata": {"bbox": [{"page": 288, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.2, "x1": 432.0, "y1": 267.14}]}, "hash": "b3732b690e1f1415d13f715b04579a9aaca34c2604b57b8e42766f10b9bc49a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "103a4c2c-dbd8-43e9-8317-81b986b82e4d", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 332.39, "y0": 138.85, "x1": 427.21, "y1": 249.3}]}, "hash": "710a3617589b173ebeb60c6d66625018b830ca99d2d5cdfb4679c921e37c41c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even with LoRA, instruction tuning can become expensive. There are a lot of calls to \nthe frontier model involved in creating the dataset. Each evolved instruction involves \nat least three LLM calls: the first to evolve the instruction, the second to generate the \nanswer, and the third to evaluate the example. A training dataset of 10,000 examples \ninvolves more than 30,000 calls since some of the examples will not meet the quality \ncheck. In addition, unlike Adapter Tuning, instruction training takes hours on a GPU, \nand its result is a model that\u2019s capable of performing only a narrow subset of tasks. \nThe cost of a production system that relies on many such models will be high. <br><br>Therefore, make sure you truly need instruction tuning before you invest in creating \na training dataset using Evol-Instruct. For simple tasks, a few in-context demonstra\u2010 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "103a4c2c-dbd8-43e9-8317-81b986b82e4d": {"__data__": {"id_": "103a4c2c-dbd8-43e9-8317-81b986b82e4d", "embedding": null, "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 332.39, "y0": 138.85, "x1": 427.21, "y1": 249.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "79228d4f-13fa-4555-896c-ce4206dea5d5", "node_type": "1", "metadata": {"bbox": [{"page": 289, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.04, "x1": 432.07, "y1": 209.94}]}, "hash": "d0583378e7d754d64c2e8585b3ef8d77450c71b67e336c1a87eb33537c6ca526", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6498597a-d236-48d0-aea3-492dd2d4c43b", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 82.6, "x1": 223.61, "y1": 180.25}]}, "hash": "a6a9d841d82ad2b6c272f48b8235c0d35daab8ebd09a7c9beb0e75eda903daf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Usage scenarios**\nComplex mathematical problems, \nlogical deductions, and sequential \nreasoning tasks in which step-by- \nstep thinking is required. <br><br>Complex tasks involving strategic \nthinking, planning, or creative \nwriting that require exploring \nmultiple solution paths. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6498597a-d236-48d0-aea3-492dd2d4c43b": {"__data__": {"id_": "6498597a-d236-48d0-aea3-492dd2d4c43b", "embedding": null, "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 82.6, "x1": 223.61, "y1": 180.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "103a4c2c-dbd8-43e9-8317-81b986b82e4d", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 332.39, "y0": 138.85, "x1": 427.21, "y1": 249.3}]}, "hash": "710a3617589b173ebeb60c6d66625018b830ca99d2d5cdfb4679c921e37c41c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2216e220-f0e1-4903-a543-765bc6d8b176", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.8, "x1": 432.0, "y1": 394.1}]}, "hash": "4856a9ab4e5dd1c88466b48d420caa0fa948d5a6d9dbd6f458adb335c92ae857", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Tree of**\n**Thoughts**\n**(ToT) (Pattern**\n**14)**<br><br>Many strategic or logical tasks \ncannot be solved by following a \nsingle linear reasoning path, so the \nmodel needs to explore multiple \nalternatives. \nFully fine-tuning large \nfoundational models for specialized \ntasks is computationally expensive \nand requires significant data. \n**Adapter**\n**Tuning**\n**(Pattern 15)**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2216e220-f0e1-4903-a543-765bc6d8b176": {"__data__": {"id_": "2216e220-f0e1-4903-a543-765bc6d8b176", "embedding": null, "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.8, "x1": 432.0, "y1": 394.1}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6498597a-d236-48d0-aea3-492dd2d4c43b", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 82.6, "x1": 223.61, "y1": 180.25}]}, "hash": "a6a9d841d82ad2b6c272f48b8235c0d35daab8ebd09a7c9beb0e75eda903daf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28e1be47-aceb-49e2-aee4-c7f364d027bb", "node_type": "1", "metadata": {"bbox": [{"page": 291, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.29, "x1": 432.01, "y1": 604.6}]}, "hash": "ce8fa6ea167a583105567d18eef2b7f17b64f09e0bcee2f055035c99d1918c59", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Summary**\nThis chapter demonstrated how to extend the capabilities of foundational models \nbeyond their pretraining. We introduced you to patterns that can overcome these \nlimits, focusing on Chain of Thought (CoT) reasoning to break down tough prob\u2010 \nlems into steps and Tree of Thoughts (ToT) to explore and evaluate multiple reason\u2010 \ning paths. You also learned about Adapter Tuning for efficient fine-tuning with small \ndatasets and Evol-Instruct for creating instruction-tuning datasets for new, complex \ntasks. These four patterns are summarized in Table 5-1. <br><br>Table 5-1. Patterns for Extending Model Capability <br><br>**Patterns**\n**Chain of**\n**Thought**\n**(CoT) (Pattern**\n**13)** **Problems**\nFoundational models often \nstruggle with multistep reasoning \ntasks and therefore generate \nincorrect or fabricated answers. <br><br>**Solutions**\nCoT prompts the model to \nbreak down complex problems \ninto intermediate reasoning \nsteps before providing the final \nanswer. \nToT treats problem-solving \nas a tree search by \ngenerating multiple reasoning \npaths, evaluating them, and \nbacktracking as needed. \nAdapter Tuning trains small add- \non neural network layers, leaving \nthe original model weights \nfrozen and making it efficient for \nspecialized adaptation.  Adapting models for specific tasks \nlike classification, summarization, \nor specialized chatbots with a \nsmall (100\u201310 k) dataset of \nexamples. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28e1be47-aceb-49e2-aee4-c7f364d027bb": {"__data__": {"id_": "28e1be47-aceb-49e2-aee4-c7f364d027bb", "embedding": null, "metadata": {"bbox": [{"page": 291, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.29, "x1": 432.01, "y1": 604.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2216e220-f0e1-4903-a543-765bc6d8b176", "node_type": "1", "metadata": {"bbox": [{"page": 290, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.8, "x1": 432.0, "y1": 394.1}]}, "hash": "4856a9ab4e5dd1c88466b48d420caa0fa948d5a6d9dbd6f458adb335c92ae857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6d9231a-63d9-441d-8e4a-ddb1de142b76", "node_type": "1", "metadata": {"bbox": [{"page": 292, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.07, "x1": 432.01, "y1": 582.33}]}, "hash": "7983dd2ce256fb370d8393c97a536db549a1d78f10705b0ad9303bc9b3846c89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Patterns**\n**Evol-Instruct**\n**(Pattern 16)**<br><br>**Problems**\nCreating high-quality datasets for \ninstruction-tuning models on new \nand complex enterprise tasks is \ndifficult and time-consuming. \n**Solutions**\nEvol-Instruct efficiently generates \ninstruction-tuning datasets by \nevolving instructions through \nmultiple iterations of LLM- \ngenerated tasks and answers. <br><br>We discussed different techniques within these patterns, such as Zero-shot, Few-shot, \nand Auto-CoT for CoT. and thought generation, path evaluation, beam search, and \nsummary generation for ToT:. Along the way, we highlighted important considera\u2010 \ntions like data gaps, nonsequential logic, implementation complexity, latency, cost, \nand how to choose between alternatives. This chapter also showed you Adapter \nTuning\u2019s architecture, training, and inference, emphasizing its efficiency and limita\u2010 \ntions. Overall, this chapter gave you strategies to teach LLMs new tasks and improve \ntheir reasoning abilities in specialized areas they weren\u2019t originally trained for. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6d9231a-63d9-441d-8e4a-ddb1de142b76": {"__data__": {"id_": "f6d9231a-63d9-441d-8e4a-ddb1de142b76", "embedding": null, "metadata": {"bbox": [{"page": 292, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.07, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "28e1be47-aceb-49e2-aee4-c7f364d027bb", "node_type": "1", "metadata": {"bbox": [{"page": 291, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.29, "x1": 432.01, "y1": 604.6}]}, "hash": "ce8fa6ea167a583105567d18eef2b7f17b64f09e0bcee2f055035c99d1918c59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "767123c4-0bc0-405f-84e6-2931d7010b50", "node_type": "1", "metadata": {"bbox": [{"page": 293, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.85, "x1": 432.01, "y1": 351.35}]}, "hash": "b1982933162473b06ff29467c4f6885e8751b820cf9eadb86ef5b3dd3b64afa0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 6**\n**Improving Reliability**<br><br>Foundational models are inherently stochastic, which means they involve random \nvariables or probability. As a result, GenAI applications built on top of these models \ncan suffer from inconsistent output, factual inaccuracies, and hallucinations. This \nchapter introduces four patterns that are designed to mitigate these challenges. <br><br>LLM-as-Judge (Pattern 17) facilitates systematic evaluation of output by employing \nLLMs, either directly via prompting or through ML/fine-tuned models, to assess \nquality. Reflection (Pattern 18) enables models to critique their own work and \niteratively refine generated content. Dependency Injection (Pattern 19) promotes \ntestability and robustness by enabling the mocking of components. Finally, Prompt \nOptimization (Pattern 20) offers a structured approach to refining input prompts and \nthus maximizes output reliability across diverse input distributions. <br><br>Together, these patterns form a framework that enhances the dependability of GenAI \napplications. By integrating mechanisms for evaluation, self-correction, uncertainty \nquantification, modular design, and input optimization, you can build more reliable \nsystems and foster greater user confidence in the outputs generated by your AI \napplications. <br><br>**Pattern 17: LLM-as-Judge**\nThe LLM-as-Judge pattern is a way to provide detailed, multidimensional feedback \nthat you can use to compare models, track improvements, and guide further develop\u2010 \nment. LLM-as-Judge represents a promising middle ground between fully automated \nmetrics and human evaluation, and it offers scalable yet nuanced assessment capabili\u2010 \nties for GenAI systems. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "767123c4-0bc0-405f-84e6-2931d7010b50": {"__data__": {"id_": "767123c4-0bc0-405f-84e6-2931d7010b50", "embedding": null, "metadata": {"bbox": [{"page": 293, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.85, "x1": 432.01, "y1": 351.35}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f6d9231a-63d9-441d-8e4a-ddb1de142b76", "node_type": "1", "metadata": {"bbox": [{"page": 292, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.07, "x1": 432.01, "y1": 582.33}]}, "hash": "7983dd2ce256fb370d8393c97a536db549a1d78f10705b0ad9303bc9b3846c89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81849ef4-779c-4f45-b0fe-7f64e619641c", "node_type": "1", "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 223.58, "x1": 432.01, "y1": 607.79}]}, "hash": "0c6074e2c04c9d0ea2cb1f43829f9e8d0e1e656bcf4af1a59a85ad061c90a263", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outcome measurement <br><br>The gold standard of evaluation in the enterprise is to measure outcomes. Ide\u2010 \nally, you\u2019d be able to tell how good a marketing brochure is by measuring the \nincrease in sales that it drives. However, sales increases can be attributed to a \nhost of factors. Because no two customers or days are alike, it\u2019s hard to say how \nmuch of any change in sales volume was sparked by a single piece of marketing \ncontent. This is why we use outcome proxies, such as engagement\u2014proxies let us \nattribute clicks to specific pieces of content, and we can use approaches such as \nmultiarmed bandits to reduce testing times. <br><br>Human evaluation <br><br>The next-best option is to have human experts score the marketing content. \nThe benefit of this approach is that it can be done without actually deploying \nthe content. Unfortunately, human evaluation is difficult to scale\u2014it\u2019s expensive, \ntime-consuming, subject to individual biases, and limited by the experts\u2019 avail\u2010 \nability. <br><br>Automated metrics <br><br>The NLP literature suggests the use of metrics such as BLEU and ROUGE to \nmeasure content\u2019s fitness for translation and summarization, respectively. The \nadvantage of these metrics is that they\u2019re scores that can be calculated in a \nsystematic way. They work by finding the n-grams that a generated translation \nor summary and a set of reference translations or summaries have in common. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81849ef4-779c-4f45-b0fe-7f64e619641c": {"__data__": {"id_": "81849ef4-779c-4f45-b0fe-7f64e619641c", "embedding": null, "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 223.58, "x1": 432.01, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "767123c4-0bc0-405f-84e6-2931d7010b50", "node_type": "1", "metadata": {"bbox": [{"page": 293, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.85, "x1": 432.01, "y1": 351.35}]}, "hash": "b1982933162473b06ff29467c4f6885e8751b820cf9eadb86ef5b3dd3b64afa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c31e3bac-ac87-48b2-856e-21b9b62bdf99", "node_type": "1", "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 90.79, "x1": 414.0, "y1": 214.21}]}, "hash": "d9f9e92c7ca5028cead6d7e37457933d7f791fda5449919c5b0ea7d006c5d36c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Option 1: Prompting approach**<br><br>LLM-as-Judge involves creating a custom scoring rubric for your problem and then \nasking an LLM to apply the rubric to the given output. For example, you could use \nthis prompt to ask an LLM to evaluate a summary: <br><br>Given an article and a summary, provide a score in the range of 1-5 for each of the \nfollowing criteria: <br><br>- Factual accuracy <br><br>- Completeness of key points <br><br>- Conciseness <br><br>- Clarity <br><br>For each score, provide a brief justification. <br><br>**Article**<br><br>{article} <br><br>**Summary**<br><br>{summary} <br><br>**Scores**: <br><br>You have to guard against the same prompt returning different scores on the same \ninputs on different invocations, so make sure to set the temperature to zero. Doing \nclient-side caching or server-side prompt/context caching can help when it\u2019s abso\u2010 \nlutely essential that scores are repeatable. <br><br>An important step that helps with consistency is to preprocess the input data to make \nit more self-contained. For example, when evaluating conversational answers, it may \nbe important to include enough turns of the conversation and to summarize the \nagent\u2019s answers. <br><br>Another practice that helps with consistency is to expand on the calibration rubric. \nFor example, instead of just saying in your prompt that you want factual accuracy, \nyou can explain what factual accuracy means to you: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c31e3bac-ac87-48b2-856e-21b9b62bdf99": {"__data__": {"id_": "c31e3bac-ac87-48b2-856e-21b9b62bdf99", "embedding": null, "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 90.79, "x1": 414.0, "y1": 214.21}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "81849ef4-779c-4f45-b0fe-7f64e619641c", "node_type": "1", "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 223.58, "x1": 432.01, "y1": 607.79}]}, "hash": "0c6074e2c04c9d0ea2cb1f43829f9e8d0e1e656bcf4af1a59a85ad061c90a263", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8368835a-1c90-482c-a0d4-1f8b08e89abf", "node_type": "1", "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 188.98, "x1": 432.01, "y1": 485.08}]}, "hash": "84f7988c64d796aa015976f60469f81bea86f260cd8f0f37eef81cd26aea48a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Factual accuracy <br><br>*1 if any information in the summary misrepresents the article <br><br>*5 if all statements in the summary are grounded in the article <br><br>- Completeness of key points <br><br>*1 if multiple high-impact points are missing <br><br>*3 if all the major points are present <br><br>*5 if all the major points are present and the more important points receive more \ncoverage in the summary ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8368835a-1c90-482c-a0d4-1f8b08e89abf": {"__data__": {"id_": "8368835a-1c90-482c-a0d4-1f8b08e89abf", "embedding": null, "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 188.98, "x1": 432.01, "y1": 485.08}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c31e3bac-ac87-48b2-856e-21b9b62bdf99", "node_type": "1", "metadata": {"bbox": [{"page": 296, "page_height": 661.5, "page_width": 504.0, "x0": 90.0, "y0": 90.79, "x1": 414.0, "y1": 214.21}]}, "hash": "d9f9e92c7ca5028cead6d7e37457933d7f791fda5449919c5b0ea7d006c5d36c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dda6a4a3-6c99-4d6d-b3d5-0eb53e959339", "node_type": "1", "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.68, "x1": 432.0, "y1": 180.88}]}, "hash": "eb933a2cca6baf4f339d0a7131c7a9f600a34d8d4ea4f6b3e82bbf2c850ffe22", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This example shows a numerical scoring scale, but that\u2019s not the only way to evaluate \nthe response. There are some key questions to consider when choosing an evaluation \napproach: <br><br>What are you evaluating? <br><br>Do you need to score a single piece of content, or do you need the LLM to \nidentify which of two potential choices is better? Sometimes, both choices are \ngenerated content but one is a reference answer (perhaps produced by the cur\u2010 \nrent, non-LLM system). Or instead of just two choices, you might have a list of \ncontent pieces that need to be ranked, with one choice being a reference answer. <br><br>How will you use the scores? <br><br>Do you want the evaluation result to be a binary value, a numerical score, a \nranking, or a categorical value? Does the metric need to align with true outcomes \nor with human preferences? This comes down to how you will use the scores. \nFor example, if you plan to make a decision based on the output of the model, a \nbinary value is best because it will let you clearly tie the score to the correctness \nof the decision. <br><br>How will humans be involved? <br><br>If humans will use the scores, you typically need human interpretability, so you \nshould ask the LLM to generate explanations along with the scores. If humans \nare involved during the evaluation, you could make the evaluation multistage and \nprovide feedback to improve the evaluation. It could even be a conversational \n\u201ccross-examination\u201d in which some (or all) feedback is provided by an LLM. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dda6a4a3-6c99-4d6d-b3d5-0eb53e959339": {"__data__": {"id_": "dda6a4a3-6c99-4d6d-b3d5-0eb53e959339", "embedding": null, "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.68, "x1": 432.0, "y1": 180.88}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8368835a-1c90-482c-a0d4-1f8b08e89abf", "node_type": "1", "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 188.98, "x1": 432.01, "y1": 485.08}]}, "hash": "84f7988c64d796aa015976f60469f81bea86f260cd8f0f37eef81cd26aea48a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e6d2ab7-9aa2-48a1-b92e-d710d6081e22", "node_type": "1", "metadata": {"bbox": [{"page": 298, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.74, "x1": 432.01, "y1": 358.74}]}, "hash": "251dc41ea9568ff7ca5464a946d9caf3b8e6de41ea06ba76cb1afd683068a1df", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Modify your scoring rubric based on these considerations. <br><br>**Option 2: ML**<br><br>Ideally, the score generated by the LLM-as-Judge pattern will be consistent across the \ncontent you want to drive and will indicate the outcome you want. You can achieve \nthis by creating an ML model to combine the scores LLM-as-Judge outputs into a \nsingle score that predicts the outcome. To evaluate marketing content by using this \napproach, you\u2019ll follow these three steps: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e6d2ab7-9aa2-48a1-b92e-d710d6081e22": {"__data__": {"id_": "1e6d2ab7-9aa2-48a1-b92e-d710d6081e22", "embedding": null, "metadata": {"bbox": [{"page": 298, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.74, "x1": 432.01, "y1": 358.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "dda6a4a3-6c99-4d6d-b3d5-0eb53e959339", "node_type": "1", "metadata": {"bbox": [{"page": 297, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.68, "x1": 432.0, "y1": 180.88}]}, "hash": "eb933a2cca6baf4f339d0a7131c7a9f600a34d8d4ea4f6b3e82bbf2c850ffe22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7069000-2263-4ef5-b4bd-2670f7b244af", "node_type": "1", "metadata": {"bbox": [{"page": 299, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.06, "x1": 432.0, "y1": 325.65}]}, "hash": "3aabde4d2c0c25292ef93382fd5b64b6f04cbfd09a8e8e823808530bdda4d254", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One advantage of this approach is that, because you are training on LLM-produced \nscoring rubrics and real outcomes, the ML model will discount any criteria that don\u2019t \nmatter or on which the LLM is inconsistent (as long as you don\u2019t overfit,1 of course). <br><br>Often, you\u2019ll start with the prompting method and then migrate to the ML approach \nonce you have measured some outcomes and can train a model to produce scores \nthat \u201cpredict\u201d the desired outcome. <br><br>**Option 3: Fine-tuning**<br><br>Writing calibration criteria for LLM-as-Judge can be hard. What makes marketing \ncontent \u201cpersuasive,\u201d for example? In such cases, it can help to have human experts \nannotate and score content with the same scoring rubric as the LLM-as-Judge. This \ngives you a set of ideal input-response pairs, and you can use them to carry out \nAdapter Tuning (see Chapter 5) to create a fine-tuned model that\u2019s capable of produc\u2010 \ning scores similar to those produced by human experts. <br><br>A fine-tuning approach like this works in situations like applying medical diagnostics \nchecklists to patient records when you want the LLM to mimic how humans would ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7069000-2263-4ef5-b4bd-2670f7b244af": {"__data__": {"id_": "a7069000-2263-4ef5-b4bd-2670f7b244af", "embedding": null, "metadata": {"bbox": [{"page": 299, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.06, "x1": 432.0, "y1": 325.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1e6d2ab7-9aa2-48a1-b92e-d710d6081e22", "node_type": "1", "metadata": {"bbox": [{"page": 298, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.74, "x1": 432.01, "y1": 358.74}]}, "hash": "251dc41ea9568ff7ca5464a946d9caf3b8e6de41ea06ba76cb1afd683068a1df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb00854a-cfa3-4808-b312-34a371fa9bb7", "node_type": "1", "metadata": {"bbox": [{"page": 300, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.51, "x1": 432.01, "y1": 286.64}]}, "hash": "cd2de8831fc164565c514c6229b6b8f37b526c34dd75728ad75499b56b1eb0ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s say that you want the arguments to be persuasive enough to cause more people \nto vote for or against the initiative, respectively. How do you evaluate the arguments \nagainst these goals? <br><br>**Choosing an evaluation method**<br><br>The following traditional approaches fail in this scenario: <br><br>Outcome measurement <br><br>The outcome is the election result, and you could say that whichever position \ngets the most votes is the better argument. However, it\u2019s hard to separate the \nquality of this argument from other things that affect the outcome, such as \npartisanship, each voter\u2019s prior opinions about gun ownership, and external \nfactors such as recent high-profile events or a high-profile candidate endorsing \none position or the other. <br><br>Human evaluation <br><br>Human evaluation tends to be extremely biased on emotionally charged topics \nlike guns and mental illness. You\u2019d need to have a large focus group, test for \npre-existing biases, and perform statistical corrections based on these biases. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb00854a-cfa3-4808-b312-34a371fa9bb7": {"__data__": {"id_": "cb00854a-cfa3-4808-b312-34a371fa9bb7", "embedding": null, "metadata": {"bbox": [{"page": 300, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.51, "x1": 432.01, "y1": 286.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a7069000-2263-4ef5-b4bd-2670f7b244af", "node_type": "1", "metadata": {"bbox": [{"page": 299, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.06, "x1": 432.0, "y1": 325.65}]}, "hash": "3aabde4d2c0c25292ef93382fd5b64b6f04cbfd09a8e8e823808530bdda4d254", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "439299cc-4c63-48ad-ba1d-9f0f23568c37", "node_type": "1", "metadata": {"bbox": [{"page": 301, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.24, "x1": 432.01, "y1": 251.44}]}, "hash": "afbd606f2fd8b7635e111362a9193fab81703b9b96069b52369983aa0f6a7360", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These criteria can be incorporated into the prompt for LLM-as-judge: <br><br>You will be given an argument for or against a Washington State initiative which will \nappear in the voter pamphlet that is mailed to all households. <br><br>Provide a score in the range 1-5 for each of the following criteria: <br><br>{scoring_criteria} <br><br>For each score, provide a brief justification. <br><br>**Argument**: <br><br>{argument} <br><br>Using this prompt with OpenAI\u2019s GPT-4o-mini on the argument for the initiative \ngave us the results shown (in part) here: <br><br>3.**Understandable: 4**<br><br>- The language is mostly plain, and the sentences are straightforward, which makes the \nargument accessible. However, some readers may still find terms like \u201ccivil protection \norders\u201d somewhat legalistic. Avoiding such phrases or providing brief definitions could \nimprove overall comprehension. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "439299cc-4c63-48ad-ba1d-9f0f23568c37": {"__data__": {"id_": "439299cc-4c63-48ad-ba1d-9f0f23568c37", "embedding": null, "metadata": {"bbox": [{"page": 301, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.24, "x1": 432.01, "y1": 251.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cb00854a-cfa3-4808-b312-34a371fa9bb7", "node_type": "1", "metadata": {"bbox": [{"page": 300, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.51, "x1": 432.01, "y1": 286.64}]}, "hash": "cd2de8831fc164565c514c6229b6b8f37b526c34dd75728ad75499b56b1eb0ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d08a7801-73cf-4689-8f08-b89c88b4e895", "node_type": "1", "metadata": {"bbox": [{"page": 302, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.29, "x1": 432.01, "y1": 318.44}]}, "hash": "0b98462e5f9261fd370a367242a31c686e0845f88ebb0eb1339bc88edc5fba0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multiple evaluations <br><br>Instead of performing the evaluation only once, you could have multiple LLMs \nevaluate the responses. This can be useful if each LLM takes on the role of a \ndifferent stakeholder and assesses how well the LLM\u2019s response aligns with the \nneeds of that stakeholder. This is sometimes termed LLM-as-jury. <br><br>Of course, you can combine these approaches. For example, one way to get nuanced \nscoring while posing binary questions is to combine it with LLM-as-jury. This is \ntermed polling. <br><br>**Leniency**<br><br>LLMs tend to be extremely lenient with scores\u2014like professors who give every \nstudent As and Bs. This, combined with LLMs\u2019 nondeterministic nature, means that \nyou should avoid comparing the output scores for two pieces of content to determine ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d08a7801-73cf-4689-8f08-b89c88b4e895": {"__data__": {"id_": "d08a7801-73cf-4689-8f08-b89c88b4e895", "embedding": null, "metadata": {"bbox": [{"page": 302, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.29, "x1": 432.01, "y1": 318.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "439299cc-4c63-48ad-ba1d-9f0f23568c37", "node_type": "1", "metadata": {"bbox": [{"page": 301, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.24, "x1": 432.01, "y1": 251.44}]}, "hash": "afbd606f2fd8b7635e111362a9193fab81703b9b96069b52369983aa0f6a7360", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1eaa4aa-464b-496f-9d6b-fb2a6a25e673", "node_type": "1", "metadata": {"bbox": [{"page": 303, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.82, "x1": 432.01, "y1": 321.33}]}, "hash": "8a9ddb7965e1f06b36933fc997972d383510fd939f37c8ef4b12ba27100413ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LLMs may also favor lengthy reviews over concise ones, even if both reviews contain \nthe same relevant details. LLMs may miss information that\u2019s located in the middle of \nlengthy text, and they tend to favor information at the beginning or end of an answer. \nThese biases are often referred to as length bias and positional bias respectively. <br><br>Because of the inconsistency and bias issues with using frontier LLMs, it may be \nworth considering fine-tuned small language models (like PandaLM) that have lower \ncosts . You can use models like PatronusAI for specific evaluation tasks like multimo\u2010 \ndal responses, and you can also use them for industry-specific custom metrics. <br><br>**Caveats**<br><br>In our zero-shot prompt, we asked the LLM to provide an explanation of the score: <br><br>For each score, provide a brief justification <br><br>There is some evidence that doing this, while offering interpretability, generally has \na negative impact on the model\u2019s evaluation performance and bias mitigation. It may \nbe that the process of self-explanation introduces deeper biases. If you don\u2019t need \nexplanations, then you can take the probability-weighted mean score of the generated \ndistribution (instead of choosing the most likely score, as would normally be the case ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1eaa4aa-464b-496f-9d6b-fb2a6a25e673": {"__data__": {"id_": "c1eaa4aa-464b-496f-9d6b-fb2a6a25e673", "embedding": null, "metadata": {"bbox": [{"page": 303, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.82, "x1": 432.01, "y1": 321.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d08a7801-73cf-4689-8f08-b89c88b4e895", "node_type": "1", "metadata": {"bbox": [{"page": 302, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.29, "x1": 432.01, "y1": 318.44}]}, "hash": "0b98462e5f9261fd370a367242a31c686e0845f88ebb0eb1339bc88edc5fba0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a99473-860e-49a5-bbd2-f9d5a1b7f481", "node_type": "1", "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 240.34, "x1": 432.0, "y1": 606.65}]}, "hash": "7bf0af86a3cf781206d876d446dbfe053eef1788674b42984dc8831032861d2f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DoorDash implements LLM-as-Judge for LLM-based Dasher support automation, \nand AWS uses it for model evaluation on Amazon Bedrock. Acrolinx uses AI guard\u2010 \nrails that they implement with LLM-as-Judge to maintain brand voice consistency \nacross content. <br><br>**Pattern 18: Reflection**\nThe Reflection pattern is an agentic approach in which an AI system evaluates its own \nreasoning, decisions, and outputs before finalizing them. This self-monitoring pro\u2010 \ncess allows the system to identify errors, improve solutions, and refine its approach to \nproblems. <br><br>Here, the AI system as a whole is reflecting and improving, and the foundational \nmodel used to generate the response remains the same. You\u2019ll sometimes see this \nprocess referred to as self-reflection, but there\u2019s no reason why the evaluation has to be \ndone by an LLM at all, let alone by the same LLM that generates the content. <br><br>**Problem**\nSuppose you are using an LLM-based tool, such as ChatGPT or Perplexity, through \na web interface. If the LLM produces a suboptimal or incorrect response, you can ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2a99473-860e-49a5-bbd2-f9d5a1b7f481": {"__data__": {"id_": "e2a99473-860e-49a5-bbd2-f9d5a1b7f481", "embedding": null, "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 240.34, "x1": 432.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c1eaa4aa-464b-496f-9d6b-fb2a6a25e673", "node_type": "1", "metadata": {"bbox": [{"page": 303, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.82, "x1": 432.01, "y1": 321.33}]}, "hash": "8a9ddb7965e1f06b36933fc997972d383510fd939f37c8ef4b12ba27100413ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a3baf32-1e8d-4b37-8af1-e69df72875c9", "node_type": "1", "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.21, "x1": 432.0, "y1": 225.48}]}, "hash": "6a0b96bdb294169309c81671f7723c1627876d5092739c71840d8d872ff7799a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*# Create a list of LLM messages to send to the model.*\nmessages: List[LLMMessage] = [*self._system_messages] \n**for**m**in**self._session_memory[message.session_id]: \n**if**isinstance(m, CodeReviewResult): \n messages.append(UserMessage(content=m.review, source=\"Reviewer\")) \n**elif**isinstance(m, CodeReviewTask): \n messages.append(AssistantMessage( \n content=m.code_writing_scratchpad, \n source=\"Coder\")) \n**elif**isinstance(m, CodeWritingTask): \n messages.append(UserMessage(content=m.task, source=\"User\")) \n**else**: \n**raise ValueError**(f\"Unexpected message type: {m}\") <br><br>**Example**\nLet\u2019s look at an end-to-end example of using Reflection to design a company logo. \nThe full code is on GitHub. <br><br>**Evaluate and critique**<br><br>The first step is to build a good evaluator. You can use Pattern 17, LLM-as-Judge, and \nprovide a scoring rubric for logos: <br><br>Analyze the following proposed logo for {company}. <br><br>{company_description} <br><br>Score the logo 1-5 on each of the following criteria: <br><br>- It is clear from the logo what the name of the company is. <br><br>- The logo and image are appropriate for what the company does. <br><br>- The logo does not conflict with any well-known brand or competitors. <br><br>- The logo design is streamlined and clean. <br><br>- The logo stands out and is easy to recognize. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a3baf32-1e8d-4b37-8af1-e69df72875c9": {"__data__": {"id_": "2a3baf32-1e8d-4b37-8af1-e69df72875c9", "embedding": null, "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.21, "x1": 432.0, "y1": 225.48}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e2a99473-860e-49a5-bbd2-f9d5a1b7f481", "node_type": "1", "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 240.34, "x1": 432.0, "y1": 606.65}]}, "hash": "7bf0af86a3cf781206d876d446dbfe053eef1788674b42984dc8831032861d2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24af4f21-8a06-418e-a896-3ce61908ad42", "node_type": "1", "metadata": {"bbox": [{"page": 308, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 602.250004}]}, "hash": "ecb0daf14e62e206ed9df94e6e13fff6015de0ddb84adcb16487ff81662679c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Explain your scores. <br><br>Try it by sending the preceding prompt to an LLM (we used Claude Sonnet 3.7) along \nwith the logo of the company Pydantic company (see Figure 6-3), using this code: <br><br>agent = Agent(CLAUDE, \n model_settings = { \n \"temperature\": 0*# for LLM-as-Judge*\n }) \nresult**=**agent**.**run_sync([prompt, ImageUrl(url**=\"...\")**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24af4f21-8a06-418e-a896-3ce61908ad42": {"__data__": {"id_": "24af4f21-8a06-418e-a896-3ce61908ad42", "embedding": null, "metadata": {"bbox": [{"page": 308, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 602.250004}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2a3baf32-1e8d-4b37-8af1-e69df72875c9", "node_type": "1", "metadata": {"bbox": [{"page": 306, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.21, "x1": 432.0, "y1": 225.48}]}, "hash": "6a0b96bdb294169309c81671f7723c1627876d5092739c71840d8d872ff7799a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6df3df07-e780-41db-afc2-e1ec0a185488", "node_type": "1", "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 378.15, "x1": 432.0, "y1": 607.79}]}, "hash": "71e6aa60111e5a9cd3e60d47c6734b4dacc384d0bcc314788b046db35cb43fee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 6-4. Initial logo designed by Gemini for a hypothetical restaurant <br><br>After applying the evaluator to the generated image, we got this, in part: <br><br>No conflicts (4/5): The design appears to be unique and doesn\u2019t immediately evoke \nany major sushi chain logos. The simple nigiri illustration is somewhat generic to the \nindustry, but the overall composition with the bold typography creates a distinctive \nlook. I\u2019m giving it a 4 instead of 5 only because there are many sushi restaurants that \nuse similar nigiri imagery, though this particular execution seems unique. <br><br>You could calculate the overall score and threshold it to determine whether or not to \nmove forward. However, it\u2019s hard to determine an appropriate threshold. For our first \nattempt at a score, we calculated a weighted average: <br><br>**def**score(self) -> float: \n**return**(10*self.clarity + \n 10*self.appropriateness + \n 30*self.no_conflicts + \n 30*self.clean_design + \n 20*self.easy_to_recognize) / 500.0 <br><br>The PydanticAI logo received a score of 0.9, but so did the sushi logo. This is, \nsurprisingly, a common problem\u2014since LLMs tend to be lenient scorers, it\u2019s hard to \ncome up with a rubric that can distinguish between a \u201cgood enough\u201d design and a \ngreat one. <br><br>A simple workaround is to have exactly one round of criticism. Then, no threshold is \nrequired. <br><br>**Pattern 18: Reflection** **|** **291**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6df3df07-e780-41db-afc2-e1ec0a185488": {"__data__": {"id_": "6df3df07-e780-41db-afc2-e1ec0a185488", "embedding": null, "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 378.15, "x1": 432.0, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "24af4f21-8a06-418e-a896-3ce61908ad42", "node_type": "1", "metadata": {"bbox": [{"page": 308, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 602.250004}]}, "hash": "ecb0daf14e62e206ed9df94e6e13fff6015de0ddb84adcb16487ff81662679c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d4c44e6-ebc3-4d81-ada8-519e511e2996", "node_type": "1", "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.69, "x1": 431.99, "y1": 355.56}]}, "hash": "c97a33af3b220a1a1fada4a80ed61307a9205cf57b47b700bbfb8afbecaa226e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Apply Criticism**<br><br>The next step is to apply the feedback to create an improved prompt for the next \niteration. You can use the following prompt to have Claude generate instructions for \nthe logo designer: <br><br>Use the following feedback to generate detailed extra instructions to send back to the \ndesigner of the logo. <br><br>{critique} <br><br>When we did so, the detailed instructions included the following: <br><br>## Suggested Refinements: <br><br>1.**Increase Distinctiveness (Addressing the 4/5 on Uniqueness)**: <br><br>- Consider adding a subtle, unique twist to the nigiri illustration to differentiate it \nfrom other sushi restaurants (perhaps a signature knife mark, a unique shape to \nthe rice, or a distinctive way the fish drapes over the rice) <br><br>- Explore incorporating a small, simple element that represents Hiroshi\u2019s specific \napproach or specialty ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d4c44e6-ebc3-4d81-ada8-519e511e2996": {"__data__": {"id_": "3d4c44e6-ebc3-4d81-ada8-519e511e2996", "embedding": null, "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.69, "x1": 431.99, "y1": 355.56}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6df3df07-e780-41db-afc2-e1ec0a185488", "node_type": "1", "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 378.15, "x1": 432.0, "y1": 607.79}]}, "hash": "71e6aa60111e5a9cd3e60d47c6734b4dacc384d0bcc314788b046db35cb43fee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72172e4-3057-4829-a1cc-47da2571f4e9", "node_type": "1", "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 200.14, "x1": 432.01, "y1": 605.54}]}, "hash": "8a52e6fe2bf54fed05e3dcd2301eae0130ef3c7a6366164ed8769b872d527bdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note the impact of the italicized instruction on the right side of Figure 6-5. <br><br>**Regenerate**<br><br>Finally, send the previously generated logo, along with the changes to make, to the \nlogo designer: <br><br>**def**design_logo(company, \n company_description, \n output_filename, \n previous_logo, \n changes_to_make) -> str: \n client = genai.Client(api_key=os.environ['GEMINI_API_KEY']) <br><br> prompt = f\"\"\" \n Here's a proposed logo image for {company}. \n {company_description} <br><br> Please edit the image and make the following changes. \n Return only the final image after all edits. \n {changes_to_make} \n \"\"\" \n previous_image = Image.open(previous_logo) \n contents = [prompt, previous_image] <br><br> response = client.models.generate_content( \n model=GEMINI, \n contents=contents, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b72172e4-3057-4829-a1cc-47da2571f4e9": {"__data__": {"id_": "b72172e4-3057-4829-a1cc-47da2571f4e9", "embedding": null, "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 200.14, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3d4c44e6-ebc3-4d81-ada8-519e511e2996", "node_type": "1", "metadata": {"bbox": [{"page": 309, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.69, "x1": 431.99, "y1": 355.56}]}, "hash": "c97a33af3b220a1a1fada4a80ed61307a9205cf57b47b700bbfb8afbecaa226e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d55d06a1-bce2-4b5a-828d-391ee5b842cf", "node_type": "1", "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 131.14, "x1": 432.0, "y1": 192.04}]}, "hash": "78874a8440bccd08aea2cb06c85b95f73335f1759e460e4f5d5e2d53f49554c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mitigation of reasoning and planning limitations <br><br>Reflection allows for iterative refinement of reasoning processes. For novel and \nopen-ended tasks, it\u2019s often easier to perform a task once and then determine \nhow to improve it than to plan out the approach beforehand. <br><br>Transparency <br><br>The reflection process creates explicit reasoning traces that improve explainabil\u2010 \nity. <br><br>As AI systems take on more complex and consequential tasks, their ability to reflect \nhas become essential to their reliable performance. <br><br>**Tradeoff: Cost versus quality**<br><br>Reflection involves making multiple LLM inference calls, which adds computational \ncosts and latency. You\u2019ll need to evaluate the potential improvement in response \nquality against these costs before you can decide whether to add more rounds of \nreflection or increase the evaluation threshold. <br><br>At the time of writing, many frontier model API providers are struggling with \navailability. You often need to have a retry strategy just to get the first call to succeed, \nand Reflection, by introducing the need to make multiple calls to accomplish a task, \ncan increase tail latency beyond acceptable limits. <br><br>The cost-versus-quality consideration typically affects different use cases differently. \nIn code generation, using reflection to evaluate the output before submitting the \ncode can reduce syntax errors and logical bugs. The relative cost of doing one more \nround of reflection compared to the cost of a broken build is often significant enough \nthat such systems involve multiple stages of reflection. They may even incorporate \nexternal tools, such as compilers and sandboxes, in the evaluation step. However, \nreflection may add unacceptable latency in applications such as chatbots and real- \ntime game engines. Therefore, you might find it helpful to develop heuristics for \nyour own business that you can use to adjust the reflection depth, based on the \nproblem characteristics, the available time, and the business impact of the expected \nimprovement in quality. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d55d06a1-bce2-4b5a-828d-391ee5b842cf": {"__data__": {"id_": "d55d06a1-bce2-4b5a-828d-391ee5b842cf", "embedding": null, "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 131.14, "x1": 432.0, "y1": 192.04}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b72172e4-3057-4829-a1cc-47da2571f4e9", "node_type": "1", "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 200.14, "x1": 432.01, "y1": 605.54}]}, "hash": "8a52e6fe2bf54fed05e3dcd2301eae0130ef3c7a6366164ed8769b872d527bdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c2ce629-591d-4169-8808-e1c2070ef19b", "node_type": "1", "metadata": {"bbox": [{"page": 312, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.29, "x1": 432.01, "y1": 367.59}]}, "hash": "493dd1ea4616e212d9be27e9608bc942a6c52e022dca80b91d9ff4aacdff77f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As in the logo creation example, you can create multiple drafts, but that doesn\u2019t have \nto wait until the final stage. You could generate multiple drafts even in the first step \nand then follow a beam search\u2013like approach of editing all of them and pruning out \npoor candidates after each step, before making the final choice. Of course, this would \nincrease costs even more. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c2ce629-591d-4169-8808-e1c2070ef19b": {"__data__": {"id_": "0c2ce629-591d-4169-8808-e1c2070ef19b", "embedding": null, "metadata": {"bbox": [{"page": 312, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.29, "x1": 432.01, "y1": 367.59}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d55d06a1-bce2-4b5a-828d-391ee5b842cf", "node_type": "1", "metadata": {"bbox": [{"page": 311, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 131.14, "x1": 432.0, "y1": 192.04}]}, "hash": "78874a8440bccd08aea2cb06c85b95f73335f1759e460e4f5d5e2d53f49554c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4efbee76-ded5-4622-8bde-d1602fe79a68", "node_type": "1", "metadata": {"bbox": [{"page": 313, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.45, "x1": 432.01, "y1": 278.42}]}, "hash": "67374e9704065d20bba0607ff0e21df84e9793d725b6b37b4445da2098a54466", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Amazon uses Reflection to ensure that generated product listings are complete and \ncorrect. The validation LLM also looks for features that negatively impact listing \nperformance. <br><br>**Pattern 19: Dependency Injection**\nThe Dependency Injection pattern makes it easier to independently develop and test \neach component of an LLM chain. When you build chains of LLM calls, build them \nin a way that makes it easy to inject a mock implementation to replace any step of the \nchain. <br><br>**Problem**\nIt can be difficult to develop and test GenAI applications for several reasons: <br><br>They are nondeterministic <br><br>Unlike with most software APIs and statistical models, calls to an LLM with the \nexact same input can return different outputs each time. This makes it difficult to \nverify that the code is correct by supplying synthetic inputs and checking them \nagainst the expected output. <br><br>Models change quickly <br><br>You\u2019re often building on foundational models whose upgrade schedule you \ndon\u2019t know. Sticking to a foundational model as of a certain date can help nail \ndown the version, but you lose out on the benefits of the improvements in ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4efbee76-ded5-4622-8bde-d1602fe79a68": {"__data__": {"id_": "4efbee76-ded5-4622-8bde-d1602fe79a68", "embedding": null, "metadata": {"bbox": [{"page": 313, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.45, "x1": 432.01, "y1": 278.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0c2ce629-591d-4169-8808-e1c2070ef19b", "node_type": "1", "metadata": {"bbox": [{"page": 312, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 71.29, "x1": 432.01, "y1": 367.59}]}, "hash": "493dd1ea4616e212d9be27e9608bc942a6c52e022dca80b91d9ff4aacdff77f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4181ad8-31bd-4eca-8280-0af32859d00e", "node_type": "1", "metadata": {"bbox": [{"page": 314, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 154.46, "x1": 432.0, "y1": 302.08}]}, "hash": "ff3262bcbcd4eca935447d99e08be6a627dbc8c4c9de28da6118aedfeadb018b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 1: Generate a critique**<br><br>In the first step, you send the text of the marketing description and get back a \ncritique. The critique is defined as a structured object (see Pattern 2, Grammar, in \nChapter 2): <br><br>@dataclass \n**class Critique**: \n target_audience: List[str] \n improvements: List[str] <br><br>Next, you call the LLM with an appropriate prompt: <br><br>**def**critique(in_text: str) -> Critique: \n prompt = f\"\"\" \n You are an expert marketer for technology books. \n You will be given the marketing description for a book. \n Identify the target audience by roles (eg: Data Analyst, Data Engineer) \n Suggest exactly 5 ways that the*marketing description*can be improved so \n that it appeals better to this target audience. \n Do not suggest improvements to the book itself. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4181ad8-31bd-4eca-8280-0af32859d00e": {"__data__": {"id_": "b4181ad8-31bd-4eca-8280-0af32859d00e", "embedding": null, "metadata": {"bbox": [{"page": 314, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 154.46, "x1": 432.0, "y1": 302.08}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4efbee76-ded5-4622-8bde-d1602fe79a68", "node_type": "1", "metadata": {"bbox": [{"page": 313, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.45, "x1": 432.01, "y1": 278.42}]}, "hash": "67374e9704065d20bba0607ff0e21df84e9793d725b6b37b4445da2098a54466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f5033c6-1b63-4bc4-b41a-31ba2ea7df74", "node_type": "1", "metadata": {"bbox": [{"page": 315, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.35, "x1": 463.0, "y1": 457.91}]}, "hash": "9e0cf619690ec3660d242bcacd8dd1c3269e45017c83266aa5e93917eacea523", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This looks right, but is it right?2 <br><br>**Testing the first step**<br><br>How can you make sure that the preceding code will continue to work well and that \nit is correct? You do not want to rely on \u201ceyeballing\u201d the result (which is often deroga\u2010 \ntorily referred to as vibe checking). Human feedback\u2014both explicit (such as giving a \nthumbs-up or thumbs-down to the suggestions) and implicit (such as presenting the \npreceding list of suggestions to a user and seeing which one they select)\u2014has its place \nin product design, but you still want your test suite to test the correctness of the code. <br><br>To test the first step, you can perform assertions on the returned response to make \nsure it meets expectations: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f5033c6-1b63-4bc4-b41a-31ba2ea7df74": {"__data__": {"id_": "2f5033c6-1b63-4bc4-b41a-31ba2ea7df74", "embedding": null, "metadata": {"bbox": [{"page": 315, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.35, "x1": 463.0, "y1": 457.91}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b4181ad8-31bd-4eca-8280-0af32859d00e", "node_type": "1", "metadata": {"bbox": [{"page": 314, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 154.46, "x1": 432.0, "y1": 302.08}]}, "hash": "ff3262bcbcd4eca935447d99e08be6a627dbc8c4c9de28da6118aedfeadb018b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9afffe00-31b1-4e7b-a559-e681a5544934", "node_type": "1", "metadata": {"bbox": [{"page": 316, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.28, "x1": 432.01, "y1": 296.18}]}, "hash": "1dd2fde29a40b00244b7e1dac33b72d89a47a08d2fb2d0d51d44ecfc06a37bd0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 2: Implement a suggestion**<br><br>The second step is to implement one of the suggestions. Here, the result is a change \nlog and an improved marketing description: <br><br>@dataclass \n**class Improvement**: \n change: str \n reason: str \n modified_marketing_description: str <br><br>This step also involves an LLM call, but one of the inputs into this function is the \nCritique object that was generated in Step 1: <br><br>**def**improve(marketing_text: str, c: Critique) -> Improvement: \n prompt = f\"\"\" \n You are a helpful marketing assistant. \n You will be given the marketing description for a book, \n its target audience, and a list of suggested changes. <br><br> Pick one change from the list that best meets these criteria: \n - It does not require changing the book itself, only the marketing description. \n - It will make the book much more appealing to the target audience. \n - It requires only 1-5 lines to be changed in the text of the marketing description. \n Then, make the change and return a change log and the modified description. <br><br>**Marketing Description**: \n {marketing_text} <br><br> {c} \n \"\"\" \n print(f\"Invoking LLM to improve text\") \n agent = Agent(GEMINI, \n result_type=Improvement) \n result = agent.run_sync(prompt) \n**return**(result.data) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9afffe00-31b1-4e7b-a559-e681a5544934": {"__data__": {"id_": "9afffe00-31b1-4e7b-a559-e681a5544934", "embedding": null, "metadata": {"bbox": [{"page": 316, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.28, "x1": 432.01, "y1": 296.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2f5033c6-1b63-4bc4-b41a-31ba2ea7df74", "node_type": "1", "metadata": {"bbox": [{"page": 315, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.35, "x1": 463.0, "y1": 457.91}]}, "hash": "9e0cf619690ec3660d242bcacd8dd1c3269e45017c83266aa5e93917eacea523", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f99bbe3-d88b-4cc4-b894-32271d58c31a", "node_type": "1", "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 167.1, "x1": 582.01, "y1": 606.65}]}, "hash": "3ef6d4ded08c59cd8d1b030fe2632ca89bacf390acc3e4c159fbac7d1f01dd9e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this presents a problem: you have to invoke the LLM in Step 1 in order to \nget the Critique object to pass to the second step\u2014so there seems to be no way to \ndevelop and test Step 2 independently of Step 1: <br><br>**def**improvement_chain(a_text): \na_critique = critique(a_text) \nimproved = improve(a_text, a_critique) \nassert_improvement(improved, a_text, a_critique) <br><br>But is there a way to test Step 2 independently of Step 1? <br><br>**Solution**\nThe solution is to replace Step 1 with a mock implementation that returns a hardco\u2010 \nded result while Step 2 is being developed and tested (see Figure 6-6). You replace \nLLM calls and external tools with lightweight mocks during development and testing of \nother components. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f99bbe3-d88b-4cc4-b894-32271d58c31a": {"__data__": {"id_": "8f99bbe3-d88b-4cc4-b894-32271d58c31a", "embedding": null, "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 167.1, "x1": 582.01, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9afffe00-31b1-4e7b-a559-e681a5544934", "node_type": "1", "metadata": {"bbox": [{"page": 316, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.28, "x1": 432.01, "y1": 296.18}]}, "hash": "1dd2fde29a40b00244b7e1dac33b72d89a47a08d2fb2d0d51d44ecfc06a37bd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b30f9d62-c284-49a0-9a85-d9a33e43e125", "node_type": "1", "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.78, "x1": 432.01, "y1": 154.82}]}, "hash": "7554d680939b77b832bf61602a2a11cfb218e00116e1d42d8f6d0e7e53ab740c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " ) -> Improvement: \n c = critique_fn(in_text) \n assert_critique(c) <br><br> improved = improve_fn(in_text, c) \n assert_improvement(improved, in_text, c) <br><br>**return**improved <br><br>The chain depends on these functions. You inject a different implementation, which \nis a mock in this case, when you call the chain\u2014that\u2019s what gives the pattern its \nname. The default critique_fn is the critique() function you wrote, so the default \nbehavior of this chain is to call the LLM for both steps. <br><br>In Python, the assert statement behaves differently under different conditions. If \nyou run the code with a -O flag, it will turn off all assertions. That way, you can have \nassertions on during development and off in production, without changing the code \nitself. Running the code in a test environment (Pytest) decorates the assert state\u2010 \nments and provides detailed information about the call stack where the assertions \nfailed. <br><br>**Mocking Step 1**<br><br>When developing Step 2 or to test it independently of Step 1, you create a mock \nimplementation of the critique() function that has hardcoded values: <br><br>**def**mock_critique(in_text: str) -> Critique: \n print(f\"Using mock to critique text\") \n**return**Critique( \n target_audience = 'AI Engineers,Machine Learning Engineers,Software Engineers'.split(','), \n improvements = \"\"\" \nUse more precise language to define the problems the book solves. \nAdd specific examples of how the design patterns have been used to solve real-world problems. \nHighlight the benefits of using design patterns, such as increased efficiency, reduced costs, and improved accuracy. \nEmphasize the book's practical approach, with step-by-step instructions and code examples. \nInclude testimonials from data scientists who have used the design patterns in the book to improve their work. \n \"\"\".strip().split('**\\n**') \n ) <br><br>Now, use this mock in place of the critique() function in the chain: <br><br>improved = improvement_chain(mldp_text, critique_fn=mock_critique) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b30f9d62-c284-49a0-9a85-d9a33e43e125": {"__data__": {"id_": "b30f9d62-c284-49a0-9a85-d9a33e43e125", "embedding": null, "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.78, "x1": 432.01, "y1": 154.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8f99bbe3-d88b-4cc4-b894-32271d58c31a", "node_type": "1", "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 167.1, "x1": 582.01, "y1": 606.65}]}, "hash": "3ef6d4ded08c59cd8d1b030fe2632ca89bacf390acc3e4c159fbac7d1f01dd9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c4436ae-990e-478e-9fb5-04b2b5776af8", "node_type": "1", "metadata": {"bbox": [{"page": 319, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.81, "x1": 432.01, "y1": 305.66}]}, "hash": "1ef6ae55ab574f06debdb58662c7ed423160c4308923d1c563c135052702587a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Considerations**\nWe\u2019ve illustrated mock functions here, but you can mock objects, too, using features \nsuch as abstract classes and inheritance. Make sure to do so idiomatically in the \nlanguage you\u2019re using (Go, TypeScript, etc.). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c4436ae-990e-478e-9fb5-04b2b5776af8": {"__data__": {"id_": "7c4436ae-990e-478e-9fb5-04b2b5776af8", "embedding": null, "metadata": {"bbox": [{"page": 319, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.81, "x1": 432.01, "y1": 305.66}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b30f9d62-c284-49a0-9a85-d9a33e43e125", "node_type": "1", "metadata": {"bbox": [{"page": 318, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.78, "x1": 432.01, "y1": 154.82}]}, "hash": "7554d680939b77b832bf61602a2a11cfb218e00116e1d42d8f6d0e7e53ab740c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "217f2408-b54a-4a96-9534-dc24027aaa70", "node_type": "1", "metadata": {"bbox": [{"page": 320, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 177.04, "x1": 432.0, "y1": 320.54}]}, "hash": "14b8d75aa565b0fa15ce6e1400324e24f88e507bf8a35be7c3bb546ac45bb9b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Problem**\nThe process of building a GenAI application typically involves trial and error. The \ninput into a foundational model is typically a text prompt. To get better results, the \nfirst thing you\u2019d try is prompt engineering or changing the prompt\u2014you might make \nthe prompt more detailed, add a few examples, reorder the order of instructions, and \nso on. <br><br>If you change the foundational model (such as when your model provider releases \na new version), all your trials need to be repeated. This makes your application as \na whole very brittle to changes in the underlying LLM\u2014or changes to any other \ndependency (such as in the toolchain). <br><br>Is there an easy way to update your prompts when your dependencies change, to \nmake sure that you are maintaining the same level of performance? <br><br>**Solution**\nThere is a famous aphorism called the fundamental theorem of software engineering, \nand perhaps appropriately, its origins are convoluted. The term was coined by pro\u2010 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "217f2408-b54a-4a96-9534-dc24027aaa70": {"__data__": {"id_": "217f2408-b54a-4a96-9534-dc24027aaa70", "embedding": null, "metadata": {"bbox": [{"page": 320, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 177.04, "x1": 432.0, "y1": 320.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7c4436ae-990e-478e-9fb5-04b2b5776af8", "node_type": "1", "metadata": {"bbox": [{"page": 319, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.81, "x1": 432.01, "y1": 305.66}]}, "hash": "1ef6ae55ab574f06debdb58662c7ed423160c4308923d1c563c135052702587a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c6cf70-4a15-4376-9ca1-0deb425a7c8f", "node_type": "1", "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.45, "x1": 432.01, "y1": 605.54}]}, "hash": "859e2bfce964cb285716842bb5b5ff09b8a0a4d9eabc5e9406218c615d1795ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An evaluator <br><br>You need an evaluator that can automatically evaluate the result of running the \npipeline on an input. This evaluation could be based on comparing the result \nof the pipeline to a reference answer (for supervised training) or calculating a \nfitness score (as in Pattern 17, LLM-as-Judge; this is useful when you don\u2019t have \nreference answers). <br><br>An optimizer <br><br>You need an optimization framework that can generate multiple prompt varia\u2010 \ntions, run them against your dataset, and return an optimized pipeline. This \noptimized pipeline is the input pipeline, in which each step is populated with the \nprompt that performed best when evaluated on the set of examples. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49c6cf70-4a15-4376-9ca1-0deb425a7c8f": {"__data__": {"id_": "49c6cf70-4a15-4376-9ca1-0deb425a7c8f", "embedding": null, "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.45, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "217f2408-b54a-4a96-9534-dc24027aaa70", "node_type": "1", "metadata": {"bbox": [{"page": 320, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 177.04, "x1": 432.0, "y1": 320.54}]}, "hash": "14b8d75aa565b0fa15ce6e1400324e24f88e507bf8a35be7c3bb546ac45bb9b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8403f396-80df-4204-9e23-b599ac6e60d7", "node_type": "1", "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.94, "x1": 662.75, "y1": 252.64}]}, "hash": "de4541ce04bc3a163cd59138bca109a99d401ad3c5ebea601d2505a7a02c6b36", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next section, you\u2019ll see how these components work in DSPy, which is a \nframework that supports prompt optimization. <br><br>**Example**\nSuppose you want to improve the information that appears on the back cover of \nO\u2019Reilly books. This includes a description of the book, what readers can expect \nto learn, and blurbs (which are quotes from reviewers who have praised the book). \nToday, those descriptions are based on drafts by the books\u2019 authors who are often \npoor marketers of their own books. The full code for this example is on GitHub, so \nplease follow along with us. <br><br>**Creating a pipeline**<br><br>The first step is to configure DSPy with the LLM you want to use. Even though you \ncan use a different LLM for each step of the pipeline, we\u2019ll keep it simple and use the \nsame LLM for all the calls. <br><br>**import dspy**\nlm = dspy.LM(\"claude-3-7-sonnet-latest\", \n api_key=os.environ['ANTHROPIC_API_KEY']) \ndspy.configure(lm=lm) <br><br>As you\u2019d expect, DSPy supports all the major LLMs. <br><br>The first step of the pipeline is to extract the various parts from the text. The step will \nhave one input (the text that appears on the back cover) and one output (a Python \ndata class), so you define its signature as follows: <br><br>**class BlurbExtraction**(dspy.Signature): \n text: str = dspy.InputField(desc=\"Text from backcover\") \n blurb: Blurb = dspy.OutputField(desc=\"Extracted information\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8403f396-80df-4204-9e23-b599ac6e60d7": {"__data__": {"id_": "8403f396-80df-4204-9e23-b599ac6e60d7", "embedding": null, "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.94, "x1": 662.75, "y1": 252.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "49c6cf70-4a15-4376-9ca1-0deb425a7c8f", "node_type": "1", "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.45, "x1": 432.01, "y1": 605.54}]}, "hash": "859e2bfce964cb285716842bb5b5ff09b8a0a4d9eabc5e9406218c615d1795ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5af9b16-e7be-4705-8a2a-8a4272c3fb01", "node_type": "1", "metadata": {"bbox": [{"page": 323, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.29, "x1": 432.0, "y1": 438.15}]}, "hash": "3d670df5286b1a64095e984b920f5f8748d51a52c4d20f3b1b1e55e0f9bcf3e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For simplicity, we\u2019ll call the entire information on the back cover the blurb, although \nthe blurb is technically only praise quotes. Defined this way, the blurb on the back of \nan O\u2019Reilly book has four parts, so you can define the output Blurb class as follows: <br><br>**class Blurb**: \n about_topic: str = Field(description=\"Why the topic of book is worth learning\") \n about_book: str = Field(description=\"What book contains\") \n target_audience: List[str] = Field(description=\"Roles such as Data Engineer, Data Analyst that the book is for\") \n learning_objectives: List[str] = Field(description=\"4-6 learning objectives that complete the sentence: You will learn how to___\") <br><br>To provide more information about the variables than you can fit into the variable \nname, you can initialize them with description strings, as we did previously. <br><br>To run the pipeline, you can ask DSPy to automatically create a CoT module with the \ngiven signature: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5af9b16-e7be-4705-8a2a-8a4272c3fb01": {"__data__": {"id_": "c5af9b16-e7be-4705-8a2a-8a4272c3fb01", "embedding": null, "metadata": {"bbox": [{"page": 323, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.29, "x1": 432.0, "y1": 438.15}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8403f396-80df-4204-9e23-b599ac6e60d7", "node_type": "1", "metadata": {"bbox": [{"page": 322, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 96.94, "x1": 662.75, "y1": 252.64}]}, "hash": "de4541ce04bc3a163cd59138bca109a99d401ad3c5ebea601d2505a7a02c6b36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4af557dc-3447-439e-b446-ec7c2d1e99fe", "node_type": "1", "metadata": {"bbox": [{"page": 324, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 114.32, "x1": 786.0, "y1": 285.89}]}, "hash": "c6440b2a061d7fff1a4129a14a1adb6682a1da735a58b6001298009753988570", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, combine the two steps into a single module so that you can optimize both steps \ntogether: <br><br>**class BlurbPipeline**(dspy.Module): \n**def** __init__(self): \n**self.extract_info**= dspy.ChainOfThought(BlurbExtraction) \n**self.improve_blurb**= dspy.ChainOfThought(BlurbImprovement) <br><br>**def**forward(self, in_text: str) -> (Blurb, Blurb): \n cover_info =**self.extract_info**(text=in_text) \n improved_cover =**self.improve_blurb**( \n current_cover=cover_info.blurb) \n**return**cover_info.blurb, make_blurb(improved_cover.toDict()) <br><br>The forward() method in the module calls the steps one after the other, passing the \noutput of the first step as the input of the second. <br><br>To run the pipeline, use the following code: <br><br>program = BlurbPipeline() \norig_blurb, improved_blurb = program(in_text=...) <br><br>How well does it do? <br><br>**Results**<br><br>The current text on the back cover of the Machine Learning Design Patterns book \nidentifies the following learning objectives: <br><br>You\u2019ll learn how to: <br><br>*Identify and mitigate common challenges when training, evaluating, and deploying \nML models <br><br>*Represent data for different ML model types, including embeddings, feature crosses, \nand more <br><br>*Choose the right model type for specific problems ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4af557dc-3447-439e-b446-ec7c2d1e99fe": {"__data__": {"id_": "4af557dc-3447-439e-b446-ec7c2d1e99fe", "embedding": null, "metadata": {"bbox": [{"page": 324, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 114.32, "x1": 786.0, "y1": 285.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c5af9b16-e7be-4705-8a2a-8a4272c3fb01", "node_type": "1", "metadata": {"bbox": [{"page": 323, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.29, "x1": 432.0, "y1": 438.15}]}, "hash": "3d670df5286b1a64095e984b920f5f8748d51a52c4d20f3b1b1e55e0f9bcf3e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "793b3124-9da0-44d8-b861-924a69cbaf2a", "node_type": "1", "metadata": {"bbox": [{"page": 325, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.31, "x1": 432.0, "y1": 398.03}]}, "hash": "37c66561dfd857cbb76b08831ce5bfd8804ed5b675d726029f169c48ad749032", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Evaluating the blurb**<br><br>To create an evaluator that measures how good a candidate blurb is, you can use \nLLM-as-Judge. While you can create a rubric for each component of the blurb, you \nknow from the \u201cConsiderations\u201d section of \u201cLLM-as-Judge: Pattern 17\u201d that it tends \nto be quite lenient. If one of the blurbs is rated 0.89 and the other is rated 0.90,3 it\u2019s \ndifficult to trust in the improvement. You\u2019ll get better results if you give the LLM the \noriginal text as a reference blurb and ask it to compare the rewritten blurb to the \noriginal: <br><br>**class BlurbScore**(dspy.Signature): \n reference_blurb: Blurb = dspy.InputField() \n blurb_to_evaluate: Blurb = dspy.InputField() \n topic_score: float = dspy.OutputField(desc=\"-1 to 1: how much more appealing the topic description is, as compared to reference\") \n contents_score: float = dspy.OutputField(desc=\"-1 to 1: how much more appealing (to target audience) the book content description is, as compared to reference\") \n objectives_score: List[float] = dspy.OutputField(desc=\"-1 to 1: score of how appealing each learning objective is to target audience, as compared to reference\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "793b3124-9da0-44d8-b861-924a69cbaf2a": {"__data__": {"id_": "793b3124-9da0-44d8-b861-924a69cbaf2a", "embedding": null, "metadata": {"bbox": [{"page": 325, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.31, "x1": 432.0, "y1": 398.03}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4af557dc-3447-439e-b446-ec7c2d1e99fe", "node_type": "1", "metadata": {"bbox": [{"page": 324, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 114.32, "x1": 786.0, "y1": 285.89}]}, "hash": "c6440b2a061d7fff1a4129a14a1adb6682a1da735a58b6001298009753988570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f52231ef-d2f1-4916-97b5-f218dca62527", "node_type": "1", "metadata": {"bbox": [{"page": 326, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.18, "x1": 432.01, "y1": 264.28}]}, "hash": "256da524e7a6dc88fc15a8770d2c5494a2278b6ac0097d49add840bf06bd0d93", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The generated prompts**<br><br>All you\u2019ve done here is to specify the input and desired output descriptions for each \nstep. The actual prompts to the language model were generated by DSPy. <br><br>This is the prompt that was generated when we performed the blurb improvement \nstep, for example: <br><br>Your input fields are: <br><br>1. `current_cover` (Blurb): Current information on book <br><br>Your output fields are: <br><br>1. `reasoning` (str) <br><br>2. `about_topic` (str): More catchy statement why topic is worth learning <br><br>... <br><br>These are just the input and output variables specified in the module signature (see \nthe full code on GitHub), except that a reasoning field has been added to the set of \noutputs in order to implement CoT to generate the reasoning part of the preceding \ntemplate. <br><br>For the book used as the input, the generated reasoning is as follows: <br><br>The current cover information is solid but could be more compelling. I\u2019ll enhance it \nby: <br><br>1. Making the \u201cabout topic\u201d section more urgent and exciting by emphasizing how \ndesign patterns give practitioners a competitive edge <br><br>2. Making the \u201cabout book\u201d section more dynamic by highlighting the practical, trans\u2010 \nformative nature of the content ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f52231ef-d2f1-4916-97b5-f218dca62527": {"__data__": {"id_": "f52231ef-d2f1-4916-97b5-f218dca62527", "embedding": null, "metadata": {"bbox": [{"page": 326, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.18, "x1": 432.01, "y1": 264.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "793b3124-9da0-44d8-b861-924a69cbaf2a", "node_type": "1", "metadata": {"bbox": [{"page": 325, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.31, "x1": 432.0, "y1": 398.03}]}, "hash": "37c66561dfd857cbb76b08831ce5bfd8804ed5b675d726029f169c48ad749032", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95ea57dc-154d-4031-a8b5-32f9af749b9e", "node_type": "1", "metadata": {"bbox": [{"page": 327, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.7, "x1": 432.01, "y1": 311.61}]}, "hash": "19f5bcccc4ef8d5fb256806f9deb2c6db3ae1c828bdb82c528a892afc06e27de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When we did this, we got a score of 0.74\u2014which is higher than the 0.63 we got \nearlier. This is the start of that blurb: <br><br>Machine learning design patterns are the secret weapons of elite AI teams, distilling \ndecades of hard-won expertise into actionable blueprints for success. In an era where \nML systems increasingly drive critical business decisions, these patterns provide the \narchitectural foundation needed to build robust, scalable, and ethical AI solutions that \navoid costly pitfalls that plague most projects. <br><br>As an author of the book and the one who drafted the original description, \u201celite AI \nteams\u201d makes me cringe. Good thing the LLM has no such qualms\u2014I can see how \nthis would be much more appealing to potential buyers. <br><br>You carried out N trials and chose the best one, and the variations among the trials \nwere in how the reasoning string in CoT was constructed. The problem with this \napproach is that you have to run inference 10 times. Is there a way to create a better \nprompt that would work for any book and run inference only once? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95ea57dc-154d-4031-a8b5-32f9af749b9e": {"__data__": {"id_": "95ea57dc-154d-4031-a8b5-32f9af749b9e", "embedding": null, "metadata": {"bbox": [{"page": 327, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.7, "x1": 432.01, "y1": 311.61}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f52231ef-d2f1-4916-97b5-f218dca62527", "node_type": "1", "metadata": {"bbox": [{"page": 326, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.18, "x1": 432.01, "y1": 264.28}]}, "hash": "256da524e7a6dc88fc15a8770d2c5494a2278b6ac0097d49add840bf06bd0d93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ced4b6c-bc30-4ba2-bb20-a0c3ef7a2fba", "node_type": "1", "metadata": {"bbox": [{"page": 328, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.43, "x1": 432.01, "y1": 288.27}]}, "hash": "e1a493f955f59ccddf47e4a5919675a747f9f420dd30dbb7907dbb57ff9d626a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can then save the optimized pipeline and use it for inference: <br><br>orig_blurb, optimized_blurb = optimized_pipeline( \n in_text=mldp_text \n) <br><br>More to the point, every time the LLM version changes, you can rerun the pipeline, \nget a prompt that performs well across many books, and continue on. Your code itself \ndoesn\u2019t have prompts. <br><br>**Considerations**\nA naive solution to make it easier to update your application is to use a prompt \nlibrary. If you externalize all your prompts to a configuration file, it\u2019ll be easier to \nchange the prompts over time. However, this sort of prompt management doesn\u2019t \nsolve the core problem, which is that you\u2019ll still have to experiment manually \nwhenever your dependencies change. Prompt management and versioning are good \npractices, though. We intentionally didn\u2019t list them as patterns because Prompt Opti\u2010 \nmization provides a more encompassing solution. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ced4b6c-bc30-4ba2-bb20-a0c3ef7a2fba": {"__data__": {"id_": "2ced4b6c-bc30-4ba2-bb20-a0c3ef7a2fba", "embedding": null, "metadata": {"bbox": [{"page": 328, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.43, "x1": 432.01, "y1": 288.27}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "95ea57dc-154d-4031-a8b5-32f9af749b9e", "node_type": "1", "metadata": {"bbox": [{"page": 327, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.7, "x1": 432.01, "y1": 311.61}]}, "hash": "19f5bcccc4ef8d5fb256806f9deb2c6db3ae1c828bdb82c528a892afc06e27de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae043c54-47a6-422e-9c38-4b71dad7f186", "node_type": "1", "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 387.25, "x1": 423.73, "y1": 605.63}]}, "hash": "537d2785eab2f4b7cd7e446bcf79ba860f2a7f9110ab7f157535bfeb71a4c6b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nDSPy was introduced by Omar Khattab and his colleagues at Stanford in the paper \n\u201cDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines\u201d \nin 2023. <br><br>**Summary**\nReliability is a critical concern when building GenAI applications, since the foun\u2010 \ndational models on which they are built can produce inconsistent, incorrect, or \nhallucinated outputs that undermine user trust. This chapter presents four patterns \nthat address different aspects of reliability challenges: LLM-as-Judge enables system\u2010 \natic evaluation of outputs through prompting, machine learning, and fine-tuning \napproaches; Reflection empowers models to critique and improve their own respon\u2010 \nses; Dependency Injection creates testable, mockable components for robust appli\u2010 \ncation architecture; and Prompt Optimization systematically refines prompts to \nmaximize reliability across diverse inputs. Table 6-1 summarizes these patterns. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae043c54-47a6-422e-9c38-4b71dad7f186": {"__data__": {"id_": "ae043c54-47a6-422e-9c38-4b71dad7f186", "embedding": null, "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 387.25, "x1": 423.73, "y1": 605.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2ced4b6c-bc30-4ba2-bb20-a0c3ef7a2fba", "node_type": "1", "metadata": {"bbox": [{"page": 328, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.43, "x1": 432.01, "y1": 288.27}]}, "hash": "e1a493f955f59ccddf47e4a5919675a747f9f420dd30dbb7907dbb57ff9d626a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff46bd3c-3963-40c7-9730-a7366ec681d6", "node_type": "1", "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.14, "x1": 432.01, "y1": 428.65}]}, "hash": "abc214234c6ff5edce1826583b14c11a4192ca13f281e88c9260f241101e5c07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 6-1. Patterns for increasing reliability <br><br>**Patterns**\n**LLM-as-Judge**\n**(Pattern 17)**<br><br>**Problems**\nEvaluation of GenAI \ncapabilities is hard because \nthe tasks that GenAI performs \nare open-ended. <br><br>**Solutions**\nProvide detailed, multi-dimensional \nfeedback that can be used \nto compare models, track \nimprovements, and guide further \ndevelopment. \nThe feedback is used to modify the \nprompt that is sent to the LLM a \nsecond time. \n**Reflection**\n**(Pattern 18)**<br><br>How can you get the LLM \nto correct an earlier response \nin response to feedback or \ncriticism? \nYou need to independently \ndevelop and test each \ncomponent of an LLM chain.  Reliable performance in \nmost complex tasks where \nthe approach can\u2019t be \npredetermined. \nAny situation in which you \nchain LLM calls or use external \ntools. <br><br>When you build chains of LLM calls, \nbuild them so that it\u2019s easy to inject a \nmock implementation to replace any \nstep of the chain. \nSystematically set the prompts used \nin a GenAI pipeline by optimizing \nthem on a dataset of examples.  **Prompt**\n**Optimization**\n**(Pattern 20)**\nYou need to easily update \nprompts when dependencies \nchange to maintain level of \nperformance. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff46bd3c-3963-40c7-9730-a7366ec681d6": {"__data__": {"id_": "ff46bd3c-3963-40c7-9730-a7366ec681d6", "embedding": null, "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.14, "x1": 432.01, "y1": 428.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ae043c54-47a6-422e-9c38-4b71dad7f186", "node_type": "1", "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 387.25, "x1": 423.73, "y1": 605.63}]}, "hash": "537d2785eab2f4b7cd7e446bcf79ba860f2a7f9110ab7f157535bfeb71a4c6b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fec5ecb8-90a0-4bd2-ba3e-973d1815d4ed", "node_type": "1", "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 192.67, "x1": 432.0, "y1": 582.33}]}, "hash": "cf30fa68e7ac61241fcb272c682099271f77444734a80460089ecfb22286851a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Any situation in which you have \nto reduce the maintenance \noverhead associated with LLM \nversion changes (and other \ndependencies). <br><br>Together, these patterns form a comprehensive toolkit for increasing the reliability of \nLLM applications across various dimensions. LLM-as-Judge and Reflection focus on \noutput quality assessment, while Dependency Injection addresses structural reliability \nthrough better testing practices, and Prompt Optimization ensures consistent per\u2010 \nformance through systematic prompt engineering. By implementing these patterns \nappropriately, developers can significantly enhance the trustworthiness of their LLM \napplications and therefore make them more suitable for production environments \nwhere reliability is paramount. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fec5ecb8-90a0-4bd2-ba3e-973d1815d4ed": {"__data__": {"id_": "fec5ecb8-90a0-4bd2-ba3e-973d1815d4ed", "embedding": null, "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 192.67, "x1": 432.0, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ff46bd3c-3963-40c7-9730-a7366ec681d6", "node_type": "1", "metadata": {"bbox": [{"page": 329, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 261.14, "x1": 432.01, "y1": 428.65}]}, "hash": "abc214234c6ff5edce1826583b14c11a4192ca13f281e88c9260f241101e5c07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2b79dd8-25b2-4f5b-8750-08ab0cb13915", "node_type": "1", "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.26, "x1": 432.0, "y1": 181.1}]}, "hash": "8b2cf21124f026ae7fbe8a455a8c66fa7c6ad6be121e05d0071f30491f08e171", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 7**\n**Enabling Agents to Take Action**<br><br>The patterns we\u2019ve covered in this book so far have involved creating content in some \nform. In this chapter, we\u2019ll discuss three patterns that enable applications built on \nfoundational models to interact with the world\u2014often, this is the line beyond which \nwe consider the application to be agentic. Tool Calling (Pattern 21) enables models to \ninvoke external functions through structured interfaces. Code Execution (Pattern 22) \nallows models to write and run code to solve complex problems. Multiagent Collabo\u2010 \nration (Pattern 23) orchestrates specialized AI agents in hierarchical, peer-to-peer, or \nmarket-based architectures to tackle complex tasks through division of labor. <br><br>**Pattern 21: Tool Calling**\nThe Tool Calling pattern enables an LLM to act in the world, either to obtain infor\u2010 \nmation (for example, through web search) or to make a change to the environment \n(for example, by writing out a file). It does so by having the LLM emit special tokens \nwhen it determines that a function needs to be called, along with the arguments to \npass to that function. A client-side postprocessor invokes the function with those \nparameters and sends the results back to the LLM, and the LLM incorporates the \nfunction results in its response. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2b79dd8-25b2-4f5b-8750-08ab0cb13915": {"__data__": {"id_": "f2b79dd8-25b2-4f5b-8750-08ab0cb13915", "embedding": null, "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.26, "x1": 432.0, "y1": 181.1}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fec5ecb8-90a0-4bd2-ba3e-973d1815d4ed", "node_type": "1", "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 192.67, "x1": 432.0, "y1": 582.33}]}, "hash": "cf30fa68e7ac61241fcb272c682099271f77444734a80460089ecfb22286851a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e379785-7af4-4d44-a5fa-8e69ef2b58a7", "node_type": "1", "metadata": {"bbox": [{"page": 331, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.22, "x1": 432.01, "y1": 329.22}]}, "hash": "0ef52f44c8c7c92cf000580e3c751754fa099523b63e372453d16975f20f7bdf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Problem**\nAt their core, multimodal LLMs generate content in different forms (text, images, \naudio, and video). This is enough to provide capabilities such as creating research \nreports, translating between languages, and generating code. If the content generation \nis limited by the internal knowledge that the foundational model acquired during \ntraining, it is possible to inject new knowledge using RAG (see Chapter 2 and \nChapter 3). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e379785-7af4-4d44-a5fa-8e69ef2b58a7": {"__data__": {"id_": "7e379785-7af4-4d44-a5fa-8e69ef2b58a7", "embedding": null, "metadata": {"bbox": [{"page": 331, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.22, "x1": 432.01, "y1": 329.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f2b79dd8-25b2-4f5b-8750-08ab0cb13915", "node_type": "1", "metadata": {"bbox": [{"page": 330, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 84.26, "x1": 432.0, "y1": 181.1}]}, "hash": "8b2cf21124f026ae7fbe8a455a8c66fa7c6ad6be121e05d0071f30491f08e171", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72166321-1dca-4273-9ec2-700b2d669006", "node_type": "1", "metadata": {"bbox": [{"page": 332, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.77, "x1": 432.01, "y1": 202.67}]}, "hash": "6604be04c35a4cd626182c6225c3a90a82759019c61785bb4cb3739e9d7c6d3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**How Tool Calling works**<br><br>Tool Calling is an extension of Pattern 2, Grammar (see Chapter 2). LLMs are trained \nto emit a special token whenever a function needs to be called, along with the argu\u2010 \nments to that function. For example, the LLM that needs to invoke the flight-booking \nAPI might emit the following text:1 <br><br>Thanks for booking a flight with us! <br><br>[CALL_TOOL: book_flight, TK 161, 2025-06-12, Economy] <br><br>This text is processed by a program that calls the following function: <br><br>fd = book_flight(\"TK 161\", datetime.strptime(\"2025-06-12\"), \"Economy\") <br><br>As you can see, this program picks up the function name and the arguments from \nthe emitted string, makes the necessary type conversions (such as for the date), and \ninvokes the call. This function can then invoke the API from Turkish Airlines or \nExpedia and return the response. <br><br>The return value of the function is then inserted into the original response in place of \nthe tool call: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72166321-1dca-4273-9ec2-700b2d669006": {"__data__": {"id_": "72166321-1dca-4273-9ec2-700b2d669006", "embedding": null, "metadata": {"bbox": [{"page": 332, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.77, "x1": 432.01, "y1": 202.67}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7e379785-7af4-4d44-a5fa-8e69ef2b58a7", "node_type": "1", "metadata": {"bbox": [{"page": 331, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 105.22, "x1": 432.01, "y1": 329.22}]}, "hash": "0ef52f44c8c7c92cf000580e3c751754fa099523b63e372453d16975f20f7bdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c512290b-5a14-4689-a471-61a3ff057da4", "node_type": "1", "metadata": {"bbox": [{"page": 333, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 233.95, "x1": 432.01, "y1": 271.56}]}, "hash": "5f6acf836730d2004e31a80f1637dff0113ac60a62e6b092bef25489f936ea30", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Calculations <br><br>LLMs can use calculator tools, GIS analytics, and optimization solvers to go \nbeyond text generation to solving tasks that require these sophisticated capabili\u2010 \nties. <br><br>Interleaving reasoning and action (ReAct) <br><br>Patterns such as Pattern 13, CoT*Chapter 5), need not be restricted to simply \nlisting steps. Armed with a set of tools, the LLM can carry out the steps and mod\u2010 \nify its behavior based on the tools\u2019 response. The idea of interleaving reasoning \nand action goes by the name ReAct. Because ReAct is \u201cjust\u201d a combination of \nCoT and Tool Calling, we didn\u2019t make it a separate pattern in this book. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c512290b-5a14-4689-a471-61a3ff057da4": {"__data__": {"id_": "c512290b-5a14-4689-a471-61a3ff057da4", "embedding": null, "metadata": {"bbox": [{"page": 333, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 233.95, "x1": 432.01, "y1": 271.56}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "72166321-1dca-4273-9ec2-700b2d669006", "node_type": "1", "metadata": {"bbox": [{"page": 332, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.77, "x1": 432.01, "y1": 202.67}]}, "hash": "6604be04c35a4cd626182c6225c3a90a82759019c61785bb4cb3739e9d7c6d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576e8886-ad7a-46b5-b76b-c248e2aa0bec", "node_type": "1", "metadata": {"bbox": [{"page": 335, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 171.3, "x1": 633.0, "y1": 606.65}]}, "hash": "0c254cb2acfd36ac6399feadd59466f14a7de75d3456e3a1cfd8c530f337bad7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 1: Implement the flight-booking function.**The first step (see Figure 7-1) is to imple\u2010 \nment a flight-booking function that can delegate the work to an external API, such as \none from the airline, and return a structured response. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "576e8886-ad7a-46b5-b76b-c248e2aa0bec": {"__data__": {"id_": "576e8886-ad7a-46b5-b76b-c248e2aa0bec", "embedding": null, "metadata": {"bbox": [{"page": 335, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 171.3, "x1": 633.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c512290b-5a14-4689-a471-61a3ff057da4", "node_type": "1", "metadata": {"bbox": [{"page": 333, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 233.95, "x1": 432.01, "y1": 271.56}]}, "hash": "5f6acf836730d2004e31a80f1637dff0113ac60a62e6b092bef25489f936ea30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e07be2e-7da4-4e32-940e-c63ddc36c42e", "node_type": "1", "metadata": {"bbox": [{"page": 336, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 103.67, "x1": 423.75, "y1": 161.67}]}, "hash": "c793962c7fe80992826fab29b2cc7354aeb4bb28e5c8546535d8ed72ed34c92c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " ... \n } \n ) \n*# Return structured response*\n booking_data = response.json() \n**return**BookingData(**booking_data) <br><br>**Step 2: Call the model with a tool definition.**Next, make the preceding function one of \nthe tools that\u2019s available to the LLM: <br><br>tools = [{ \n \"type\": \"function\", \n \"name\": \"book_flight\", \n \"description\": \"Books a flight using the airline API\", \n \"parameters\": { \n \"type\": \"object\", \n \"properties\": { \n \"flight_code\": { \n \"type\": \"string\", \n \"description\": \"IATA flight code like AA 123\" \n }, \n \"departure_date\": { \n \"type\": \"string\", \n \"description\": \"Date of departure in YYYY-MM-DD format (e.g., 2025-05-20)\" \n }, \n \"cabin_class\": { \n \"type\": \"string\", \n \"enum\": [\"economy\", \"premium_economy\", \"business\", \"first\"], \n \"description\": \"Class of travel\" \n }, \n ... <br><br>Then, pass in the list of tools to the model (in reality, you just pass in the description \nof the tools): <br><br>response = client.responses.create( \n model=\"gpt-4.1\", \n input=[{ \n \"role\": \"user\", \n \"content\": \"Book me an economy class ticket from Mauritius to Istanbul on June 12 on whatever non-stop flight is available.\" \n}], \n tools=tools, \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e07be2e-7da4-4e32-940e-c63ddc36c42e": {"__data__": {"id_": "7e07be2e-7da4-4e32-940e-c63ddc36c42e", "embedding": null, "metadata": {"bbox": [{"page": 336, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 103.67, "x1": 423.75, "y1": 161.67}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "576e8886-ad7a-46b5-b76b-c248e2aa0bec", "node_type": "1", "metadata": {"bbox": [{"page": 335, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 171.3, "x1": 633.0, "y1": 606.65}]}, "hash": "0c254cb2acfd36ac6399feadd59466f14a7de75d3456e3a1cfd8c530f337bad7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "601fb2c4-3d1f-4866-9a48-783cefe84dc7", "node_type": "1", "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 256.85, "x1": 432.01, "y1": 556.54}]}, "hash": "4f219b2467bfd6583e1965544b59d0b33f47d9fb80b5a731ff6918f01c7f1970", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\u2019s likely that your LLM preference on a specific use will vary over time, for both \ntechnical and commercial reasons. The variations among LLMs, the importance of \nTool Calling to any agentic use case, and the likelihood that you\u2019ll change LLMs \nseveral times in your project all point to the need to develop GenAI code by using an \nLLM-agnostic framework (such as PydanticAI, LangChain, LangGraph, or LiteLLM). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "601fb2c4-3d1f-4866-9a48-783cefe84dc7": {"__data__": {"id_": "601fb2c4-3d1f-4866-9a48-783cefe84dc7", "embedding": null, "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 256.85, "x1": 432.01, "y1": 556.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7e07be2e-7da4-4e32-940e-c63ddc36c42e", "node_type": "1", "metadata": {"bbox": [{"page": 336, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 103.67, "x1": 423.75, "y1": 161.67}]}, "hash": "c793962c7fe80992826fab29b2cc7354aeb4bb28e5c8546535d8ed72ed34c92c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d3e47b7-4f4d-4dcf-8d72-b76c7a0f6961", "node_type": "1", "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.6, "x1": 432.0, "y1": 244.29}]}, "hash": "bb31171f64b8212a3083d679c29194bbc0f6913bb068681ad938f72173ae2e76", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 3: Process the output and invoke functions on the client side.**You might have other \nfunctions that, for instance, find available flights, compare fare prices, extract pass\u2010 \nport information from the customer\u2019s profile, and so on. The LLM will choose the \nright function to call in the context of the conversation or workflow\u2014and tying many \nsuch steps into a CoT is where ReAct comes in. <br><br>When the model determines that it needs to call a function, its response will include a \ntool call like this one: <br><br>[{ \n \"type\": \"function_call\", \n \"id\": \"fc_12345xyz\", \n \"call_id\": \"call_12345xyz\", \n \"name\": \"book_flight\", \n \"arguments\": \"{\\\"flight_code\\\":\\\"TK 161\\\", \n... <br><br>The LLM doesn\u2019t actually call the external function, since that would be unsafe \n(unless it\u2019s one of the supported tools such as web search or sandboxed code exe\u2010 \ncution). OpenAI doesn\u2019t want adversarial actors injecting arbitrary code into its \nLLM! Instead, you have to make this call on the client side. Presumably, you trust \nthe book_flight() function\u2014after all, you wrote it. To make the call, process the \nresponse: <br><br>tool_call = response.output[0] \n**if**tool_call.name == \"book_flight\": \n args = json.loads(tool_call.arguments) \nresult = book_flight(args[\"flight_code\"], ... ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d3e47b7-4f4d-4dcf-8d72-b76c7a0f6961": {"__data__": {"id_": "5d3e47b7-4f4d-4dcf-8d72-b76c7a0f6961", "embedding": null, "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.6, "x1": 432.0, "y1": 244.29}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "601fb2c4-3d1f-4866-9a48-783cefe84dc7", "node_type": "1", "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 256.85, "x1": 432.01, "y1": 556.54}]}, "hash": "4f219b2467bfd6583e1965544b59d0b33f47d9fb80b5a731ff6918f01c7f1970", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ac69615-14df-4f64-a476-de9376bdae35", "node_type": "1", "metadata": {"bbox": [{"page": 338, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.45, "x1": 432.05, "y1": 601.79}]}, "hash": "3088e03d9fde1940c5513adcc53c62f4d79a6bd462ddd0a6f93f7dc93445ca70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 4: Supply the result and call the model again.**Take the result and append it to the \nmessages: <br><br>input_messages.append(tool_call)*# append model's function call message*\ninput_messages.append({*# append result message*\n \"type\": \"function_call_output\", \n \"call_id\": tool_call.call_id, \n \"output\": json.dumps(result) \n}) <br><br>response_2 = client.responses.create( \n model=\"gpt-4.1\", \n input=input_messages, \n tools=tools, \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ac69615-14df-4f64-a476-de9376bdae35": {"__data__": {"id_": "1ac69615-14df-4f64-a476-de9376bdae35", "embedding": null, "metadata": {"bbox": [{"page": 338, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.45, "x1": 432.05, "y1": 601.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5d3e47b7-4f4d-4dcf-8d72-b76c7a0f6961", "node_type": "1", "metadata": {"bbox": [{"page": 337, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.6, "x1": 432.0, "y1": 244.29}]}, "hash": "bb31171f64b8212a3083d679c29194bbc0f6913bb068681ad938f72173ae2e76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1754b9f0-98b1-4e44-8f44-d38d180936a3", "node_type": "1", "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.96, "x1": 709.5, "y1": 605.54}]}, "hash": "382edab58625e529ab2301f97103db4609b0cbc1d28e1c37f28e3321da57e0ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 5: Final model response.**The model will now incorporate the result into its output: <br><br>\u201cGreat news! I\u2019ve successfully booked your flight from Mauritius (MRU) to Istanbul \n(IST). <br><br>Booking details: <br><br>- Booking reference: ... <br><br>Now that you\u2019ve seen what happens in a low-level API, let\u2019s see how much of this is \nabstracted away when you use a high-level framework such as LangGraph along with \na protocol such as MCP. <br><br>**Tool Calling with LangGraph and MCP**<br><br>You can use frameworks and protocols to simplify the steps listed in the previous \nsection. Supplying function details to the foundational model (Steps 1 and 2) is \nsimplified by the Model Context Protocol (MCP), which is described in more detail \nin this section. Client processing (Steps 3 and 4) can be simplified by a client-side \nframework such as LangGraph. Step 5 remains the same. <br><br>As shown in Figure 7-2, your application communicates with different MCP servers \nthrough an MCP client that is embedding in your application. 3rd parties provide \nMCP servers ranging from simple API interfaces to complex payment processes. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1754b9f0-98b1-4e44-8f44-d38d180936a3": {"__data__": {"id_": "1754b9f0-98b1-4e44-8f44-d38d180936a3", "embedding": null, "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.96, "x1": 709.5, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1ac69615-14df-4f64-a476-de9376bdae35", "node_type": "1", "metadata": {"bbox": [{"page": 338, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.45, "x1": 432.05, "y1": 601.79}]}, "hash": "3088e03d9fde1940c5513adcc53c62f4d79a6bd462ddd0a6f93f7dc93445ca70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd9cd194-efde-4912-b656-d828341bbd76", "node_type": "1", "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.7, "x1": 432.0, "y1": 309.14}]}, "hash": "8331c20e5f0893915ffbc44affa6cc0c6efe5cce2f6e4bb540ad739bff705b8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Make sure that the function name, the parameters, and their respective docstrings \nare self-descriptive. In essence, make sure that all the descriptions that would have \nappeared in Step 2 in the OpenAI Responses API are now present in the function \ndefinition itself: <br><br>@mcp.tool() \n**async def**book_flight(flight_code: str, departure_date: datetime, cabin_class: CabinClass, passenger_details: List[PassengerInfo]) -> BookingData: \n*\"\"\"*\n*Books a flight using the airline API*<br><br>*Args:*\n*flight_code: IATA airline flight code such as AA 123*\n*departure_date: Date of departure*\n*return_date: Date of return in YYYY-MM-DD format*\n*cabin_class: Class of travel (economy, premium_economy, business, first)*\n*passenger_details: List of passenger information including names and passport details*<br><br>*Returns:*\n*Booking confirmation details including booking reference, flight numbers, and total price*\n*\"\"\"*\n... <br><br>If all your clients will also be written in Python, you can have the communication \nhappen via interprocess communication (standard input/output or stdio). In that \ncase, you can expose the tools to clients with the following: <br><br>**if** __name__== \"__main__\": \n mcp.run(transport=\"stdio\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd9cd194-efde-4912-b656-d828341bbd76": {"__data__": {"id_": "bd9cd194-efde-4912-b656-d828341bbd76", "embedding": null, "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.7, "x1": 432.0, "y1": 309.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1754b9f0-98b1-4e44-8f44-d38d180936a3", "node_type": "1", "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 317.96, "x1": 709.5, "y1": 605.54}]}, "hash": "382edab58625e529ab2301f97103db4609b0cbc1d28e1c37f28e3321da57e0ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50c3b015-0ec2-4551-bfe6-6afff9e75a28", "node_type": "1", "metadata": {"bbox": [{"page": 342, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.95, "x1": 432.0, "y1": 199.14}]}, "hash": "3c041c1d6d17a3ff49890b724308ee0df70c82530aa372763f2c5e62ff8a4a71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, if your clients might be written in other languages or might be on \nother machines, you need a network protocol: <br><br>**if** __name__== \"__main__\": \n mcp.run(transport=\"streamable-http\") <br><br>**MCP client.**On the client side, you create an MCP client (which serves as a stub to the \nMCP server) and pass in the locations of the local or remote MCP servers: <br><br>**from langchain_mcp_adapters.client import**MultiServerMCPClient \n**async with**MultiServerMCPClient( \n { \n \"flight_booking\": { \n \"command\": \"python\", \n*# Replace with absolute path to your Python file*\n \"args\": [\"/path/to/flight_booking.py\"], \n \"transport\": \"stdio\", \n }, \n \"flight_options\": { \n*# The default is for the server to start on port 8000*\n \"url\": \"http://localhost:8000/mcp\", \n \"transport\": \"streamable_http\", \n } ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50c3b015-0ec2-4551-bfe6-6afff9e75a28": {"__data__": {"id_": "50c3b015-0ec2-4551-bfe6-6afff9e75a28", "embedding": null, "metadata": {"bbox": [{"page": 342, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.95, "x1": 432.0, "y1": 199.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bd9cd194-efde-4912-b656-d828341bbd76", "node_type": "1", "metadata": {"bbox": [{"page": 340, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.7, "x1": 432.0, "y1": 309.14}]}, "hash": "8331c20e5f0893915ffbc44affa6cc0c6efe5cce2f6e4bb540ad739bff705b8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a115760-469b-43c2-9a94-8a26e81e5d6e", "node_type": "1", "metadata": {"bbox": [{"page": 343, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.23, "x1": 432.01, "y1": 453.05}]}, "hash": "517fdc8c4f96d8c6d7751e11e95ff71a266aba268c6921a644cc97406614b004", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, create a ReAct agent that will automatically invoke the tools when necessary \nand embed their responses into its final response: <br><br>agent = create_react_agent( \n \"anthropic:claude-3-7-sonnet-latest\", \n client.get_tools(), \n) \n... \nweather_response =**await**agent.ainvoke( \n {\"messages\": [{\"role\": \"user\", \"content\": user_input}]} \n) \nprint(weather_response['messages'][-1].content) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a115760-469b-43c2-9a94-8a26e81e5d6e": {"__data__": {"id_": "0a115760-469b-43c2-9a94-8a26e81e5d6e", "embedding": null, "metadata": {"bbox": [{"page": 343, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.23, "x1": 432.01, "y1": 453.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "50c3b015-0ec2-4551-bfe6-6afff9e75a28", "node_type": "1", "metadata": {"bbox": [{"page": 342, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.95, "x1": 432.0, "y1": 199.14}]}, "hash": "3c041c1d6d17a3ff49890b724308ee0df70c82530aa372763f2c5e62ff8a4a71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e75d103-8f42-4922-84bb-5b2dd2655e16", "node_type": "1", "metadata": {"bbox": [{"page": 344, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.23, "x1": 432.01, "y1": 300.73}]}, "hash": "19bac118563834c1bf178a2abe69c586caf63a112e19a42345df564f8eeee09c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " Question: \n\"\"\" <br><br>... <br><br>agent = create_react_agent( \n \"anthropic:claude-3-7-sonnet-latest\", \n client.get_tools(), \n prompt = system_message \n ) <br><br>**Considerations**\nTool Calling relies on the LLM to determine that it needs to invoke a tool and \nthen generate the tool call, but there are some ways in which you can increase the \nreliability of this behavior. MCP is becoming a popular protocol, but at the time of \nwriting, it has a few limitations. Tool Calling also increases the vulnerability of your \napplication to adversarial attacks. <br><br>**Improving Reliability**\nBecause Tool Calling relies on the model to know when to emit the tokens for a tool \ncall, you need to use clear and detailed function names and parameter descriptions. \nUse the system prompt to describe policies on when to use each function (such as \nwhen to search for flights versus when to book and how long search results remain \nvalid). You should also include examples of valid inputs (such as flight codes) and use \nparameter types (such as enums) to take advantage of the Grammar pattern. <br><br>The fewer functions and parameters you use, the more accurate the model will be\u2014at \nthe time of writing (June 2025), the limit seems to be 3 to 10 tools, depending on the \nLLM. Don\u2019t make the model fill in information that you know deterministically: if \nyou already know a passenger\u2019s details, you could offload the burden of maintaining \nthat from the model. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e75d103-8f42-4922-84bb-5b2dd2655e16": {"__data__": {"id_": "3e75d103-8f42-4922-84bb-5b2dd2655e16", "embedding": null, "metadata": {"bbox": [{"page": 344, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.23, "x1": 432.01, "y1": 300.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0a115760-469b-43c2-9a94-8a26e81e5d6e", "node_type": "1", "metadata": {"bbox": [{"page": 343, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.23, "x1": 432.01, "y1": 453.05}]}, "hash": "517fdc8c4f96d8c6d7751e11e95ff71a266aba268c6921a644cc97406614b004", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd0026a-812e-4a5e-820a-2a1f5c0e7368", "node_type": "1", "metadata": {"bbox": [{"page": 345, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.83, "x1": 432.01, "y1": 268.53}]}, "hash": "9f522c020eb2c843830c808af1c47996e8c512db0d4750004285d4c98d314f0f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Collaboration <br><br>In MCP, communication between the client and the server is mostly one-way, \nwhereas in the real world, you may want collaborative, interactive workflows. \nGoogle and IBM introduced support for agent-to-agent communication in \nApril/May 2025 through the A2A and ACP protocols, respectively. We discuss \nA2A in the \u201cMultiagent Collaboration: Pattern 23\u201d section. <br><br>Streaming <br><br>Tool calls can take a long time. If you\u2019re building interactive GenAI applications \nthat employ Tool Calling, you can enhance the user experience by showing real- \ntime progress during long operations. You can also use the streamable HTTP \ntransport mechanism to give clients the option to stream output responses from \nthe model. Standard MCP calls, at the time of writing, are terminated in 30\u201460 \nseconds, depending on network configurations. <br><br>However, MCP is rapidly evolving, and doubtless, some or all of these shortcomings \nmay have been addressed by the time you are reading this. Or perhaps MCP has been \nreplaced by something better. Regardless of which protocol or framework \u201cwins,\u201d we ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fd0026a-812e-4a5e-820a-2a1f5c0e7368": {"__data__": {"id_": "1fd0026a-812e-4a5e-820a-2a1f5c0e7368", "embedding": null, "metadata": {"bbox": [{"page": 345, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.83, "x1": 432.01, "y1": 268.53}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3e75d103-8f42-4922-84bb-5b2dd2655e16", "node_type": "1", "metadata": {"bbox": [{"page": 344, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.23, "x1": 432.01, "y1": 300.73}]}, "hash": "19bac118563834c1bf178a2abe69c586caf63a112e19a42345df564f8eeee09c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "544b5863-8b81-4e92-85cb-51cd3b8730db", "node_type": "1", "metadata": {"bbox": [{"page": 346, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.04, "y1": 367.14}]}, "hash": "00d76ee4afd5d8acbea13de49325531db2d20117bf9fc56f4666bb897989f349", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Map-Reduce <br><br>You dispatch isolated subagents to process individual pieces of the untrusted \nprompt with Tool Calling. The data returned from the \u201cmap\u201d operation is passed \nto a \u201creduce\u201d operation that either does not use an LLM or processes outputs in a \nconstrained Action-Selector way. <br><br>Dual-LLM <br><br>You combine a privileged LLM that plans actions and uses tools with a sand\u2010 \nboxed LLM that processes untrusted data without tool access. <br><br>Code-Then-Execute <br><br>The LLM agent writes a formal computer program to solve the task, which can \ncall tools and spawn unprivileged LLMs for untrusted text. This program is then \nexecuted on untrusted data to maintain a fixed control flow, even if the data itself \nis malicious. See Pattern 22, Code Execution. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "544b5863-8b81-4e92-85cb-51cd3b8730db": {"__data__": {"id_": "544b5863-8b81-4e92-85cb-51cd3b8730db", "embedding": null, "metadata": {"bbox": [{"page": 346, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.04, "y1": 367.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1fd0026a-812e-4a5e-820a-2a1f5c0e7368", "node_type": "1", "metadata": {"bbox": [{"page": 345, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 92.83, "x1": 432.01, "y1": 268.53}]}, "hash": "9f522c020eb2c843830c808af1c47996e8c512db0d4750004285d4c98d314f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19fdd741-111e-4fa9-9249-c8c70ac9ccb2", "node_type": "1", "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 536.25, "x1": 432.0, "y1": 607.89}]}, "hash": "fbd428a90525ab357ee0dea1c84407190ffc988e76bd473f6715994b35b7160d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GitHub provides an MCP server that LLM applications can employ for version \nmanagement and continuous integration. Sentry allows AI assistants to connect with \nit through MCP to provide error handling and performance monitoring. Zapier AI \nActions showcases enterprise-scale tool calling through MCP, which enables agentic \nAI applications to perform a wide variety of actions across thousands of apps for \nautomation workflows. <br><br>**Pattern 22: Code Execution**\nIn the Code Execution pattern, LLMs cause actions (like database updates) to happen \nby generating code that is then executed by an external system. If the purpose is to \ngenerate content like graphs and charts, then the code is executed in a sandbox. <br><br>**Problem**\nLLMs aren\u2019t very good at tasks such as creating graphs or annotating images with text, \nbut you can\u2019t use Tool Calling to do these tasks, either. To create graphs, you don\u2019t \ntypically just call an API. Instead, you write the graph specifications in a program\u2010 \nming language, such as Matplotlib, or in a DSL, such as Mermaid. To annotate images \nwith text, you supply the specifications by invoking a framework like ImageMagick \nwith a set of command-line instructions. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19fdd741-111e-4fa9-9249-c8c70ac9ccb2": {"__data__": {"id_": "19fdd741-111e-4fa9-9249-c8c70ac9ccb2", "embedding": null, "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 536.25, "x1": 432.0, "y1": 607.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "544b5863-8b81-4e92-85cb-51cd3b8730db", "node_type": "1", "metadata": {"bbox": [{"page": 346, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 110.03, "x1": 432.04, "y1": 367.14}]}, "hash": "00d76ee4afd5d8acbea13de49325531db2d20117bf9fc56f4666bb897989f349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c95fa39-13bd-46f7-b7c7-c1050b3ef02a", "node_type": "1", "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.6, "x1": 432.0, "y1": 352.54}]}, "hash": "07b9deefc8ca95f3047b8623586e69de0045f1500bfc8310ecdc3d0d5e48425a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Example**\nLet\u2019s look at an end-to-end example (the full code is on GitHub). We\u2019re going to \ndepict the results of a basketball tournament. For example, we want the LLM to \ngenerate a graph that looks like Figure 7-4\u2014which tells us that Georgia lost to \nGonzaga in round 64, and Gonzaga was in turn eliminated in round 32. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c95fa39-13bd-46f7-b7c7-c1050b3ef02a": {"__data__": {"id_": "9c95fa39-13bd-46f7-b7c7-c1050b3ef02a", "embedding": null, "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.6, "x1": 432.0, "y1": 352.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "19fdd741-111e-4fa9-9249-c8c70ac9ccb2", "node_type": "1", "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 536.25, "x1": 432.0, "y1": 607.89}]}, "hash": "fbd428a90525ab357ee0dea1c84407190ffc988e76bd473f6715994b35b7160d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76e01180-cd69-4c8e-9e36-55d224684a59", "node_type": "1", "metadata": {"bbox": [{"page": 349, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.23, "x1": 432.0, "y1": 251.54}]}, "hash": "c29bd8d15d07a88ed8c19bc97116f89f0e4c13f6e7b1422adf448fc381e586ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 7-4. Part of an image depicting the results of a basketball tournament <br><br>To generate this graph, you\u2019ll use a two-step process. In the first step, you have \nthe LLM generate the DSL that corresponds to a graph-drawing program called \nGraphviz. This DSL, which is called DOT, allows you to represent graphs concisely. <br><br>You can show the LLM an example of transforming part of a basketball result into \nDOT: <br><br>I\u2019ll give you the results of a basketball tournament. <br><br>Use the data to generate Graphviz subgraphs. <br><br>Make sure to name the subgraphs cluster_xxx <br><br>Here\u2019s an example: <br><br>**Input**: <br><br>Saturday, March 29, 2025 (Elite Eight) <br><br>(1) Florida 84, (3) Texas Tech 79 <br><br>(1) Duke 85, (2) Alabama 65 <br><br>Sunday, March 30, 2025 (Elite Eight) <br><br>(1) Houston 69, (2) Tennessee 50 <br><br>(1) Auburn 70, (2) Michigan State 64 <br><br>**Output**: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76e01180-cd69-4c8e-9e36-55d224684a59": {"__data__": {"id_": "76e01180-cd69-4c8e-9e36-55d224684a59", "embedding": null, "metadata": {"bbox": [{"page": 349, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.23, "x1": 432.0, "y1": 251.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9c95fa39-13bd-46f7-b7c7-c1050b3ef02a", "node_type": "1", "metadata": {"bbox": [{"page": 348, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.6, "x1": 432.0, "y1": 352.54}]}, "hash": "07b9deefc8ca95f3047b8623586e69de0045f1500bfc8310ecdc3d0d5e48425a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83e66da3-8e67-49bb-bada-5da0edf25532", "node_type": "1", "metadata": {"bbox": [{"page": 350, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.42, "x1": 432.01, "y1": 376.47}]}, "hash": "4655c48aa240d8184213afa7b95dbd7678271c2359ac2dc1c7e2adc827929ffa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 7-5. Depiction of the results of the four games in the Elite Eight. <br><br>You can obtain the tournament results through a web search via Tool Calling or \nPattern 6, Basic RAG (from Chapter 3), and you can send the resulting DSL to DOT \nto generate the graph. A part of that result was shown in Figure 7-4. <br><br>**Considerations**\nMake sure to do Code Execution in a sandbox environment, which provides an \nisolated, secure space where LLM-generated code can be executed without risking \nthe underlying system. Typically, sandbox environments are subject to constraints \non CPU, memory, network access, and execution time, and they\u2019re monitored to \nprevent infinite loops and resource exhaustion. They\u2019re typically implemented with \ncontainerization technologies like Docker, virtual machines, or specialized runtime \nenvironments with strong security boundaries. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83e66da3-8e67-49bb-bada-5da0edf25532": {"__data__": {"id_": "83e66da3-8e67-49bb-bada-5da0edf25532", "embedding": null, "metadata": {"bbox": [{"page": 350, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.42, "x1": 432.01, "y1": 376.47}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "76e01180-cd69-4c8e-9e36-55d224684a59", "node_type": "1", "metadata": {"bbox": [{"page": 349, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.23, "x1": 432.0, "y1": 251.54}]}, "hash": "c29bd8d15d07a88ed8c19bc97116f89f0e4c13f6e7b1422adf448fc381e586ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1aa94f3-5b60-4654-8af4-aa483b434a49", "node_type": "1", "metadata": {"bbox": [{"page": 351, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.03, "x1": 432.0, "y1": 243.94}]}, "hash": "9d5a10d5801507deb85f63c94e3fb2f072b515e2b4927a8f761348fb26e1e29e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nIn their paper on CodeT5, Wang et al. (2021) designed a transformer model that \nleverages code semantics, especially identifiers, to do well at code-understanding and \ngeneration tasks. DeepMind\u2019s AlphaCode, OpenAI\u2019s Codex, and open-source models \nlike StarCoder have advanced the field in practical ways. Hyunh and Lin (2025) \nsurveyed LLMs for code generation. HumanEval by Chen et al. (2021) has become a \nstandard benchmark for assessing code generation capabilities. <br><br>Claude employs Mermaid code as an intermediate format to produce architecture \ndiagrams and flowcharts. Gemini generates Pandas code to perform financial analy\u2010 \nsis. <br><br>**Pattern 23: Multiagent Collaboration**\nMultiagent architectures allow you to solve real-world problems by using specialized \nsingle-purpose agents and to organize them in ways that mimic human organiza\u2010 \ntional structures. They can help you go beyond the limitations of a single LLM call. <br><br>**Problem**\nPatterns 21 (Tool Calling), 22 (Code Execution), 13 (CoT), and 18 (Reflection) \nallow you to go beyond building passive applications that simply respond to user \nqueries. AI agents\u2019 ability to take actions, respond to feedback, and adapt their \nbehavior enables them to become more autonomous. However, being barely able to ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1aa94f3-5b60-4654-8af4-aa483b434a49": {"__data__": {"id_": "e1aa94f3-5b60-4654-8af4-aa483b434a49", "embedding": null, "metadata": {"bbox": [{"page": 351, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.03, "x1": 432.0, "y1": 243.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "83e66da3-8e67-49bb-bada-5da0edf25532", "node_type": "1", "metadata": {"bbox": [{"page": 350, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 77.42, "x1": 432.01, "y1": 376.47}]}, "hash": "4655c48aa240d8184213afa7b95dbd7678271c2359ac2dc1c7e2adc827929ffa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "186050ab-6594-46c2-a7b5-68653138ad2a", "node_type": "1", "metadata": {"bbox": [{"page": 352, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.7, "x1": 432.01, "y1": 258.0}]}, "hash": "cc915c16b7a7576d63f9078371d8ae55bd1dc7be8bdf094d18096e9b7522ae8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Problems with domain adaptation <br><br>Single models trained on general data distributions may lack specialized expertise \nin particular domains. Fine-tuning can partially address this limitation, but often \nat the cost of catastrophic forgetting or reduced performance in other domains. <br><br>Multiagent systems can help address these limitations. <br><br>**Solution**\nMultiagent systems implement a division of cognitive labor that mirrors human \norganizational structures and allows for more efficient allocation of model capacity. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "186050ab-6594-46c2-a7b5-68653138ad2a": {"__data__": {"id_": "186050ab-6594-46c2-a7b5-68653138ad2a", "embedding": null, "metadata": {"bbox": [{"page": 352, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.7, "x1": 432.01, "y1": 258.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e1aa94f3-5b60-4654-8af4-aa483b434a49", "node_type": "1", "metadata": {"bbox": [{"page": 351, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.03, "x1": 432.0, "y1": 243.94}]}, "hash": "9d5a10d5801507deb85f63c94e3fb2f072b515e2b4927a8f761348fb26e1e29e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "152b64b6-2de6-481a-b735-9e252d568a6a", "node_type": "1", "metadata": {"bbox": [{"page": 353, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.09, "x1": 432.01, "y1": 314.69}]}, "hash": "57360aa7a8bd87ef4437a6f843b6202f918f14f3c867c64b095872ef2d13f123", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multiagent architectures offer better scalability than monolithic approaches do. \nRather than scaling vertically by increasing model size, multiagent systems can \nscale horizontally by adding more specialized agents as needed. You can update or \nreplace individual agents without having to retrain the entire system, and that allows \nfor more agile development and deployment. Computational resources can also be \nallocated dynamically based on current needs, with more resources directed to the \nmost critical or computationally intensive subtasks. <br><br>Multiagent systems are also more robust. Critical capabilities can be replicated across \nmultiple agents to reduce the impact of individual agent failures. Different agents \ncan approach problems with different methodologies, which increases the likelihood \nof finding effective solutions. Agents can verify each other\u2019s outputs to identify and \ncorrect errors that might go undetected in a single-agent system. <br><br>Recent research indicates that multiagent systems can exhibit emergent capabilities, \nwhich is behavior for which they were not explicitly trained. Through interaction and ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "152b64b6-2de6-481a-b735-9e252d568a6a": {"__data__": {"id_": "152b64b6-2de6-481a-b735-9e252d568a6a", "embedding": null, "metadata": {"bbox": [{"page": 353, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.09, "x1": 432.01, "y1": 314.69}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "186050ab-6594-46c2-a7b5-68653138ad2a", "node_type": "1", "metadata": {"bbox": [{"page": 352, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.7, "x1": 432.01, "y1": 258.0}]}, "hash": "cc915c16b7a7576d63f9078371d8ae55bd1dc7be8bdf094d18096e9b7522ae8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29d93411-773a-481e-8772-dcfe093691ef", "node_type": "1", "metadata": {"bbox": [{"page": 354, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.3, "x1": 1385.25, "y1": 303.74}, {"page": 355, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.9, "x1": 488.5, "y1": 346.6}]}, "hash": "eba152eec5cd5982d99a780ce4f3059329527450b9de4cac80870b22e630408a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Hierarchical structures.**Hierarchical multiagent architectures organize agents in a tree\u2010 \nlike structure with clear lines of authority and responsibility. In an executive-worker \nmodel, for instance, a high-level executive agent decomposes tasks, delegates them to \nspecialized worker agents, and then integrates their outputs into a coherent response. \nIn multilevel hierarchies, there are multiple layers of executive-worker hierarchies, \nwith midlevel managers coordinating groups of specialized agents. <br><br>Hierarchical structures typically centralize their decision-making, with higher-level \nagents having authority over lower-level agents. To use such a structure, you\u2019ll typi\u2010 \ncally need a task decomposition algorithm, some sort of priority-based scheduling, \nand a way to aggregate outputs from different agents. <br><br>The simplest hierarchical architecture is prompt chaining, in which the response of \nthe first agent in the chain becomes the input to the second agent, and so on. This \nis also called a sequential workflow, which is a hierarchy where there is only one leaf \nnode and all the other nodes have exactly one child. All inputs go to the root node, \nand the final response comes from the leaf node. <br><br>For example, here\u2019s a sequential workflow in LangChain that consists of three steps. \nGiven a topic, the chain is structured to generate a paragraph on the topic, give it a \ntitle, and then extract keywords from the paragraph: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29d93411-773a-481e-8772-dcfe093691ef": {"__data__": {"id_": "29d93411-773a-481e-8772-dcfe093691ef", "embedding": null, "metadata": {"bbox": [{"page": 354, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.3, "x1": 1385.25, "y1": 303.74}, {"page": 355, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.9, "x1": 488.5, "y1": 346.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "152b64b6-2de6-481a-b735-9e252d568a6a", "node_type": "1", "metadata": {"bbox": [{"page": 353, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.09, "x1": 432.01, "y1": 314.69}]}, "hash": "57360aa7a8bd87ef4437a6f843b6202f918f14f3c867c64b095872ef2d13f123", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5572ad3c-c278-4fe4-a883-6f9dbc9d38ba", "node_type": "1", "metadata": {"bbox": [{"page": 356, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 96.18, "x1": 446.0, "y1": 351.65}]}, "hash": "581081dba6cce6cdfeb7e1d0d5148ff9b6acf679df8cb27347934fd059668119", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this workflow, each agent collaborates with agents up and down its hierarchy \nthrough input_variables and output_key. <br><br>**Peer-to-peer networks.**Peer-to-peer architectures distribute authority and responsibil\u2010 \nity more evenly across agents. In collaborative networks, agents work together as \nequals, sharing information and coordinating activities without rigid hierarchical \ncontrol. There is usually a voting or consensus mechanism to decide on a course \nof action, so tasks are addressed through collective effort rather than top-down \nassignment. <br><br>For example, here\u2019s a CrewAI task that involves getting consensus from three agents: <br><br>*# peer-to-peer task*\nvoting_and_consensus_task = Task( \n description=(\"Review the preliminary recommendations from all editors for article '{article_id}'. Engage in up to 3 rounds of discussion to reach a consensus on whether to ACCEPT, REJECT, or REVISE the article. If a consensus is not reached after 3 rounds, a majority vote will determine the outcome.\" \n ), \n expected_output=(\"The final decision (ACCEPT, REJECT, or REVISE) for the article, along with a summary of the reviews. This output should reflect the consensus or majority vote after discussion.\" \n ), \n agent=[senior_editor, content_editor, research_editor],*# All agents participate in this task*\n context=[senior_editor_review_task, content_editor_review_task, research_editor_review_task], <br><br>*# Determine the highest bidder*\n highest_bid = 0 \n winner =**None**\n**for**agent_name, bid_amount**in**bids.items(): \n**if**bid_amount > highest_bid: \n highest_bid = bid_amount \n winner = agent_name <br><br>**return**winner, highest_bid <br><br>The use of auction mechanisms is not limited to buying used cars. You can use \nan auction mechanism for any kind of task assignment in which each agent can \nindependently determine how well it can do the task with the resources it currently \nhas. Suppose you have a set of agents, each of which uses a different algorithm \nto solve an optimization task. Each agent might bid for the task based on the antici\u2010 \npated improvement or based on the anticipated optimization time. Here\u2019s a more \ncomplex English auction, where the auctioneer sets a minimum price, bidders openly \nannounce successively higher bids, and agents drop out once their resources are \nexhausted: <br><br>**def**run_auction(agents, task_description, starting_bid, increment=100): \n current_bid = starting_bid \n highest_bidder =**None**\n active_bidders = list(agents)*# All agents start as active*\n**while**len(active_bidders) > 1: \n print(f\"**\\n**Current Bid is: ${current_bid:,}. Bidders remaining: {len(active_bidders)}\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5572ad3c-c278-4fe4-a883-6f9dbc9d38ba": {"__data__": {"id_": "5572ad3c-c278-4fe4-a883-6f9dbc9d38ba", "embedding": null, "metadata": {"bbox": [{"page": 356, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 96.18, "x1": 446.0, "y1": 351.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "29d93411-773a-481e-8772-dcfe093691ef", "node_type": "1", "metadata": {"bbox": [{"page": 354, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 87.3, "x1": 1385.25, "y1": 303.74}, {"page": 355, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.9, "x1": 488.5, "y1": 346.6}]}, "hash": "eba152eec5cd5982d99a780ce4f3059329527450b9de4cac80870b22e630408a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "778289c5-46ff-4e3c-82fb-de2bd109c098", "node_type": "1", "metadata": {"bbox": [{"page": 357, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.74, "x1": 432.01, "y1": 232.84}]}, "hash": "88253931095efee37382ef891230bace749210ddcf3e3db508183b6500885bbf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*# Get ready for next round*\n round_winner_name = max(bids_this_round, key=bids_this_round.get) \n current_bid = bids_this_round[round_winner_name] \n highest_bidder = round_winner_name <br><br>*# Remove bidders who passed*\n**for**agent**in**bidders_to_remove: \n active_bidders.remove(agent) <br><br>**if**highest_bidder: \n print(f\"The auction is over! The last bidder standing is {highest_bidder}.\") \n**return**highest_bidder, current_bid \n**else**: \n*# This would happen if no one were to bid above the starting price.*\n print(\"The auction ended with no bids.\") \n**return None**, starting_bid <br><br>Market-based architectures require auction algorithms, utility functions, and/or some \nsort of virtual currency for resource allocation. <br><br>**Human-in-the-loop.**Peer-to-peer and market-based systems require mechanisms to \nresolve conflicts. The simplest is to make one of the agents a proxy for a human and \nhave it pass the conflict to that human, who resolves the conflict and allows the agent \nmechanism to proceed. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "778289c5-46ff-4e3c-82fb-de2bd109c098": {"__data__": {"id_": "778289c5-46ff-4e3c-82fb-de2bd109c098", "embedding": null, "metadata": {"bbox": [{"page": 357, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.74, "x1": 432.01, "y1": 232.84}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5572ad3c-c278-4fe4-a883-6f9dbc9d38ba", "node_type": "1", "metadata": {"bbox": [{"page": 356, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 96.18, "x1": 446.0, "y1": 351.65}]}, "hash": "581081dba6cce6cdfeb7e1d0d5148ff9b6acf679df8cb27347934fd059668119", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0a4f0a5-39fe-4e83-a779-269db96baff6", "node_type": "1", "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 342.84, "x1": 432.0, "y1": 605.54}]}, "hash": "3195e403687ff2df71625b54d4d29413d106bcd7713e23827d9679ad4d01c4b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Collaborative content creation <br><br>Content creation benefits from specialized agents focusing on different aspects \nof the content. Different agents can specialize in research, outlining, drafting, \nediting, and fact-checking, thus working together to produce high-quality writ\u2010 \nten content. Agents that are specialized in text, image, audio, and video genera\u2010 \ntion can collaborate to create integrated multimedia content. Different agents \ncan represent different characters or narrative elements to create dynamic and \nresponsive storytelling experiences. <br><br>Adversarial verification <br><br>Multiagent systems can implement adversarial approaches to verification. One \nset of agents (the red team) attempts to find flaws, biases, or vulnerabilities \nin content, while another set of agents (the blue team) defends or iteratively ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0a4f0a5-39fe-4e83-a779-269db96baff6": {"__data__": {"id_": "a0a4f0a5-39fe-4e83-a779-269db96baff6", "embedding": null, "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 342.84, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "778289c5-46ff-4e3c-82fb-de2bd109c098", "node_type": "1", "metadata": {"bbox": [{"page": 357, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.74, "x1": 432.01, "y1": 232.84}]}, "hash": "88253931095efee37382ef891230bace749210ddcf3e3db508183b6500885bbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3d19d2e-b0b7-4a47-9e78-7a773358262a", "node_type": "1", "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 320.03}]}, "hash": "dded578ad93345ec814c3d320525d4710f0d32e053c57ca71fc0fff57aa5312a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assume that you\u2019re in charge of producing ninth-grade workbooks on different topics \nand that you\u2019re going to use an agentic approach to create that content. You\u2019ll follow \nthese steps (the numbers correspond to those in Figure 7-6 and are referenced in the \nfollowing subsections): <br><br>1. \n1. The workflow starts when a human user sends a topic to the Task Assigner agent. <br><br>2. The Task Assigner assigns the topic to one of the agents on your content writing \n2. \nteam. For example, the topic will be assigned to an agent trained on historical \ntexts if the topic is \u201cWhy was the Battle of Plassey so pivotal?\u201d. <br><br>3. \n3. The history content-writing agent generates an initial draft, using tools such as <br><br>fact checkers and textbooks (using RAG). <br><br>4. \n4. The draft is sent to a review panel consisting of several agents playing different <br><br>roles. <br><br>5. \n5. The review panel discusses the draft, providing different perspectives. <br><br>6. \n6. The panel secretary summarizes the panel\u2019s feedback into a tangible set of direc\u2010 <br><br>tives. <br><br>7. \n7. The feedback is provided to the history content\u2013writing agent. <br><br>8. \n8. The writing agent uses the feedback to rewrite the initial draft and submits the <br><br>result as the final content. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3d19d2e-b0b7-4a47-9e78-7a773358262a": {"__data__": {"id_": "b3d19d2e-b0b7-4a47-9e78-7a773358262a", "embedding": null, "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 320.03}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a0a4f0a5-39fe-4e83-a779-269db96baff6", "node_type": "1", "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 342.84, "x1": 432.0, "y1": 605.54}]}, "hash": "3195e403687ff2df71625b54d4d29413d106bcd7713e23827d9679ad4d01c4b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90cb922f-bcab-450d-ba72-bda575bb213a", "node_type": "1", "metadata": {"bbox": [{"page": 361, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.58, "x1": 432.0, "y1": 299.85}]}, "hash": "19c8e77e7013176432026784cb97e6df393965ba7a38c9702c89865077d4c74e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 1: Setting up the workflow**<br><br>The initial part of the workflow is a hierarchical structure\u2014all requests flow to the \nTask Assigner, which chooses one of the writers. This pattern, in which a group of \nworkers is fronted by a classifier, is called a router. <br><br>There are four agents: the human proxy that submits the topic to the Task Assigner, \nthe Task Assigner, and two writers. In AG2, you can create the two writers and the \nhuman proxy with the following code: <br><br>llm_config = LLMConfig( \n api_type=\"google\",*# or \"openai\"*\n model=\"gemini-2.0-flash\",*# or \"gpt-4o-mini\"*\n api_key=os.environ.get(\"GEMINI_API_KEY\"),*# or \"OPENAI_API_KEY\"*\n temperature=0.2, \n) \nhistory_writer_prompt=\"You are a historian ...\" \nmath_writer_prompt=\"You are a math teacher ...\" \n**with**llm_config: \n history_writer = ConversableAgent(name=\"history_writer\", \n system_message=history_writer_prompt) \n math_writer = ConversableAgent(name=\"math_writer\", \n system_message=math_writer_prompt) \n human = ConversableAgent(name=\"human\", human_input_mode=\"ALWAYS\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90cb922f-bcab-450d-ba72-bda575bb213a": {"__data__": {"id_": "90cb922f-bcab-450d-ba72-bda575bb213a", "embedding": null, "metadata": {"bbox": [{"page": 361, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.58, "x1": 432.0, "y1": 299.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b3d19d2e-b0b7-4a47-9e78-7a773358262a", "node_type": "1", "metadata": {"bbox": [{"page": 360, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 320.03}]}, "hash": "dded578ad93345ec814c3d320525d4710f0d32e053c57ca71fc0fff57aa5312a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d87ceba1-1e4d-44fe-8c4a-406d00706124", "node_type": "1", "metadata": {"bbox": [{"page": 363, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.41, "x1": 432.0, "y1": 268.92}]}, "hash": "f90e327395216b5d86b5ec733dfb7fb5ddc8763b7b7d3d2da381a59e33b95e58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, based on the response, you choose the writer agent that will create the initial \ndraft for this question: <br><br>writer = json.loads(task_response.messages[-1]['content'])['writer'] \n**if**writer == 'HISTORIAN': \n writer = history_writer \n**else**: \n writer = math_writer <br><br>**Step 3: Creating the initial draft**<br><br>To create the initial draft, ask the Task Assigner to pose the original question to the \nwriter agent: <br><br>content_response = task_assigner.run(recipient=writer, \n message=question, \n max_turns=1) \ncontent_response.process() \ninitial_draft = content_response.messages[-1]['content'] <br><br>Here\u2019s the question: <br><br>Why was the Battle of Plassey so pivotal? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d87ceba1-1e4d-44fe-8c4a-406d00706124": {"__data__": {"id_": "d87ceba1-1e4d-44fe-8c4a-406d00706124", "embedding": null, "metadata": {"bbox": [{"page": 363, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.41, "x1": 432.0, "y1": 268.92}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "90cb922f-bcab-450d-ba72-bda575bb213a", "node_type": "1", "metadata": {"bbox": [{"page": 361, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.58, "x1": 432.0, "y1": 299.85}]}, "hash": "19c8e77e7013176432026784cb97e6df393965ba7a38c9702c89865077d4c74e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00ef839a-3ab4-460e-b8e5-d9f455a0a3e9", "node_type": "1", "metadata": {"bbox": [{"page": 364, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.43, "x1": 432.0, "y1": 351.6}]}, "hash": "be78a1f77c02c763eca055df4d442824000103d7b231461185e9a2169c5dba63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "reviews, context, last_agent = initiate_group_chat( \n pattern=pattern, \n max_rounds=len(reviewers)+1,*# everyone speaks once*\n messages=f\"\"\" \n You are part of a review panel ... \n {question} \n ... \n {answer} \n \"\"\" \n) <br><br>**Step 5: Having the review panel discussion**<br><br>Based on the question and the article about the Battle of Plassey, each of the reviewers \nprovides feedback from its perspective and responds to others\u2019 feedback. <br><br>The district administrator says this, in part: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00ef839a-3ab4-460e-b8e5-d9f455a0a3e9": {"__data__": {"id_": "00ef839a-3ab4-460e-b8e5-d9f455a0a3e9", "embedding": null, "metadata": {"bbox": [{"page": 364, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.43, "x1": 432.0, "y1": 351.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d87ceba1-1e4d-44fe-8c4a-406d00706124", "node_type": "1", "metadata": {"bbox": [{"page": 363, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 94.41, "x1": 432.0, "y1": 268.92}]}, "hash": "f90e327395216b5d86b5ec733dfb7fb5ddc8763b7b7d3d2da381a59e33b95e58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c68f481-dba2-4b86-9843-cda6bd230dff", "node_type": "1", "metadata": {"bbox": [{"page": 365, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.27, "x1": 432.0, "y1": 413.48}]}, "hash": "790bd29bcc1bf55e469ad770e8b8018c9bd048842d1201dd5b9a4eadabfa8e66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Step 7: Supplying feedback to the history-writing agent**<br><br>To have the secretary agent supply the feedback to the writing agent, you can use the \nfollowing: <br><br>rewrite_response = last_agent.run(recipient=history_writer, max_turns=2, \n message=f\"\"\" \nPlease incorporate the feedback from a review panel to \nupdate content you wrote for a 9th-grade workbook. <br><br>You were asked to write the answer for this question: \n{question} <br><br>You wrote the following: \n{answer} <br><br>The reviewer panel has provided the following feedback: \n{reviews.chat_history[-1]['content']} <br><br>Incorporate the feedback to rewrite the content.\"\"\") <br><br>Setting max_turns=2 means that there can be one round of clarifying questions. <br><br>**Step 8: Generating the final content**<br><br>The writing agent now generates the final content, which provides a clearer and more \nbalanced answer based on the previous feedback: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c68f481-dba2-4b86-9843-cda6bd230dff": {"__data__": {"id_": "4c68f481-dba2-4b86-9843-cda6bd230dff", "embedding": null, "metadata": {"bbox": [{"page": 365, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.27, "x1": 432.0, "y1": 413.48}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "00ef839a-3ab4-460e-b8e5-d9f455a0a3e9", "node_type": "1", "metadata": {"bbox": [{"page": 364, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.43, "x1": 432.0, "y1": 351.6}]}, "hash": "be78a1f77c02c763eca055df4d442824000103d7b231461185e9a2169c5dba63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7919dc62-f635-4486-845f-26fbb3e54d54", "node_type": "1", "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 255.65, "x1": 423.75, "y1": 512.02}]}, "hash": "369773d65bc3fa38216cdcbcdb3c2d334efeae9ccdd9888bec6f41f3836892b1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The secretary responds with feedback based on the review panel\u2019s discussions (which \nare part of its memory): <br><br>**Combine the last two sentences:**This creates a stronger concluding statement, \nemphasizing the long-term impact. <br><br>The final response from the history-writing agent now ends with this: <br><br>Because of the Battle of Plassey, the British gained a crucial foothold and the \nresources necessary to expand their influence in India. This battle had long-lasting \nconsequences, shaping the region\u2019s economy and society for generations to come. <br><br>**Considerations**\nAt the time of writing, significant challenges remain in implementing and evaluating \nmultiagent systems, including computational overhead, consistency maintenance, \ncompounding errors, and unpredictable emergent behaviors. An influential article \nfrom Anthropic recommends using simple, composable patterns rather than complex \nframeworks to implement these types of systems. <br><br>If possible, we recommend that you implement agents that are peers so that they \ncan operate in parallel. This reduces the \u201cwall-clock\u201d time associated with performing \na task, although the overall computational overhead remains the same. Paralleliz\u2010 \ning agent operations and performing agent operations asynchronously can reduce \nlatency. <br><br>The overhead of interagent communication can be significant, particularly in sys\u2010 \ntems with many agents or complex interaction patterns. The computational cost of \ncoordination increases nonlinearly with the number of agents, potentially limiting \nscalability. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7919dc62-f635-4486-845f-26fbb3e54d54": {"__data__": {"id_": "7919dc62-f635-4486-845f-26fbb3e54d54", "embedding": null, "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 255.65, "x1": 423.75, "y1": 512.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4c68f481-dba2-4b86-9843-cda6bd230dff", "node_type": "1", "metadata": {"bbox": [{"page": 365, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 78.27, "x1": 432.0, "y1": 413.48}]}, "hash": "790bd29bcc1bf55e469ad770e8b8018c9bd048842d1201dd5b9a4eadabfa8e66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37605183-27c4-4691-99bf-2bac4417d33b", "node_type": "1", "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 97.25, "y0": 131.45, "x1": 353.25, "y1": 243.75}]}, "hash": "52d8ddf079c8c6cf511d68ddc8e590da4278b1f9d78bf95902cc3a6c9c62eb32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The Agent2Agent Protocol Simplifies Multiagent Communication**\nThe complexity associated with multiagent communication is decreasing as the \nAgent2Agent (A2A) protocol is increasingly adopted. This allows agents that were \nwritten in different frameworks and that run on different machines to communicate \nseamlessly. <br><br>For example, if you have a Python agent that was built using the Pydantic framework, \nyou can make it available through A2A: <br><br>**from pydantic_ai import**Agent \nagent = Agent('openai:gpt-4.1', ...) \n... \napp = agent.to_a2a() <br><br>You can then run the application itself in an ASGI server such as Uvicorn to make it \navailable at a web URL: <br><br>uvicorn agent_to_a2a:app --host 0.0.0.0 --port 8093 <br><br>A front-end developer can invoke this Python agent from a TypeScript agent that was \nwritten using the Mastra framework by simply supplying its URL: <br><br>**import**{ A2A } from \"@mastra/client-js\"; \n*// Access the Python agent through its A2A web URL*\n**const**a2a =**new**A2A({ serverUrl: \"https://...server.com:8093\" }); ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37605183-27c4-4691-99bf-2bac4417d33b": {"__data__": {"id_": "37605183-27c4-4691-99bf-2bac4417d33b", "embedding": null, "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 97.25, "y0": 131.45, "x1": 353.25, "y1": 243.75}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7919dc62-f635-4486-845f-26fbb3e54d54", "node_type": "1", "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 80.25, "y0": 255.65, "x1": 423.75, "y1": 512.02}]}, "hash": "369773d65bc3fa38216cdcbcdb3c2d334efeae9ccdd9888bec6f41f3836892b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e03d29-d56f-4462-a8cb-c63d24c63901", "node_type": "1", "metadata": {"bbox": [{"page": 367, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.64, "x1": 432.01, "y1": 257.54}]}, "hash": "ddf555f8c835d8668747229d57a3aaee2a39a3172ae79167ca4812b7f950c8c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**const**task = await a2a.sendTask({ \n id: randomUUID(), \n message: { \n role: 'user', \n parts: [...] \n } \n}); <br><br>**const**stream = a2a.streamTaskUpdates(task.id, (update) => { \n console.log(\"Task update:\", update); \n}); ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99e03d29-d56f-4462-a8cb-c63d24c63901": {"__data__": {"id_": "99e03d29-d56f-4462-a8cb-c63d24c63901", "embedding": null, "metadata": {"bbox": [{"page": 367, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.64, "x1": 432.01, "y1": 257.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "37605183-27c4-4691-99bf-2bac4417d33b", "node_type": "1", "metadata": {"bbox": [{"page": 366, "page_height": 661.5, "page_width": 504.0, "x0": 97.25, "y0": 131.45, "x1": 353.25, "y1": 243.75}]}, "hash": "52d8ddf079c8c6cf511d68ddc8e590da4278b1f9d78bf95902cc3a6c9c62eb32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "409b493f-7693-4cf5-a8b3-734125930ee1", "node_type": "1", "metadata": {"bbox": [{"page": 368, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.67, "x1": 432.01, "y1": 391.91}]}, "hash": "2e045da88072cf42a2a933eed833691a880f28bc29eccd270dedfb9e7fbd0946", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Task verification <br><br>These failures are related to inadequate verification processes that fail to detect \nor correct errors or premature termination of tasks. They highlight challenges in \nensuring the final output\u2019s correctness and reliability. <br><br>Making improvements in these areas isn\u2019t always enough to achieve high reliability. \nThis indicates that more fundamental changes to agent organization, communication \nprotocols, context management, and verification integration are required. Given this, \nif you can employ UX design, introduce a human-in-the-loop, or set customer \nexpectations so that a single-agent system suffices, then we recommend that you \nchoose one of those approaches instead. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "409b493f-7693-4cf5-a8b3-734125930ee1": {"__data__": {"id_": "409b493f-7693-4cf5-a8b3-734125930ee1", "embedding": null, "metadata": {"bbox": [{"page": 368, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.67, "x1": 432.01, "y1": 391.91}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "99e03d29-d56f-4462-a8cb-c63d24c63901", "node_type": "1", "metadata": {"bbox": [{"page": 367, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 127.64, "x1": 432.01, "y1": 257.54}]}, "hash": "ddf555f8c835d8668747229d57a3aaee2a39a3172ae79167ca4812b7f950c8c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8111fa1e-282a-4aeb-ad76-325b5957aa0b", "node_type": "1", "metadata": {"bbox": [{"page": 369, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 507.2, "x1": 220.2, "y1": 604.6}]}, "hash": "90f8fdbcc99ddb33fed69085768c3cc1d0236befe632a893d251783949d2bc62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Summary**\nThe three patterns discussed in this chapter and summarized in Table 7-1 represent \nthe frontier of AI capabilities. They transform models from passive information \nprocessors into active participants that can access external tools, execute code, and \ncollaborate in teams to achieve sophisticated outcomes that would be impossible for a \nsingle agent working in isolation. <br><br>Table 7-1. Patterns for Enabling Action <br><br>**Patterns**\n**Tool Calling**\n**(Pattern 21)**<br><br>**Problems**\nHow can you bridge the LLM \nand a software API so that the \nLLM can invoke the API and get \nthe job done? <br><br>**Solutions**\nThe LLM emits special tokens when \nit determines that a function needs \nto be called, and it also emits the \nparameters to pass to that function. A \nclient-side postprocessor invokes the \nfunction with those parameters, and it \nsends the results back to the LLM. The \nLLM then incorporates the function \nresults into its response. <br><br>**Usage scenarios**\nWhenever you want the \nLLM to not just state \nthe steps needed but also \nexecute those steps. This also \nallows you to incorporate \nup-to-date knowledge from \nreal-time sources, connect \nto transactional enterprise \nsystems, perform calculations, \nand use optimization solvers. \nCreating graphs, annotating \nimages, and updating \ndatabases. \n**Code Execution**\n**(Pattern 22)**\nYou have a software system \nthat can do the task, but \ninvoking it involves a DSL. \nLLMs generate code that an external \nsystem then executes. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8111fa1e-282a-4aeb-ad76-325b5957aa0b": {"__data__": {"id_": "8111fa1e-282a-4aeb-ad76-325b5957aa0b", "embedding": null, "metadata": {"bbox": [{"page": 369, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 507.2, "x1": 220.2, "y1": 604.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "409b493f-7693-4cf5-a8b3-734125930ee1", "node_type": "1", "metadata": {"bbox": [{"page": 368, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 118.67, "x1": 432.01, "y1": 391.91}]}, "hash": "2e045da88072cf42a2a933eed833691a880f28bc29eccd270dedfb9e7fbd0946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "017a8080-0874-485c-8c38-3963e7facec5", "node_type": "1", "metadata": {"bbox": [{"page": 370, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.87, "x1": 432.01, "y1": 582.33}]}, "hash": "db8304c3fe3d52f8f8cc2ebdec571db4c0eebd7fb32d7d505f16cfb85a7fb332", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Patterns**\n**Multiagent**\n**Collaboration**\n**(Pattern 23)**<br><br>**Problems**\nYour model needs to handle \nmultistep tasks that require \ndifferent tools, maintain \ncontent over extended \ninteractions, evaluate situations \nand take appropriate actions \nwithout human intervention, \nand adapt to user preferences. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "017a8080-0874-485c-8c38-3963e7facec5": {"__data__": {"id_": "017a8080-0874-485c-8c38-3963e7facec5", "embedding": null, "metadata": {"bbox": [{"page": 370, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.87, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8111fa1e-282a-4aeb-ad76-325b5957aa0b", "node_type": "1", "metadata": {"bbox": [{"page": 369, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 507.2, "x1": 220.2, "y1": 604.6}]}, "hash": "90f8fdbcc99ddb33fed69085768c3cc1d0236befe632a893d251783949d2bc62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9da0fe77-879c-42b5-87e9-3c8a9369b93e", "node_type": "1", "metadata": {"bbox": [{"page": 371, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.72, "x1": 497.01, "y1": 304.5}]}, "hash": "b353321e4600538b966a7f46fd2de8b0bc9ff3a5a26a4545217f5e999859cf30", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 8**\n**Addressing Constraints**<br><br>Deploying LLMs in production environments presents a unique set of challenges \nthat go far beyond simply getting a model to work. While LLMs offer remarkable \ncapabilities, they also demand substantial computational resources, introduce latency \nconcerns, and can quickly become cost prohibitive at scale. The gap between a proof- \nof-concept that works on a single query and a production system serving thousands \nof users is often overlooked. <br><br>In this chapter, we provide patterns that address concerns you\u2019re likely to face when \ndeploying LLMs in production systems. Whether you\u2019re facing hardware limitations, \nbudget constraints, or strict latency requirements, the patterns presented here offer \nproven strategies for optimizing your LLM deployment. <br><br>We\u2019ll explore five key patterns that tackle different aspects of production constraints. \nThe section on the Small Language Model (Pattern 24) shows you how to reduce \ncomputational overhead through model distillation and quantization techniques. The \nsection on Prompt Caching (Pattern 25) demonstrates how to eliminate redundant \nprocessing and reduce both costs and latency. The section on Optimizing Inference \n(Pattern 26) covers advanced techniques like continuous batching and speculative \ndecoding to maximize hardware utilization. The section on Degradation Testing (Pat\u2010 \ntern 27) provides the metrics you need to validate that your LLM-based application \nis performing well, and it also covers actions that you can take if it\u2019s falling short in \nsome aspect of performance. Finally, the section on Long-Term Memory (Pattern 28) \nhelps you maintain user history between sessions and remember user requests for \npersonalization. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9da0fe77-879c-42b5-87e9-3c8a9369b93e": {"__data__": {"id_": "9da0fe77-879c-42b5-87e9-3c8a9369b93e", "embedding": null, "metadata": {"bbox": [{"page": 371, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.72, "x1": 497.01, "y1": 304.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "017a8080-0874-485c-8c38-3963e7facec5", "node_type": "1", "metadata": {"bbox": [{"page": 370, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 129.87, "x1": 432.01, "y1": 582.33}]}, "hash": "db8304c3fe3d52f8f8cc2ebdec571db4c0eebd7fb32d7d505f16cfb85a7fb332", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c71edb0b-fb2a-4534-90de-9ae86f9431b4", "node_type": "1", "metadata": {"bbox": [{"page": 373, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.2, "x1": 432.01, "y1": 288.3}]}, "hash": "6606cf2eb8816baa62bde967ed0f35e1d781158570f3a3a57c9e98734c83d638", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**GPU requirements**<br><br>LLMs can require multiple state-of-the-art GPUs to run inference. For example, \nthe Llama 4 Scout model by Meta requires 4 \u00d7 H 100 GPUs with 80 GB. The \nsingle-instance price point at the time of writing (June 2025) can be above $10/hour. <br><br>Couldn\u2019t you just use a smaller model? If the task is easy, perhaps. However, smaller \nmodels often struggle with complex tasks. For example, suppose you ask a model to \ngenerate documentation for some Python code: <br><br>*# Complex prompt that might challenge smaller models*\nprompt = f\"\"\"Below is a Python code snippet. Please generate comprehensive documentation for it. <br><br>Code: \n{code_example} <br><br>Documentation:\"\"\" <br><br>The Gemma 3 27B model does well on this task, annotating the code with the \ndocumentation: <br><br>*\"\"\"*\n*This module provides a simple task management system.*", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c71edb0b-fb2a-4534-90de-9ae86f9431b4": {"__data__": {"id_": "c71edb0b-fb2a-4534-90de-9ae86f9431b4", "embedding": null, "metadata": {"bbox": [{"page": 373, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.2, "x1": 432.01, "y1": 288.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9da0fe77-879c-42b5-87e9-3c8a9369b93e", "node_type": "1", "metadata": {"bbox": [{"page": 371, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.72, "x1": 497.01, "y1": 304.5}]}, "hash": "b353321e4600538b966a7f46fd2de8b0bc9ff3a5a26a4545217f5e999859cf30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5d13e85-bf66-4ccb-846b-bc7092de000e", "node_type": "1", "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.91, "x1": 432.01, "y1": 605.54}]}, "hash": "b7b45686dd820f8252767387c4af39df22f1bde3dc34c3b4277d036d015fb97b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is there a way to use a SLM without compromising on response quality? <br><br>**Solution**\nYou\u2019ll often prototype with a frontier foundational model and then try to replace \nthe large foundational model with a smaller one. There are three options for going \nfrom a large foundational model to a smaller one without compromising unduly on \nresponse quality. Option 1, distillation, reduces the size of the model by narrowing \nits knowledge scope\u2014the smaller model does only the tasks required by your applica\u2010 \ntion. Option 2, quantization, compromises a tiny bit of model accuracy by reducing \nthe precision of the model parameters to consume less memory. Option 3, speculative \ndecoding, uses a smaller and a larger model in tandem\u2014this reduces latency without \ncompromising accuracy, but at the expense of complexity. <br><br>Another alternative to reduce model latency is model pruning. However, we won\u2019t \ndiscuss pruning in this chapter because it typically produces lower-quality results \nthan the three options covered in this pattern. Unlike the alternatives demonstrated \nhere, pruning significantly impacts the LLM\u2019s performance. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5d13e85-bf66-4ccb-846b-bc7092de000e": {"__data__": {"id_": "a5d13e85-bf66-4ccb-846b-bc7092de000e", "embedding": null, "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.91, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c71edb0b-fb2a-4534-90de-9ae86f9431b4", "node_type": "1", "metadata": {"bbox": [{"page": 373, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.2, "x1": 432.01, "y1": 288.3}]}, "hash": "6606cf2eb8816baa62bde967ed0f35e1d781158570f3a3a57c9e98734c83d638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "411b3211-e640-40f2-93c9-93ddd2a96699", "node_type": "1", "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 100.4, "x1": 432.01, "y1": 349.1}]}, "hash": "4c71c7a4a43a03f8b7a6a693f0967f04db53167529d51a84e7c7df6b75802438", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8-1 shows how fine-tuning an SLM works. The input data is used to prompt \nboth models for a response, and instead of providing fixed responses, you use the \ntokens generated by the teacher model as the expected output for the smaller model. \nDuring the training, you can vary the larger model\u2019s temperature parameter to con\u2010 \ntrol the generated responses\u2019 probability distribution. <br><br>**The training process.**In this section, we highlight key parts of the training process\u2014 \nplease refer to the full example in our Github repository for context. <br><br>In the training process, you distill a larger language model into a smaller one under \nthe assumption that the smaller one should generate the same response as the larger \none. Then, you update the smaller model based on the difference between the respon\u2010 \nses of the smaller and larger models. <br><br>First, you need to generate a model response from the larger \u201cteacher\u201d model. For \nthis, you can focus on the forward pass by using torch.no_grad() because you don\u2019t \nneed to calculate any gradients or update the larger model. Using torch.no_grad() \nwill save resources and speed up the inference: <br><br>**with**torch.no_grad(): \n teacher_outputs = self.teacher_model(**inputs) \n teacher_logits = teacher_outputs.logits ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "411b3211-e640-40f2-93c9-93ddd2a96699": {"__data__": {"id_": "411b3211-e640-40f2-93c9-93ddd2a96699", "embedding": null, "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 100.4, "x1": 432.01, "y1": 349.1}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a5d13e85-bf66-4ccb-846b-bc7092de000e", "node_type": "1", "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 357.91, "x1": 432.01, "y1": 605.54}]}, "hash": "b7b45686dd820f8252767387c4af39df22f1bde3dc34c3b4277d036d015fb97b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa01f046-2894-414b-8934-c00584b6c791", "node_type": "1", "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 200.69, "x1": 432.01, "y1": 439.64}]}, "hash": "364b44e5418b7c8566b121b9b3bc6a025355ece1ac807a1646e974e7d4df21bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once you have the expected output from the teacher model, you can generate the \nresponse from the student model: <br><br>student_outputs = model(**inputs) \nstudent_logits = student_outputs.logits <br><br>Next, update the student model to mimic the teacher model. The first step is to \ncapture the standard language modeling loss for the student model: <br><br>task_loss = student_outputs.loss <br><br>After capturing the initial student loss, you need to apply the temperature scaling \nbefore calculating the distillation loss. You want to scale the logits to make the \nprobability distributions of both models more comparable, and you can do this by \ndividing the logits by the temperature. A lower temperature will concentrate around \nhigh confidence predictions, while a higher temperature will flatten the probabilities. \nScaling will reduce large mismatches between the teacher and student outputs and \navoid sharp update swings: <br><br>student_logits = student_logits / self.temperature \nteacher_logits = teacher_logits / self.temperature <br><br>With the scaled logits, you can now measure the similarity between the probability \ndistributions of the student and teacher models. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa01f046-2894-414b-8934-c00584b6c791": {"__data__": {"id_": "aa01f046-2894-414b-8934-c00584b6c791", "embedding": null, "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 200.69, "x1": 432.01, "y1": 439.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "411b3211-e640-40f2-93c9-93ddd2a96699", "node_type": "1", "metadata": {"bbox": [{"page": 375, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 100.4, "x1": 432.01, "y1": 349.1}]}, "hash": "4c71c7a4a43a03f8b7a6a693f0967f04db53167529d51a84e7c7df6b75802438", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05d539ea-44d1-46c1-a2e7-b021b3ae14ca", "node_type": "1", "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.7, "x1": 432.01, "y1": 192.84}]}, "hash": "397e823abb100ee88fd8b8d928217c6776cb445fc9dbb221ae3b3e5e0abd228e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first term (entropy) is computed using log probabilities of P, and the second \nterm (cross-entropy) uses probabilities from Q. The log_softmax term provides \nthe log-probabilities of P (the student\u2019s distribution), which makes this calculation \nefficient. <br><br>You need to calculate the log probabilities for the student by using torch.log_soft \nmax(student_logits), while the teacher\u2019s logit probability distribution is calcula\u2010 \nted based on torch.softmax(teacher_logits).The 'batchmean' averages the loss \nacross the entire batch during the training. Finally, the divergence loss is then scaled \nby the square of the temperature, and the temperature scaling softens the probability \ndistribution. This prevents the model from becoming too confident. For example, \nif the model predicts one token close to 1 and all other tokens near zero, the \ntemperature scaling will dampen the probability values to 0.6, 0.2, 0.1, and so on. \nThis dampening preserves the teacher model\u2019s \u201cdark knowledge\u201d of alternative tokens \nand enriches the learning signal for the student model. <br><br>distillation_loss = torch.nn.functional.kl_div( \n torch.log_softmax(student_logits, dim=-1), \n torch.softmax(teacher_logits, dim=-1), \n reduction='batchmean' \n)*(self.temperature**2) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05d539ea-44d1-46c1-a2e7-b021b3ae14ca": {"__data__": {"id_": "05d539ea-44d1-46c1-a2e7-b021b3ae14ca", "embedding": null, "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.7, "x1": 432.01, "y1": 192.84}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "aa01f046-2894-414b-8934-c00584b6c791", "node_type": "1", "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 200.69, "x1": 432.01, "y1": 439.64}]}, "hash": "364b44e5418b7c8566b121b9b3bc6a025355ece1ac807a1646e974e7d4df21bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daab348c-23b6-498b-a7c6-2a873bd96d63", "node_type": "1", "metadata": {"bbox": [{"page": 377, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.44, "x1": 432.01, "y1": 312.58}]}, "hash": "d18baefba5e261b482bad3ae509039ac724c1656ffce7b50665c6c4fafbea74d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, you\u2019ll use a softmax for the teacher model to make sure it is a valid \nprobability distribution (that is, that the sum of values equals 1), which aligns with \nthe mathematical definition of KL divergence. <br><br>As the last step, you can now combine the two losses: the loss from the student\u2019s \nperformance of the task and the student\u2019s ability to mimic the teacher (this is the \ndistillation_loss). <br><br>loss = (1 - self.alpha)*task_loss + self.alpha*distillation_loss ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "daab348c-23b6-498b-a7c6-2a873bd96d63": {"__data__": {"id_": "daab348c-23b6-498b-a7c6-2a873bd96d63", "embedding": null, "metadata": {"bbox": [{"page": 377, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.44, "x1": 432.01, "y1": 312.58}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "05d539ea-44d1-46c1-a2e7-b021b3ae14ca", "node_type": "1", "metadata": {"bbox": [{"page": 376, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 95.7, "x1": 432.01, "y1": 192.84}]}, "hash": "397e823abb100ee88fd8b8d928217c6776cb445fc9dbb221ae3b3e5e0abd228e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1568921c-912f-4f8c-884a-e38c8d51b90a", "node_type": "1", "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 292.84, "x1": 432.01, "y1": 605.54}]}, "hash": "0f9c32dae4a43f0851902e1855d28bceb4a0ff433a76a81c5ddd1aa16ce0d930", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Option 2: Model quantization**<br><br>LLMs typically store weights as 32-bit floating-point numbers (FP32) and therefore \nconsume significant memory. For example, a 70 B-parameter model in FP32 requires \napproximately 280 GB of memory just to store the weights. This precision level, while \nmathematically convenient during training, is often unnecessary for inference and \ncreates substantial memory overhead that limits deployment options. <br><br>**The concept.**Full-precision computation is resource-intensive, and that leads to \nslower matrix multiplications. This computation slowness then leads to higher token \ngeneration latency and reduced throughput for multirequest serving scenarios. <br><br>Unlike distillation, which sacrifices some of the model\u2019s knowledge breadth for \nefficiency, quantization maintains nearly all the knowledge and capabilities of the \noriginal model while addressing hardware constraints. By reducing precision (such \nas by using 8-bit or even 4-bit integers) as shown in Figure 8-2, the model\u2019s memory \nfootprint shrinks significantly. For example, an FP32 weight takes 4 bytes while an \nINT8 weight only takes 1 byte. However, the lower memory footprint comes with a \ntrade-off: lower precision means less memory and faster computations but potential \nloss of accuracy. But the accuracy degradation due to quantization tends not to be ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1568921c-912f-4f8c-884a-e38c8d51b90a": {"__data__": {"id_": "1568921c-912f-4f8c-884a-e38c8d51b90a", "embedding": null, "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 292.84, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "daab348c-23b6-498b-a7c6-2a873bd96d63", "node_type": "1", "metadata": {"bbox": [{"page": 377, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 76.44, "x1": 432.01, "y1": 312.58}]}, "hash": "d18baefba5e261b482bad3ae509039ac724c1656ffce7b50665c6c4fafbea74d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb5e56c2-b605-427f-9cd7-608d82422d44", "node_type": "1", "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.78, "x1": 432.01, "y1": 284.74}]}, "hash": "a96767811af060544023015e10a59f975ef8a145949c47bb28ba95c0c63dc2ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stability. Two frameworks that support mixed-precision training are Microsoft\u2019s \nDeepSpeed and NVIDIA\u2019s Megatron-LM. <br><br>Dynamic quantization adjusts the quantization parameters during training based \non activation statistics. This helps the model adapt to changing distributions \nthroughout the training process. This means that highly important activations \nare less compressed, while less essential activations are quantized to a higher \ndegree. PyTorch has offered great support for dynamic quantization for several \nyears. <br><br>Post-training quantization <br><br>If you have a fully trained float 32 model, you can quantize it in one of three \nways: weight-only quantization, full-model quantization, and using advanced \npost-training techniques. <br><br>Weight-only quantization quantizes only the model weights while keeping activa\u2010 \ntions at a higher precision. This means that the learnable parameters (weights) \nare stored at a lower precision but the computed values that flow through the \nnetwork (activations) are computed at a high precision like float 32. Standard \ntechniques are GPT quantization (GPTQ) and activation-aware weight quantiza\u2010 \ntion (AWQ). <br><br>Full-model quantization quantizes both weights and activations, offering maxi\u2010 \nmum efficiency gains but potentially more quality degradation. The quantization \ncan be done statically, using a calibration dataset to determine the quantization \nparameters, or dynamically, by computing the quantization parameters on the fly \nduring inference. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb5e56c2-b605-427f-9cd7-608d82422d44": {"__data__": {"id_": "bb5e56c2-b605-427f-9cd7-608d82422d44", "embedding": null, "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.78, "x1": 432.01, "y1": 284.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1568921c-912f-4f8c-884a-e38c8d51b90a", "node_type": "1", "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 292.84, "x1": 432.01, "y1": 605.54}]}, "hash": "0f9c32dae4a43f0851902e1855d28bceb4a0ff433a76a81c5ddd1aa16ce0d930", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87760bb4-e9dc-4161-a20a-f5437f573091", "node_type": "1", "metadata": {"bbox": [{"page": 380, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.27, "x1": 432.0, "y1": 372.77}]}, "hash": "9fccb5be64fb5a4b163323ec0a23b403130f51e46402da82a7bce4cadfd0e8f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More advanced post-training quantization techniques include the following: <br><br>Quantized low-rank adaptation (QLoRA) <br><br>This builds on top of the LoRA concept, where you can fine-tune an adapter \nthat works with the original base model. QLoRA can be used to efficiently \nfine-tune already quantized models. <br><br>Sparse-quantized representation (SPQR) <br><br>This combines quantization and making weights sparse. <br><br>BitNet <br><br>This is an extremely low-bit approach that pushes quantization to binary \nor ternary representations. Microsoft has published a 1-bit quantization for \nmodels. <br><br>**The method.**You can use the BitsAndBytesConfig Python library to perform post- \ntraining quantization. This library allows you to reduce memory usage while main\u2010 \ntaining reasonable performance. You can find a complete example in the Github ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87760bb4-e9dc-4161-a20a-f5437f573091": {"__data__": {"id_": "87760bb4-e9dc-4161-a20a-f5437f573091", "embedding": null, "metadata": {"bbox": [{"page": 380, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.27, "x1": 432.0, "y1": 372.77}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bb5e56c2-b605-427f-9cd7-608d82422d44", "node_type": "1", "metadata": {"bbox": [{"page": 379, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 77.78, "x1": 432.01, "y1": 284.74}]}, "hash": "a96767811af060544023015e10a59f975ef8a145949c47bb28ba95c0c63dc2ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "209a3e40-b0e1-4ea9-abdd-e4e86f6b3763", "node_type": "1", "metadata": {"bbox": [{"page": 381, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.6, "x1": 432.01, "y1": 309.4}]}, "hash": "d7879fae9b2f753f6dbc42e6a57cd31ece3a2be98fe8de641930d3e5c79b968a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once you\u2019ve defined the configuration, you can load the original base model by using \nHugging Face\u2019s AutoModelForCausalLM class: <br><br>quantized_model = AutoModelForCausalLM.from_pretrained( \n model_name, \n quantization_config=quantization_config, \n device_map=\"auto\", \n torch_dtype=torch.float16, \n token=hf_token \n) <br><br>You can then use the model like any other Hugging Face model\u2014for inference locally, \nor you can save and deploy it: <br><br>outputs = quantized_model.generate( \n**inputs, \n max_length=max_length, \n num_return_sequences=1, \n temperature=0.7, \n do_sample=**True**\n) <br><br>If you have decided to host your own model rather than use a full-resolution frontier \nmodel, it\u2019s worth checking whether a 4-bit version of that model would lose much in \nthe way of accuracy. If not, you can save considerably on cost and latency. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "209a3e40-b0e1-4ea9-abdd-e4e86f6b3763": {"__data__": {"id_": "209a3e40-b0e1-4ea9-abdd-e4e86f6b3763", "embedding": null, "metadata": {"bbox": [{"page": 381, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.6, "x1": 432.01, "y1": 309.4}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "87760bb4-e9dc-4161-a20a-f5437f573091", "node_type": "1", "metadata": {"bbox": [{"page": 380, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 119.27, "x1": 432.0, "y1": 372.77}]}, "hash": "9fccb5be64fb5a4b163323ec0a23b403130f51e46402da82a7bce4cadfd0e8f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09aa26d9-a6fd-4eb5-b87c-b233257b9689", "node_type": "1", "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 168.9, "x1": 432.01, "y1": 606.65}]}, "hash": "2eb7a510e1bdc8743dce923ece5aebb43c48f4dda21a8d206f3a5f17f3b0ecb9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Why it works.**The fundamental principle behind speculative decoding is that not all \ntokens require the computational power of a large model for accurate generation. \nToken difficulty varies significantly\u2014simple, predictable tokens like common words \nor obvious completions can be reliably generated by the smaller student model, while \ncomplex or context-dependent tokens benefit from the teacher model\u2019s advanced \ncapabilities. This selective use of computational resources allows for significant speed \nimprovements while maintaining the quality standards of the larger model. The \napproach is particularly effective because it balances the trade-off between speed and \naccuracy by dynamically choosing the appropriate model based on the complexity of \nthe current generation task (see Figure 8-3). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09aa26d9-a6fd-4eb5-b87c-b233257b9689": {"__data__": {"id_": "09aa26d9-a6fd-4eb5-b87c-b233257b9689", "embedding": null, "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 168.9, "x1": 432.01, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "209a3e40-b0e1-4ea9-abdd-e4e86f6b3763", "node_type": "1", "metadata": {"bbox": [{"page": 381, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.6, "x1": 432.01, "y1": 309.4}]}, "hash": "d7879fae9b2f753f6dbc42e6a57cd31ece3a2be98fe8de641930d3e5c79b968a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d9d9298-2ef5-4516-9af6-c36d844d24db", "node_type": "1", "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 87.3, "x1": 293.0, "y1": 157.0}]}, "hash": "a689cea7a537c5e34b58b8f3e654f31eeb7b16ef6a72d7e70b818a3c843abeb0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Step 1: \nStudent: \"The [talented] [chef]\" \nTeacher: \u2713 Accepts (common phrase) <br><br>Step 2: \nStudent: \"cooked [a] [delicious]\" \nTeacher: \u2713 Accepts (common food context) <br><br>Step 3: \nStudent: \"[soup]\" \nTeacher: \u2717 Rejects \nTeacher generates: \"bouillabaisse\" (rare, specific word) <br><br>Step 4: \nStudent: \"[for] [dinner]\" \nTeacher: \u2713 Accepts (common ending) <br><br>You achieve the inference speed increases for two reasons. First of all, you generate \nthe proposal tokens through the smaller LLM. In addition, you can generate multiple \ntokens at once. You can request multiple tokens since you have a second model to \nvalidate the predictions, and you can afford it because the initial token predictions \nare fast and cheap. Secondly, the validation of the prediction is also fast, and you only \nneed to correct the predictions for tokens where the smaller model made a mistake. <br><br>**The method.**Many LLM deployment frameworks support speculative decoding in \none form or another. For our core example (see GitHub), we\u2019re demonstrating specu\u2010 \nlative decoding with vLLM, which is a frequently used framework for serving LLM \nmodels like Llama-3.2-3B. In our example, we used a smaller model to predict the \nnext tokens and a larger model to validate the prediction and, if needed, correct \nthe prediction. We used Google\u2019s Gemma2 2B model to predict the next tokens and \nthe larger Gemma2 9B model to validate the prediction and, if needed, correct the \nprediction. Sampling the tokens, checking them with the larger model, and correcting \nthem is done under the hood by the LLM serving framework, which in our case is \nvLLM: <br><br>**from vllm import**LLM, SamplingParams \nprompts = [ \n \"The future of AI is\", \n] \nsampling_params = SamplingParams(temperature=0.8, top_p=0.95) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d9d9298-2ef5-4516-9af6-c36d844d24db": {"__data__": {"id_": "7d9d9298-2ef5-4516-9af6-c36d844d24db", "embedding": null, "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 87.3, "x1": 293.0, "y1": 157.0}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "09aa26d9-a6fd-4eb5-b87c-b233257b9689", "node_type": "1", "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 168.9, "x1": 432.01, "y1": 606.65}]}, "hash": "2eb7a510e1bdc8743dce923ece5aebb43c48f4dda21a8d206f3a5f17f3b0ecb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe5b7378-fead-44f5-959c-3a4e195cc181", "node_type": "1", "metadata": {"bbox": [{"page": 384, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.9, "x1": 414.0, "y1": 169.71}]}, "hash": "dcbafd4ab9ee58e046e479a07f25fe65feed5a5b2650dae0aac173b0dcea94f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "llm = LLM( \n model=\"google/gemma-2-9b-it\", \n tensor_parallel_size=1, \n speculative_model=\"google/gemma-2-2b-it\", \n num_speculative_tokens=5, \n) \noutputs = llm.generate(prompts, sampling_params) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe5b7378-fead-44f5-959c-3a4e195cc181": {"__data__": {"id_": "fe5b7378-fead-44f5-959c-3a4e195cc181", "embedding": null, "metadata": {"bbox": [{"page": 384, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.9, "x1": 414.0, "y1": 169.71}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7d9d9298-2ef5-4516-9af6-c36d844d24db", "node_type": "1", "metadata": {"bbox": [{"page": 383, "page_height": 661.5, "page_width": 504.0, "x0": 89.0, "y0": 87.3, "x1": 293.0, "y1": 157.0}]}, "hash": "a689cea7a537c5e34b58b8f3e654f31eeb7b16ef6a72d7e70b818a3c843abeb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e937e334-b291-4e0f-b7a4-a71e9671b2c6", "node_type": "1", "metadata": {"bbox": [{"page": 385, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.69, "x1": 432.0, "y1": 368.04}]}, "hash": "45fa0b3de8c02571f88c172878458eea0d2d4d8f5d505e828ed8166233af1a5a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, it will respond with this: <br><br>This code defines a `TaskManager` class that manages a collection of `Task` objects. \nIt provides methods for adding, retrieving, deleting, and listing tasks. The `Task` class \nrepresents a single task with its ID, title, description, completion status, and a list of \ntags. <br><br>... ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e937e334-b291-4e0f-b7a4-a71e9671b2c6": {"__data__": {"id_": "e937e334-b291-4e0f-b7a4-a71e9671b2c6", "embedding": null, "metadata": {"bbox": [{"page": 385, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.69, "x1": 432.0, "y1": 368.04}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fe5b7378-fead-44f5-959c-3a4e195cc181", "node_type": "1", "metadata": {"bbox": [{"page": 384, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 92.9, "x1": 414.0, "y1": 169.71}]}, "hash": "dcbafd4ab9ee58e046e479a07f25fe65feed5a5b2650dae0aac173b0dcea94f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f2ebf6-bb0b-46cd-a1d8-8511d0564c56", "node_type": "1", "metadata": {"bbox": [{"page": 386, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 333.23}]}, "hash": "dde95f354a7335bce0c3da1595fefcb37fccf387653348a1770ae76b5519f348", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our Github repository, we provide you with a script that will generate the training \nset for our example. The script will also generate Python code examples for the \ndistillation. <br><br>$ generate_training_set.py <br><br>If you want to reuse this example for your specific business problem, the downstream \nimplementation expects the following data structure: <br><br>{ \n \"examples\": [ \n \"\"\"import json \n import logging \n ... \n \"\"\", \n ... \n ], \n \"metadata\": { \n \"num_examples\": 1000, \n \"generated_at\": \"2025-05-30 09:58:41\", \n \"model_used\": \"claude-3-7-sonnet-20250219\" \n } \n} <br><br>**Distilling Gemma.**We used Gemma 3 12B as the teacher and Gemma 3 1B as the \nstudent. The teacher is reasonably good at the task. The training requires GPUs with \na large memory because both the teacher and the student model need to be held in ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98f2ebf6-bb0b-46cd-a1d8-8511d0564c56": {"__data__": {"id_": "98f2ebf6-bb0b-46cd-a1d8-8511d0564c56", "embedding": null, "metadata": {"bbox": [{"page": 386, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 333.23}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e937e334-b291-4e0f-b7a4-a71e9671b2c6", "node_type": "1", "metadata": {"bbox": [{"page": 385, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 86.69, "x1": 432.0, "y1": 368.04}]}, "hash": "45fa0b3de8c02571f88c172878458eea0d2d4d8f5d505e828ed8166233af1a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5acdcca-6a13-4cd4-9166-d7ac1a9c8f28", "node_type": "1", "metadata": {"bbox": [{"page": 387, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.74, "x1": 432.01, "y1": 344.03}]}, "hash": "491a4581fee55aae7784a5e497cd8bf25911c5bb4829a1de51d784cdb845a7bf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Quantization**<br><br>After creating a distilled Gemma 3 1B model, we quantized it from 32-bit floating \npoint weights to 4-bit integer weights. The output looks identical to the larger model \noutput, but the inference is much faster: only 19 seconds, compared to several \nminutes for the 32-bit model. <br><br>The full example, which is available in our Github repository, shows the combined \ndistillation and quantization of the Gemma 3 model. <br><br>**Considerations**\nIn production systems, the Small Language Model pattern, which consists of distilla\u2010 \ntion on a narrow range of tasks and/or quantization, can help you meet cost and/or \nlatency constraints. However, model distillation and quantization come with several \nlimitations, so it may be worth considering other alternatives. <br><br>**Limitations**<br><br>In the case of model distillation, you need to consider the loss of generality. Student \nmodels often excel only in their trained tasks and sacrifice the teacher\u2019s broader \ncapabilities. That\u2019s an acceptable limitation if your distilled model is used in a \ndomain-specific application. However, if you use the distilled model in a general ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5acdcca-6a13-4cd4-9166-d7ac1a9c8f28": {"__data__": {"id_": "e5acdcca-6a13-4cd4-9166-d7ac1a9c8f28", "embedding": null, "metadata": {"bbox": [{"page": 387, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.74, "x1": 432.01, "y1": 344.03}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98f2ebf6-bb0b-46cd-a1d8-8511d0564c56", "node_type": "1", "metadata": {"bbox": [{"page": 386, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.44, "x1": 432.01, "y1": 333.23}]}, "hash": "dde95f354a7335bce0c3da1595fefcb37fccf387653348a1770ae76b5519f348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58026760-9a14-444a-85aa-45a9aae1c390", "node_type": "1", "metadata": {"bbox": [{"page": 388, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.23, "x1": 432.01, "y1": 360.74}]}, "hash": "9635ca81f9ad093d5170a80b6dcdb1cd64396c7eff1080ba8c4d485fa8008c2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Alternatives**<br><br>Distillation, quantization, and speculative decoding can be used individually or in \ncombination. <br><br>If you don\u2019t care about cost, only latency, consider model sharding, in which the \nmodel server divides a model among several devices. This distributes the compu\u2010 \ntational workload among different GPUs, which can lower latency without losing \naccuracy, as you would with a smaller model. <br><br>Parallelization presents another powerful strategy for improving the performance \nof larger LLMs. Instead of processing multiple requests sequentially, you can pro\u2010 \ncess them simultaneously, which significantly decreases the effective latency across \nmultiple requests. This approach particularly shines in high-traffic scenarios where \nindividual requests use only a fraction of the model\u2019s context length. However, \nparallelization has clear limitations: it remains constrained by both the model\u2019s \nmaximum context length and the available GPU memory. Despite these constraints, \nparallelization often provides substantial performance benefits for many production \ndeployments. Consider it first when you\u2019re optimizing deployment latency. <br><br>Continuous batching (see the section on Inference Optimization, Pattern 26) takes the \nparallelization concept even further. Instead of processing fixed batches, this techni\u2010 \nque dynamically pulls new requests from a queue whenever space becomes available \nin the current batch. This approach proves especially effective when handling a high ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58026760-9a14-444a-85aa-45a9aae1c390": {"__data__": {"id_": "58026760-9a14-444a-85aa-45a9aae1c390", "embedding": null, "metadata": {"bbox": [{"page": 388, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.23, "x1": 432.01, "y1": 360.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e5acdcca-6a13-4cd4-9166-d7ac1a9c8f28", "node_type": "1", "metadata": {"bbox": [{"page": 387, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.74, "x1": 432.01, "y1": 344.03}]}, "hash": "491a4581fee55aae7784a5e497cd8bf25911c5bb4829a1de51d784cdb845a7bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10980c55-07b3-4091-ba5a-859dd76552ae", "node_type": "1", "metadata": {"bbox": [{"page": 389, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.49, "x1": 432.01, "y1": 307.2}]}, "hash": "95f8042e9eca94323d454992bce97eb0198d91b2842e02f1a5d634455811ec15", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If your application is domain specific, Pattern 15, Adapter Tuning (from Chapter 5) \ncan be a good alternative. The model will have a smaller memory footprint and \nprovide lower latencies, but it comes with additional fine-tuning complexity. <br><br>**References**\nKnowledge distillation was introduced in Hinton, Vinyals, and Dean\u2019s seminal 2015 \npaper \u201cDistilling the Knowledge in a Neural Network.\u201d The field of LLM quantization \nwas significantly advanced by GPTQ for accurate post-training quantization and \nAWQ for activation-aware weight quantization, with QLoRA demonstrating how to \ncombine quantization with parameter-efficient fine-tuning. Microsoft\u2019s BitNet papers \npushed quantization to its extreme, showing that 1-bit and 1.58-bit transformers \ncould maintain competitive performance while dramatically reducing computational \nrequirements. Speculative decoding was introduced by Leviathan, Kalman, and Mat\u2010 \nias (2022), who showed that it could accelerate off-the-shelf models without retrain\u2010 \ning or architecture changes. <br><br>The broader landscape of inference optimizations was captured by Xia et al. (2024) \nin their comprehensive survey \u201cUnlocking Efficiency in Large Language Model Infer\u2010 \nence.\u201d <br><br>Amazon Pharmacy employs fine-tuned SLMs to perform pharmalexical normaliza\u2010 \ntion (to standardize drug names and dosage information). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10980c55-07b3-4091-ba5a-859dd76552ae": {"__data__": {"id_": "10980c55-07b3-4091-ba5a-859dd76552ae", "embedding": null, "metadata": {"bbox": [{"page": 389, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.49, "x1": 432.01, "y1": 307.2}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "58026760-9a14-444a-85aa-45a9aae1c390", "node_type": "1", "metadata": {"bbox": [{"page": 388, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.23, "x1": 432.01, "y1": 360.74}]}, "hash": "9635ca81f9ad093d5170a80b6dcdb1cd64396c7eff1080ba8c4d485fa8008c2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b000c5c3-cbbd-4f9d-9465-c93e4e15cae9", "node_type": "1", "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 320.34, "x1": 531.0, "y1": 606.65}]}, "hash": "c9b54abd6202d0ac6db299c67a1a4632e407563956723d9ec5900305975c94ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "User time <br><br>You can improve your application\u2019s UX by short-circuiting request generation \nfor prompts that it has seen before by reusing previously computed responses. \nReuse cuts down the average wait time, which is valuable in interactive applica\u2010 \ntions where users expect fast responses. In a user session that involves multiple \nrequests, returning some responses quicker will reduce the total time that it takes \nthe user to resolve their issue. <br><br>Hardware costs <br><br>If your system is recomputing requests, you may be procuring more hardware\u2014 \nsuch as GPUs or Tensor Processing Units (TPUs)\u2014than your application actually \nneeds. <br><br>Is there a way to avoid wasteful recomputation? <br><br>**Solution**\nYou can cache LLM responses to repetitive requests. You don\u2019t even need the whole \nrequest to repeat\u2014Prompt Caching can work even if only part of the request is \nrepeated. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b000c5c3-cbbd-4f9d-9465-c93e4e15cae9": {"__data__": {"id_": "b000c5c3-cbbd-4f9d-9465-c93e4e15cae9", "embedding": null, "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 320.34, "x1": 531.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "10980c55-07b3-4091-ba5a-859dd76552ae", "node_type": "1", "metadata": {"bbox": [{"page": 389, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.49, "x1": 432.01, "y1": 307.2}]}, "hash": "95f8042e9eca94323d454992bce97eb0198d91b2842e02f1a5d634455811ec15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "842a92dc-b7c6-4730-bdf6-06a781779d33", "node_type": "1", "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 161.74, "x1": 432.0, "y1": 312.24}]}, "hash": "86a2218dbe700b205fc2762bb6f298ba07f3123eb2b887941481d0bf4ee99d01", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "prompt_template = PromptTemplate.from_template( \n*\"\"\"*\n*What are the steps to put a freeze on my credit card account?*\n*\"\"\"*\n) \nchain = prompt_template | model | parser <br><br>Besides this in-memory cache, LangChain supports other caching stores, such as \nRedis and Cassandra. Regardless of the cache store you pick, the setup is the same: \npick your cache and then call set_llm_cache. Every time you invoke the LLM from \nyour code, LangChain will check the cache before firing your request to the LLM. <br><br>**Model provider client-side caching.**Model providers like OpenAI provide the option to \ncache client-side as well. If you set the OPENAI_CACHE_DIR environmental variable \nwith a directory path, then the client will cache previously generated responses in this \ndirectory. You can find an example and a speed comparison in our Github repository. <br><br>os.environ[\"OPENAI_CACHE_DIR\"] = \"./oai_cache\" \nresponse = openai.chat.completions.create( \n model=\"gpt-3.5-turbo\", \n messages=[{\"role\": \"user\", \"content\": \"What is the capital of Monaco? Provide a detailed answer.\"}], \n) <br><br>**Semantic caching.**The downside of response caching is that it requires an exact text \nmatch. To avoid this, you can use a semantic cache (see Figure 8-5), where the cache is \nsearched for queries that are similar, not only exactly the same. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "842a92dc-b7c6-4730-bdf6-06a781779d33": {"__data__": {"id_": "842a92dc-b7c6-4730-bdf6-06a781779d33", "embedding": null, "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 161.74, "x1": 432.0, "y1": 312.24}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b000c5c3-cbbd-4f9d-9465-c93e4e15cae9", "node_type": "1", "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 320.34, "x1": 531.0, "y1": 606.65}]}, "hash": "c9b54abd6202d0ac6db299c67a1a4632e407563956723d9ec5900305975c94ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a6b86b-fca0-4bb8-b1f2-b7881005466c", "node_type": "1", "metadata": {"bbox": [{"page": 393, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 244.14, "x1": 432.01, "y1": 606.65}]}, "hash": "e027110ef2e1878e4dcef3f1cbd117c517f0e17a3a4f32b4931d44afe40ab10a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are a few different ways to accomplish this: <br><br>Using a canonical form of the key <br><br>To increase the hit rate against the cache, you can use a canonical representation \nof the request as the key by replacing synonyms and removing phrases that \ndon\u2019t change the meaning of the request. You can use NLP operations such as \nstemming and text normalization to create such a canonical key. <br><br>Using multiple keys for the same response <br><br>You can also implement a semantic cache by generating semantically similar \nrequests for the initial request and storing the response against all these similar \nkeys (see Figure 8-5). This way, you increase the odds of a cache hit on subse\u2010 \nquent, similar queries. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6a6b86b-fca0-4bb8-b1f2-b7881005466c": {"__data__": {"id_": "e6a6b86b-fca0-4bb8-b1f2-b7881005466c", "embedding": null, "metadata": {"bbox": [{"page": 393, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 244.14, "x1": 432.01, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "842a92dc-b7c6-4730-bdf6-06a781779d33", "node_type": "1", "metadata": {"bbox": [{"page": 391, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 161.74, "x1": 432.0, "y1": 312.24}]}, "hash": "86a2218dbe700b205fc2762bb6f298ba07f3123eb2b887941481d0bf4ee99d01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3efc200d-28ed-44e9-8cc1-b2c4fb61c336", "node_type": "1", "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.16, "x1": 432.0, "y1": 605.54}]}, "hash": "fe5ae48738579c19b042c7a554d8a680a2c0a8a9ee3f5d2bd305fe9fa2f753b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " similarity_evaluation=SearchDistanceEvaluation(), \n ) \ncache.set_openai_key() <br><br>The problem with this approach is that you need to find a similarity threshold \nthat hits the desired balance between false positives during cache hits and false \nnegatives during cache misses. <br><br>The downside of semantic caching is that users might get the same response to \nslightly different queries, and nuances can be lost. The system might seem less \n\u201csmart\u201d if it continuously responds with the same response. <br><br>**Server-Side Prompt Caching**<br><br>Server-side caching takes a different approach than client-side caching. Instead of \ncaching entire requests, server-side caching provides prefix caching, in which the \nLLM provider stores common parts of a prompt (which are often extensive system \nprompts or examples) and reuses previously computed internal model states for those \ncommon parts. That allows subsequent prompts that use the same prefix to load an \ninitial state for the model generation from the cache and skip the redundant initial \ncomputation. Because it loads the initial state (shown in Figure 8-6), it has two major \nbenefits: <br><br>\u2022 It doesn\u2019t affect the creativity of the model. Each generation is generated by the \n\u2022 \nLLM, and only the initial state of the LLM is reused. No precomputed responses \nare used, unlike in client-side caching. <br><br>\u2022 Reusing the initial, internal states of the LLM doesn\u2019t affect the overall latency too \n\u2022 \nmuch, but because it reloads the initial state, the time to first token (TTFT) met\u2010 \nric is greatly reduced. This is particularly beneficial for streaming applications \nlike chats, where users wait for the response stream to start. Work by Anthropic \nhas shown some great results in reducing TTFT. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3efc200d-28ed-44e9-8cc1-b2c4fb61c336": {"__data__": {"id_": "3efc200d-28ed-44e9-8cc1-b2c4fb61c336", "embedding": null, "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.16, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e6a6b86b-fca0-4bb8-b1f2-b7881005466c", "node_type": "1", "metadata": {"bbox": [{"page": 393, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 244.14, "x1": 432.01, "y1": 606.65}]}, "hash": "e027110ef2e1878e4dcef3f1cbd117c517f0e17a3a4f32b4931d44afe40ab10a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "503e7bce-80da-4b28-854f-f7653d96c676", "node_type": "1", "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.77, "x1": 432.0, "y1": 316.26}]}, "hash": "7ea015225abadf7fb9beb859cdcea13fbdbc62bea056cb0d9d1990b5ac9112f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this example, we\u2019re capturing the model\u2019s response for a given prompt and storing \nthe response on disk in JSON format. Instead of using a KV database like Redis, we \nhash the prompt and use the hash as the file name for our JSON data structures, \nwhich contain the previously generated response: <br><br>**class PromptCache**: \n**def** __init__(self, cache_dir: str = \".prompt_cache\"): \n self.cache_dir = Path(cache_dir) \n self.cache_dir.mkdir(exist_ok=**True**) \n self.client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\")) <br><br>**def**_get_cache_key(self, prompt: str) -> str: \n*\"\"\"Generate a unique cache key for the prompt.\"\"\"*\n**return**hashlib.md5(prompt.encode()).hexdigest() <br><br>**def**_get_cache_path(self, cache_key: str) -> Path: \n*\"\"\"Get the path to the cache file.\"\"\"*\n**return**self.cache_dir / f\"{cache_key}.json\" <br><br>For every request, you need to check for a previously generated response for exactly \nthe same prompt. If the generated hash for the request prompt matches a JSON file \non disk, you can load the response from disk; otherwise, it will return None. <br><br>**def**get_cached_response(self, prompt: str) -> Optional[Dict[str, Any]]: \n*\"\"\"Retrieve a cached response if it exists.\"\"\"*\n cache_key = self._get_cache_key(prompt) \n cache_path = self._get_cache_path(cache_key) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "503e7bce-80da-4b28-854f-f7653d96c676": {"__data__": {"id_": "503e7bce-80da-4b28-854f-f7653d96c676", "embedding": null, "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.77, "x1": 432.0, "y1": 316.26}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3efc200d-28ed-44e9-8cc1-b2c4fb61c336", "node_type": "1", "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 328.16, "x1": 432.0, "y1": 605.54}]}, "hash": "fe5ae48738579c19b042c7a554d8a680a2c0a8a9ee3f5d2bd305fe9fa2f753b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b5dc7d0-c856-4cd3-86f7-27e1b22982ec", "node_type": "1", "metadata": {"bbox": [{"page": 396, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.6, "x1": 432.01, "y1": 222.74}]}, "hash": "21a2457a58ab06777945119bb78af76895eeb9c73fe214fa2b33506f707b4584", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**if**cache_path.exists(): \n**with**open(cache_path, 'r')**as**f: \n**return**json.load(f) \n**return None**<br><br>If the return value from get_cached_response isn\u2019t None, you can bypass the LLM \ngeneration and immediately return the previously generated response: <br><br>cached_response = self.get_cached_response(prompt) \n**if**cached_response: \n**return**cached_response[\"content\"] <br><br>For new requests, you need to generate the LLM response once, as usual. In this \nexample, we use Anthropic\u2019s Claude, but the implementation works with every LLM. <br><br>Once you get the response back from the LLM, you can store it on disk using the \ncache_response function so it will be available for future requests: <br><br>response = self.client.messages.create( \n model=\"claude-3-7-sonnet-20250219\", \n max_tokens=1000, \n messages=[{\"role\": \"user\", \"content\": prompt}] \n) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b5dc7d0-c856-4cd3-86f7-27e1b22982ec": {"__data__": {"id_": "8b5dc7d0-c856-4cd3-86f7-27e1b22982ec", "embedding": null, "metadata": {"bbox": [{"page": 396, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.6, "x1": 432.01, "y1": 222.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "503e7bce-80da-4b28-854f-f7653d96c676", "node_type": "1", "metadata": {"bbox": [{"page": 395, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 88.77, "x1": 432.0, "y1": 316.26}]}, "hash": "7ea015225abadf7fb9beb859cdcea13fbdbc62bea056cb0d9d1990b5ac9112f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10937da5-037e-4fa2-9538-13bfccf6cb6c", "node_type": "1", "metadata": {"bbox": [{"page": 397, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.41, "x1": 432.0, "y1": 229.52}]}, "hash": "077545a71b8ff7f532aac1a9c525e700b6c1dae091b744955d3584e4bc8237d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The LLM returns alternative requests. You can now store the semantic versions \ntogether with the response in the cache and save them for future lookups that match \none of the semantic versions: <br><br>semantic_requests = self._get_semantic_requests(prompt) \nall_requests = [prompt] + semantic_requests <br><br>**for**request**in**all_requests: \n self.cache_response(request, response_dict) <br><br>For example, imagine a user submits the following prompt: <br><br>Prompt: Explain the concept of prompt caching in AI systems. 100 words max. <br><br>Response: # Prompt Caching in AI Systems ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10937da5-037e-4fa2-9538-13bfccf6cb6c": {"__data__": {"id_": "10937da5-037e-4fa2-9538-13bfccf6cb6c", "embedding": null, "metadata": {"bbox": [{"page": 397, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.41, "x1": 432.0, "y1": 229.52}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8b5dc7d0-c856-4cd3-86f7-27e1b22982ec", "node_type": "1", "metadata": {"bbox": [{"page": 396, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.6, "x1": 432.01, "y1": 222.74}]}, "hash": "21a2457a58ab06777945119bb78af76895eeb9c73fe214fa2b33506f707b4584", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a6eecea-77c2-4266-99cf-0b19177ebeda", "node_type": "1", "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 279.63, "x1": 432.01, "y1": 545.67}]}, "hash": "62bdaceea6109c2a07d1978eb952f938b945ac623cba4bfb697fef07d3cced5e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For more scalable implementation, you might want to use a KV database so that \nmultiple clients share the same cache. In addition, consider making the cache user \nspecific so that information from one user isn\u2019t leaked to a different user. <br><br>You can find the full implementation of our example in our GitHub repository. <br><br>**Considerations**\nFoundational models that support both server-side Prompt Caching and a large \ncontext window offer an appealing alternative to RAG. See the \u201cRAG versus Large \nContext Window\u201d sidebar discussion in the \u201cConsiderations\u201d subsection of the Pat\u2010 \ntern 6 (Basic RAG) section in Chapter 3, and also see the notebook in GitHub that \nillustrates this approach. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a6eecea-77c2-4266-99cf-0b19177ebeda": {"__data__": {"id_": "7a6eecea-77c2-4266-99cf-0b19177ebeda", "embedding": null, "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 279.63, "x1": 432.01, "y1": 545.67}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "10937da5-037e-4fa2-9538-13bfccf6cb6c", "node_type": "1", "metadata": {"bbox": [{"page": 397, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 79.41, "x1": 432.0, "y1": 229.52}]}, "hash": "077545a71b8ff7f532aac1a9c525e700b6c1dae091b744955d3584e4bc8237d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50e08c05-fb0c-4ffa-a649-f78d06af18e6", "node_type": "1", "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.53, "x1": 432.01, "y1": 268.06}]}, "hash": "a12feb3345d392c738b10f40f650d20ad61f0c355de96663db002a9398bd85e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Problem**\nFor business applications that deal with sensitive and often heavily regulated health \ncare, financial, or legal information, it can be beneficial to host your own LLM. That \nway, your confidential, proprietary, or user data stays on your servers and can\u2019t be \nshared or leaked by a third party\u2014or reused to train a future version of a model that\u2019s \nalso available to your competition. <br><br>However, hosting LLMs presents unique challenges. This pattern demonstrates three \nsolutions for optimizing the inference performance of self-hosted LLMs. We assume \nyou\u2019re already familiar with running your own inference server, since covering \nmodel-hosting fundamentals would extend beyond this book\u2019s scope. If you\u2019re new to \nmodel hosting, consider exploring example setups using vLLM or SGLang. <br><br>When you\u2019re self-hosting LLMs in production environments, inference optimization \nis crucial\u2014particularly for real-time applications like chatbots and conversational \ninterfaces. Even when such applications perform complex tasks that require larger \nLLMs (with 70 billion or more parameters), users still expect response times that are \ncomparable to those of smaller models or services like ChatGPT. <br><br>Moreover, self-hosting LLMs is often constrained by the availability of GPUs, both \nin quantity and in terms of memory, so you need to utilize the available hardware as \noptimally as possible. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50e08c05-fb0c-4ffa-a649-f78d06af18e6": {"__data__": {"id_": "50e08c05-fb0c-4ffa-a649-f78d06af18e6", "embedding": null, "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.53, "x1": 432.01, "y1": 268.06}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7a6eecea-77c2-4266-99cf-0b19177ebeda", "node_type": "1", "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 279.63, "x1": 432.01, "y1": 545.67}]}, "hash": "62bdaceea6109c2a07d1978eb952f938b945ac623cba4bfb697fef07d3cced5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9749ae7-92b2-4f41-a0cd-c23014d604a0", "node_type": "1", "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.75, "x1": 432.01, "y1": 605.54}]}, "hash": "ca28037d0861696c3c9737c246bd388b461bea1b2d71ea9a59139dcdfe41a67a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nThere are three techniques (which are not mutually exclusive) that you can employ \nto improve the efficiency of inference when self-hosting LLMs: continuous batching, \nspeculative decoding, and prompt compression. Let\u2019s look at them one by one. <br><br>**Option 1: Continuous batching**<br><br>In traditional ML, batching requests is a widely used practice to improve inference \nthroughput, especially when using GPUs that can easily parallelize requests. This \ntechnique works well because ML models are problem specific. Each request provides \nthe same input features, whether it is tabular data or embedding vectors, so requests \nare all the same size. This makes it possible to vectorize the matrix computations that \nunderlie ML models. <br><br>With LLMs, request patterns have changed, and input prompts have vastly different \nlengths. If you try to batch those requests as you would with traditional ML, the ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9749ae7-92b2-4f41-a0cd-c23014d604a0": {"__data__": {"id_": "c9749ae7-92b2-4f41-a0cd-c23014d604a0", "embedding": null, "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.75, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "50e08c05-fb0c-4ffa-a649-f78d06af18e6", "node_type": "1", "metadata": {"bbox": [{"page": 399, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.53, "x1": 432.01, "y1": 268.06}]}, "hash": "a12feb3345d392c738b10f40f650d20ad61f0c355de96663db002a9398bd85e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10160f54-a6bf-4b33-ae5b-39e7fe563069", "node_type": "1", "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 432.01, "y1": 340.94}]}, "hash": "7abf1e52d8be5881be2f1436d315e5a49d2292c4179667ef37495d53033a48df", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Continuous batching improves the processing throughput by allowing requests to \njoin and leave the batch at the granularity level of individual forward passes (itera\u2010 \ntions), rather than waiting for entire batches to complete. During each forward pass, \nthe GPU processes all sequences, and after each iteration, the GPU checks whether \nany of the sequences produced a stop token. If they did, it removes the particular \nsequence from the batch and fills the open slots with the next request sequence from \na request queue. After that, the next batch iteration will include the newly added \nsequence. <br><br>Under the hood, the LLM kernels handle the dynamic resizing of the attention \nmatrices as sequences enter or leave the batch, and they also clear the KV cache for \nexisting sequences. <br><br>Model inference servers like vLLM and SGLang offer continuous batching by default, \nso you won\u2019t need to worry about the implementation details. The main change you \nneed to make to take advantage of this is in how you submit your requests to the LLM \ninference server: instead of submitting individual requests, you should let the server \noptimize your inference. <br><br>Don\u2019t submit individual requests, as shown here: <br><br>**for**prompt**in**prompts: \n_= model.generate(prompt, sampling_params) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10160f54-a6bf-4b33-ae5b-39e7fe563069": {"__data__": {"id_": "10160f54-a6bf-4b33-ae5b-39e7fe563069", "embedding": null, "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 432.01, "y1": 340.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c9749ae7-92b2-4f41-a0cd-c23014d604a0", "node_type": "1", "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 349.75, "x1": 432.01, "y1": 605.54}]}, "hash": "ca28037d0861696c3c9737c246bd388b461bea1b2d71ea9a59139dcdfe41a67a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4056a26-f4de-4473-9a6d-8cada883c9b2", "node_type": "1", "metadata": {"bbox": [{"page": 402, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 195.49, "x1": 432.01, "y1": 327.3}]}, "hash": "cb7b4f7dd186a6c191124a94e270a1dcb167169a14cafa3a2c98a0197e25bbad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead, submit all requests to the server. <br><br>_= model.generate(prompts, sampling_params) <br><br>The inference server will take care of slotting prompts into GPU cores as they \nbecome available. <br><br>**Option 2: Speculative Decoding**<br><br>Speculative decoding is an optimization technique that leverages two distinct language \nmodels to improve generation speed while maintaining output quality. The approach \nuses a draft-target architecture, in which two complementary models work together: <br><br>\u2022 \n\u2022 A smaller, more efficient language model, which operates faster but may be less <br><br>accurate, acts as the draft model. <br><br>\u2022 A large, sophisticated LLM, which produces highly accurate outputs but is com\u2010 \n\u2022 \nputationally expensive and relatively slow, serves as the target model and ground \ntruth for token generation. <br><br>The draft model is specifically trained to emulate the behavior of the target model, \nfor example, a 1-billion-parameter model might be trained to imitate a 27-billion- \nparameter model. In this sense, the draft model is like the student and the target \nmodel is like the teacher. However, the draft model is not trained from or by the ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4056a26-f4de-4473-9a6d-8cada883c9b2": {"__data__": {"id_": "d4056a26-f4de-4473-9a6d-8cada883c9b2", "embedding": null, "metadata": {"bbox": [{"page": 402, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 195.49, "x1": 432.01, "y1": 327.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "10160f54-a6bf-4b33-ae5b-39e7fe563069", "node_type": "1", "metadata": {"bbox": [{"page": 401, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.14, "x1": 432.01, "y1": 340.94}]}, "hash": "7abf1e52d8be5881be2f1436d315e5a49d2292c4179667ef37495d53033a48df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e9dae0c-d347-402d-b6d6-0a1905c0765d", "node_type": "1", "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 299.87, "x1": 414.0, "y1": 536.21}]}, "hash": "37756f5d6030eee1be4cd26c382f62831a4c8ff7e3bbf0f004155beb50d92280", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Hard prompt compression.**If you want to reduce the length of a prompt but keep \nthe meaning in a human-readable way, you can use hard prompt compression by \nremoving redundant content, applying abbreviations, or expressing meaning through \nkeywords, as shown in Figure 8-9. <br><br>You can perform the processing through regular expressions or by asking an LLM to \ncompress the prompt. You can use the same LLM to try to reconstruct the original \nprompt from the compressed prompt to check for major information loss. LLMs \nusually respond to compressed prompts as they would have responded to the original \nprompt\u2014just make sure to add checking for information loss and comparing respon\u2010 \nses to your evaluation suite. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e9dae0c-d347-402d-b6d6-0a1905c0765d": {"__data__": {"id_": "7e9dae0c-d347-402d-b6d6-0a1905c0765d", "embedding": null, "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 299.87, "x1": 414.0, "y1": 536.21}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d4056a26-f4de-4473-9a6d-8cada883c9b2", "node_type": "1", "metadata": {"bbox": [{"page": 402, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 195.49, "x1": 432.01, "y1": 327.3}]}, "hash": "cb7b4f7dd186a6c191124a94e270a1dcb167169a14cafa3a2c98a0197e25bbad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a54ebd65-7fcb-4243-a969-ee6c042da829", "node_type": "1", "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 290.28}]}, "hash": "9c2af8e09b841e7bce5e26fc2321d7542f2db9304e79025fb9fb78ede0d1fc57", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Context: <br><br>Johann Sebastian Bach (1685-1750) stands as one of the most influential composers \nin Western classical music history. Born in Eisenach into a musical family, Bach \nreceived his early musical education from his father and later his older brother Johann \nChristoph. His compositional output spans multiple genres and demonstrates unpre\u2010 \ncedented technical mastery and innovative harmonic language. <br><br>... <br><br>His theoretical contributions include The Art of Fugue, an unfinished work exploring \nfugal technique in systematic fashion, and The Musical Offering, composed for Fred\u2010 \nerick the Great of Prussia. These works demonstrate Bach\u2019s intellectual approach to \ncomposition, treating music as both art and mathematical science. <br><br>Bach\u2019s influence extends far beyond his historical period. Mozart studied and arranged \nseveral Bach works, Beethoven called him the \u201coriginal father of harmony,\u201d and the \n19th-century Bach revival led by Felix Mendelssohn reintroduced his music to concert \naudiences. Modern jazz musicians like Bill Evans and Keith Jarrett have drawn inspira\u2010 \ntion from Bach\u2019s harmonic innovations and structural principles. <br><br>Question: Analyze Bach\u2019s compositional techniques in his keyboard works, focusing \non counterpoint, harmonic innovation, and structural organization. How do these \ntechniques reflect the musical aesthetics of the Baroque period while simultaneously \npointing toward future developments in classical music? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a54ebd65-7fcb-4243-a969-ee6c042da829": {"__data__": {"id_": "a54ebd65-7fcb-4243-a969-ee6c042da829", "embedding": null, "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 290.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7e9dae0c-d347-402d-b6d6-0a1905c0765d", "node_type": "1", "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 89.99, "y0": 299.87, "x1": 414.0, "y1": 536.21}]}, "hash": "37756f5d6030eee1be4cd26c382f62831a4c8ff7e3bbf0f004155beb50d92280", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fec35965-c571-4a85-a05c-2563e4ffa32d", "node_type": "1", "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.33, "x1": 432.0, "y1": 605.71}]}, "hash": "97f59f2dd09b60cc98d5e6a824f53f2cae03da544a43819cf422383fd0a0aa9a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Soft compression reduces that prompt to the following shorter prompt: <br><br><bach_1> <bach_2> <bach_3> ... <bach_n> <br><br>Question: Analyze Bach\u2019s compositional techniques in his keyboard works, focusing \non counterpoint, harmonic innovation, and structural organization. How do these \ntechniques reflect the musical aesthetics of the Baroque period while simultaneously \npointing toward future developments in classical music? <br><br>Instead of submitting around 70 tokens of the biographical paragraph, the prompt \nnow contains one dense vector, <bach_1>, with the encoded information. Each of the \ncontext\u2019s paragraphs is encoded into the vectors <bach_1> to <bach_n>. <br><br>Soft compression can also be provided to the model as KV values instead of embed\u2010 \ndings, as done by the 500xCompressor. Research has shown drastic improvements in \nprompt compression rates. <br><br>Soft compressions are model specific, which means a compressed prompt based on \nan encoder trained on a Llama4 model can\u2019t be submitted to a GPT-4 endpoint for a \nmodel request. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fec35965-c571-4a85-a05c-2563e4ffa32d": {"__data__": {"id_": "fec35965-c571-4a85-a05c-2563e4ffa32d", "embedding": null, "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.33, "x1": 432.0, "y1": 605.71}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a54ebd65-7fcb-4243-a969-ee6c042da829", "node_type": "1", "metadata": {"bbox": [{"page": 404, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 80.9, "x1": 432.0, "y1": 290.28}]}, "hash": "9c2af8e09b841e7bce5e26fc2321d7542f2db9304e79025fb9fb78ede0d1fc57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8742dc2-740d-4b53-b8fc-5f5cc281abf0", "node_type": "1", "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.12, "x1": 432.01, "y1": 265.99}]}, "hash": "c6f71b05c58af4e991b2a8517e3b6b952fc3edc20ee56e215b270e23e6d379a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Please note that the improvement heavily depends on the smaller \nmodel\u2019s ability to predict the correct tokens. For example, if \nnum_speculative_tokens is too long, there is a good chance that \nthe larger model will need to recompute the tokens. In that case, \nthroughput can be even slower than with the larger model alone, \nso it\u2019s important to tune the num_speculative_tokens parameter \ncarefully. <br><br>**References**\nContinuous batching was introduced by Yu et al. (2022) and is explained well by \nDaniel et al. (2023) in the Anyscale blog. A survey of prompt compression methods \nwas carried out by Li et al. (2024). Speculative decoding was introduced by Leviathan, \nKalman, and Matias (2022), who showed that it could accelerate off-the-shelf models \nwithout retraining or architecture changes. Prompt compression is well-documented \nin Li et al. (2024), with practical implementations like the 500xCompressor dem\u2010 \nonstrating significant compression ratios. A survey of efficiency improvements for \nLLMs was carried out by Xia et al. (2024). <br><br>AWS Inferentia2 demonstrates speculative decoding with Llama-2-70B/7B models, \nusing a smaller draft model to accelerate inference while maintaining accuracy on \ntheir custom AI chips. NVIDIA provides comprehensive inference optimization \nthrough TensorRT-LLM, including continuous batching, speculative inference, atten\u2010 \ntion optimizations, and model compression techniques for enterprise deployment. \nAnthropic Claude implemented dynamic batching that resulted in 37% increased \nthroughput, 28% reduced latency, and processing 1.2 million more queries per day \nthrough intelligent batch size management. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8742dc2-740d-4b53-b8fc-5f5cc281abf0": {"__data__": {"id_": "a8742dc2-740d-4b53-b8fc-5f5cc281abf0", "embedding": null, "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.12, "x1": 432.01, "y1": 265.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fec35965-c571-4a85-a05c-2563e4ffa32d", "node_type": "1", "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 278.33, "x1": 432.0, "y1": 605.71}]}, "hash": "97f59f2dd09b60cc98d5e6a824f53f2cae03da544a43819cf422383fd0a0aa9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ce4934b-ebd6-4361-afbf-c894fd1e4f95", "node_type": "1", "metadata": {"bbox": [{"page": 408, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.08, "x1": 432.01, "y1": 360.74}]}, "hash": "fe21e588d65fd64a625546959d2a339ebd07a42c92a2091eda005fb2f5fb6cc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Pattern 27: Degradation Testing**\nThe Degradation Testing pattern helps you identify bottlenecks that affect the perfor\u2010 \nmance of AI applications built on top of foundational models. In this pattern, we \noutline which metrics you should focus on, how to test your LLM-based application \nsetup, and ways to handle situations where the application does not perform to the \ndesired level. <br><br>**Problem**\nIn LLM-based AI applications, performance testing usually comes down to testing \nthe performance of the inference setup. Even though the AI application may look \nlike a traditional server application, it\u2019s not enough to perform traditional load \ntesting. Load testing identifies server failure points (like 400/500 errors), but in AI \napplications, you need to understand performance in more detail. Specifically, you ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ce4934b-ebd6-4361-afbf-c894fd1e4f95": {"__data__": {"id_": "8ce4934b-ebd6-4361-afbf-c894fd1e4f95", "embedding": null, "metadata": {"bbox": [{"page": 408, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.08, "x1": 432.01, "y1": 360.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a8742dc2-740d-4b53-b8fc-5f5cc281abf0", "node_type": "1", "metadata": {"bbox": [{"page": 407, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 82.12, "x1": 432.01, "y1": 265.99}]}, "hash": "c6f71b05c58af4e991b2a8517e3b6b952fc3edc20ee56e215b270e23e6d379a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0edede4-fd98-4250-b737-0d02c30a4095", "node_type": "1", "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 388.04, "x1": 432.01, "y1": 605.54}]}, "hash": "dee725b9b1452e9119593f681c565780dac7613bd78f76dfdc0906131789ef8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fine-grained load testing is necessary for testing AI applications, but conventional \nload testing tools lack the specific metrics that are relevant to evaluating LLM \nperformance. Monitoring the correct metric is crucial, since ambiguity can lead to \ninefficient testing and misdirect efforts to irrelevant constraints. <br><br>What constraints and metrics are relevant in LLM degradation testing? <br><br>**Solution**\nThere are a few core metrics that, if you monitor and act on them, can help safeguard \nagainst performance degradation. In addition to monitoring these core metrics, you \nneed to verify that the system remains scalable and resilient by following a systematic \ntesting regimen. <br><br>**Core metrics**<br><br>Four of the most important metrics to evaluate the LLM serving performance are \ntime to first token (TTFT), end-to-end request latency (EERL), tokens per second \n(TPS), and requests per second (RPS). Each of these is correlated with specific \nconstraints on LLM performance that in turn will lead to degradation. <br><br>**Time to first token (TTFT).**TTFT is the time between the submission of the request to \nthe LLM and the generation of the first token of the response. This metric tells you \nhow long a user would have to wait until they received the start of the model\u2019s output. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0edede4-fd98-4250-b737-0d02c30a4095": {"__data__": {"id_": "e0edede4-fd98-4250-b737-0d02c30a4095", "embedding": null, "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 388.04, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8ce4934b-ebd6-4361-afbf-c894fd1e4f95", "node_type": "1", "metadata": {"bbox": [{"page": 408, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.08, "x1": 432.01, "y1": 360.74}]}, "hash": "fe21e588d65fd64a625546959d2a339ebd07a42c92a2091eda005fb2f5fb6cc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00734b50-c197-451e-967e-e0fed853a91d", "node_type": "1", "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.78, "x1": 432.0, "y1": 379.94}]}, "hash": "41e862ba6f4a824b31a1609f616fbe1dd19e1a3c98e785f53a436bdf36228691", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of the input prompt that needs to be processed and the speed of the KV cache that is \nused to speed up the computation of the attention. <br><br>The longer the input prompt, the higher the TTFT will be. So, you should look at \nwhether you can reduce the size of the input prompt by using Prompt Compression \n(Pattern 26). You may also be able to reduce the processing needed for the prompt by \ncaching the system instruction, media, or other prefix, as discussed in the section on \nPrompt Caching (Pattern 25). <br><br>Another way is to speed up the KV cache that takes advantage of the fact that many of \nthe computations needed for the generation of the nth token were already performed \nfor the n-1th token. You can speed up this KV cache by increasing the GPU memory. \nAnother way you can optimize the KV cache for TTFT is to put the more predictable \ntext earlier in the prompt and the more \u201cdynamic\u201d text (such as RAG results) later in \nthe prompt. <br><br>One more way to decrease the TTFT is to reduce the maximum context window \nfor the served model. This acts like reducing memory requirements. With a smaller \nwindow, the KV cache will be smaller and ultimately faster. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00734b50-c197-451e-967e-e0fed853a91d": {"__data__": {"id_": "00734b50-c197-451e-967e-e0fed853a91d", "embedding": null, "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.78, "x1": 432.0, "y1": 379.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e0edede4-fd98-4250-b737-0d02c30a4095", "node_type": "1", "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 388.04, "x1": 432.01, "y1": 605.54}]}, "hash": "dee725b9b1452e9119593f681c565780dac7613bd78f76dfdc0906131789ef8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98d33290-6274-47c3-b17a-409fc9b0fe15", "node_type": "1", "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "hash": "3adf910341d8ced5685a5f7227079322affa24fa97f1adfa5be3720325a58609", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, a nontechnical but surprisingly effective way to reduce \u201cthe perceived\u201d TTFT \nis to show progress to the user whenever your application involves multiple steps. <br><br>**End-to-end request latency (EERL).**In contrast to TTFT, end-to-end request latency \n(EERL) measures the time to generate the complete response as shown in Fig\u2010 \nure 8-12. The metric also includes the queueing time, network latencies, and creation \ntime of the KV cache. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98d33290-6274-47c3-b17a-409fc9b0fe15": {"__data__": {"id_": "98d33290-6274-47c3-b17a-409fc9b0fe15", "embedding": null, "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "00734b50-c197-451e-967e-e0fed853a91d", "node_type": "1", "metadata": {"bbox": [{"page": 410, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.78, "x1": 432.0, "y1": 379.94}]}, "hash": "41e862ba6f4a824b31a1609f616fbe1dd19e1a3c98e785f53a436bdf36228691", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe962d19-fd26-49b5-9f39-5e4ad131e701", "node_type": "1", "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 218.58, "x1": 432.0, "y1": 293.99}]}, "hash": "bc33138a8203d6a5989ae12436993ed052def3b557b72643d83b5177622e9e32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This metric will provide you with a good understanding of what your application \nusers can expect during normal operations. If the metric is too slow for your applica\u2010 \ntion, you can apply the patterns presented in this chapter to see if you can increase \nperformance. You can also try to use a different underlying hardware (for example, \nby upgrading from an NVIDIA L4 to an A100 GPU (which provides you with more \nprocessing bandwidth) or by using specialized application-specific integrated circuit \n(ASIC) hardware like Groq). <br><br>EERL increases with the length of the response, so if you can formulate the result in \na way that generates fewer output tokens, do so. For example, instead of asking for a \ndetailed output, you can ask the model to generate only differences from a reference \nanswer (and use Prompt Caching to cache the reference answer). You can also use \nfew-shot learning to demonstrate more concise answers to the model. <br><br>Another way to reduce EERL is to employ parallelization. If you can break up the task \ninto two subtasks that are not dependent on each other, then you can execute the two \ncalls in parallel, and that will cut down the latency. Even if the subtasks are strictly \nsequential (because Step 2 depends on Step 1), you may be able to leverage speculative \nexecution\u2014which involves starting Step 2 at the same time as Step 1, but with a guess \nfor the result of Step 1. Once Step 1 completes, you can verify its result. If the result of \nStep 1 is what you guessed, let Step 2 run to completion and use its result. If the result \nof Step 1 doesn\u2019t match your initial guess, cancel Step 2 and relaunch it\u2014. In essence, \nyou\u2019ll speed up the process (at the expense of some wasted compute) whenever your \nguess is correct, so this technique works whenever there\u2019s a happy path for Step 1 \nthat\u2019s much more likely than the alternative. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe962d19-fd26-49b5-9f39-5e4ad131e701": {"__data__": {"id_": "fe962d19-fd26-49b5-9f39-5e4ad131e701", "embedding": null, "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 218.58, "x1": 432.0, "y1": 293.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98d33290-6274-47c3-b17a-409fc9b0fe15", "node_type": "1", "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 305.84, "x1": 432.01, "y1": 605.54}]}, "hash": "3adf910341d8ced5685a5f7227079322affa24fa97f1adfa5be3720325a58609", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be5332f3-7a96-44d3-aa44-1989e7c3aff9", "node_type": "1", "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 337.64, "x1": 432.01, "y1": 605.54}]}, "hash": "71d3a3ecf2d1f7d08bb3bee9496574920a99ac758db8968edae2af3b6bf0c990", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Tokens per second (TPS).**The TPS for a system represents the total output tokens per \nsecond throughput. This metric measures your overall system\u2019s performance instead \nof that of a single request. As the number of requests to your system increases, your \nTPS will go up as well. However, the number of requests will eventually reach a level, \ntermed the saturation point, at which your LLM setup won\u2019t be able handle any more \nrequests. You\u2019ll observe a drop in TPS after your LLM reaches the saturation point. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be5332f3-7a96-44d3-aa44-1989e7c3aff9": {"__data__": {"id_": "be5332f3-7a96-44d3-aa44-1989e7c3aff9", "embedding": null, "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 337.64, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fe962d19-fd26-49b5-9f39-5e4ad131e701", "node_type": "1", "metadata": {"bbox": [{"page": 412, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 218.58, "x1": 432.0, "y1": 293.99}]}, "hash": "bc33138a8203d6a5989ae12436993ed052def3b557b72643d83b5177622e9e32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb4ba5d3-41c8-4e4b-a08a-041a1cb1ba34", "node_type": "1", "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.09, "x1": 432.01, "y1": 324.83}]}, "hash": "b1d19a8cd776e630f2a0054f41c7f9b7a3dfb5ebc90c302a35719a210d74220a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RPS is intimately related to TPS, except that TPS also takes into account response \nlength. Usually, TPS is the operating constraint, but RPS can be useful as a measure of \nthroughput. <br><br>Throughput testing involves simulating multiple users sending queries simultane\u2010 \nously and measuring how many successful requests your system can process per \nsecond without significant performance degradation. While latency measures indi\u2010 \nvidual response speed, throughput testing determines your LLM\u2019s capacity to handle \nmultiple concurrent requests effectively. This metric becomes critical as your user \nbase grows and demand increases. <br><br>The basis of effective throughput testing lies in establishing load levels that reflect \nrealistic usage patterns of your business application. During throughput testing, \nmonitoring extends beyond simple request counting. Key metrics include response \ntime consistency, error rates, and overall system stability as concurrent load increases. \nThis comprehensive monitoring approach helps identify the point at which increased \nthroughput begins to negatively impact response quality or system reliability. Under\u2010 \nstanding these trade-offs helps you make informed decisions about system capacity \nplanning and resource allocation strategies. <br><br>Besides RPS, metrics to measure for throughput include successful request rate \n(measured in percentage), error rate (measured in percentage or errors per second), \nand concurrent users supported (measured in number of users). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb4ba5d3-41c8-4e4b-a08a-041a1cb1ba34": {"__data__": {"id_": "bb4ba5d3-41c8-4e4b-a08a-041a1cb1ba34", "embedding": null, "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.09, "x1": 432.01, "y1": 324.83}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "be5332f3-7a96-44d3-aa44-1989e7c3aff9", "node_type": "1", "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 337.64, "x1": 432.01, "y1": 605.54}]}, "hash": "71d3a3ecf2d1f7d08bb3bee9496574920a99ac758db8968edae2af3b6bf0c990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fcd4b1c-d310-41e9-96c2-6738fdf8e127", "node_type": "1", "metadata": {"bbox": [{"page": 415, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.53, "x1": 432.01, "y1": 294.93}]}, "hash": "0b3c2fe80e8746bed95d79c01d7b62fe1d16294f4e978ddf419b247c6708fedd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Scalability and resilience**<br><br>The core metrics measure latency and throughput. Other factors that you might need \nto measure and monitor include scalability and resilience. <br><br>**Scalability.**Scalability testing takes throughput evaluation a step further by examin\u2010 \ning how your LLM\u2019s performance characteristics evolve as load gradually increases \nover time. Rather than applying static load levels, this approach reveals how your \nsystem behaves as demand grows progressively and thus helps you identify scaling \nlimitations. Starting with minimal load and increasing demand over time allows you \nto observe performance trends and identify inflection points where efficiency begins \nto decline. <br><br>During the test, you monitor response times, throughput rates, and error frequencies. \nThese metrics will provide you with valuable information when your system hits \nscaling bottlenecks, and knowing when you\u2019ll hit such bottlenecks will help you \ndesign your inference setup. Such bottlenecks could stem from the choice of GPU or \nuser demand when your setup needs to scale to more instances. <br><br>Metrics to measure for scalability include throughput versus load (which measures \nRPS at different load levels), response time degradation (measured in seconds or \nmilliseconds as load increases), resource utilization (measured as a percentage for ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fcd4b1c-d310-41e9-96c2-6738fdf8e127": {"__data__": {"id_": "4fcd4b1c-d310-41e9-96c2-6738fdf8e127", "embedding": null, "metadata": {"bbox": [{"page": 415, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.53, "x1": 432.01, "y1": 294.93}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bb4ba5d3-41c8-4e4b-a08a-041a1cb1ba34", "node_type": "1", "metadata": {"bbox": [{"page": 414, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 70.09, "x1": 432.01, "y1": 324.83}]}, "hash": "b1d19a8cd776e630f2a0054f41c7f9b7a3dfb5ebc90c302a35719a210d74220a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "651448dc-7033-4afe-be24-ec734717b567", "node_type": "1", "metadata": {"bbox": [{"page": 417, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.25}]}, "hash": "1ca2e0c82d3c2c8aa3a1b8405d1d10663f243690418ee9e1250d970c611dcf18", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Load testing.**Load testing specifically targets expected peak traffic conditions to \nensure your LLM can maintain acceptable performance during high-demand periods. \nUnlike stress testing, which seeks to find breaking points, load testing validates \nperformance in realistic but demanding scenarios that your system should be able to \nhandle routinely. <br><br>This test requires accurate modeling of peak usage patterns, like usage volumes. By \nsimulating these conditions during your test, you can verify that your LLM maintains \nacceptable response times, processes requests reliably, and exhibits stable error rates \neven during the most demanding periods of normal operation. <br><br>This test will help you design your inference setup by helping you know when to scale \nthe inference setup to more instances. <br><br>Metrics to measure during load testing include peak load performance (the RPS \nduring expected high traffic), response time under load (measured in seconds or \nmilliseconds during peak conditions), error rate at peak (measured as a percentage), \nqueue length (measured as the number of pending requests), and resource utilization \nat peak (measured as the CPU/memory percentage usage at peak system usage). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "651448dc-7033-4afe-be24-ec734717b567": {"__data__": {"id_": "651448dc-7033-4afe-be24-ec734717b567", "embedding": null, "metadata": {"bbox": [{"page": 417, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4fcd4b1c-d310-41e9-96c2-6738fdf8e127", "node_type": "1", "metadata": {"bbox": [{"page": 415, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 75.53, "x1": 432.01, "y1": 294.93}]}, "hash": "0b3c2fe80e8746bed95d79c01d7b62fe1d16294f4e978ddf419b247c6708fedd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96fe2b0c-e2d5-4e99-a061-b6b441d8a076", "node_type": "1", "metadata": {"bbox": [{"page": 418, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "a953b8a4dc6c6769fce98d9222d4d32b297619c6804a53227d37e8866bcbf2e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 8-15. A poorly performing LLM inference setup <br><br>If you observe performance bottlenecks (such as in response times) as shown in our \nexample, it indicates that your computational resources are insufficient. To deal with \nthis, you can start by checking whether the memory of the underlying hardware \nis fully used, and if it is, consider upgrading to a GPU/TPU platform with higher \ncompute capabilities. This will allow for the processing of larger prompts and faster \ncomputation of billion parameter models. <br><br>Secondly, you can consider distributing the processing load among multiple GPUs \nif upgrading to a larger GPU/TPU isn\u2019t possible. You can achieve this with various \nparallelization techniques, such as data parallelism (distributing data among GPUs \nwhile each GPU processes a full model) and model parallelism (partitioning the \nmodel among multiple GPUs). <br><br>And lastly, you can apply the patterns we\u2019ve discussed in this chapter. If your goal is \nto have a fast TTFT, then model distillation, quantization, and continuous batching \nare good options. If your goal is to increase the overall throughput, then reducing the \nmodel size via distillation, quantization, or speculative decoding is a good option. <br><br>**Example**<br><br>In our Github repository, we provide a basic LLM benchmarking tool that simulates \nn users and in which each user can submit x requests. Then, we measure the time <br><br>**400** **|** **Chapter 8: Addressing Constraints**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96fe2b0c-e2d5-4e99-a061-b6b441d8a076": {"__data__": {"id_": "96fe2b0c-e2d5-4e99-a061-b6b441d8a076", "embedding": null, "metadata": {"bbox": [{"page": 418, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "651448dc-7033-4afe-be24-ec734717b567", "node_type": "1", "metadata": {"bbox": [{"page": 417, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.25}]}, "hash": "1ca2e0c82d3c2c8aa3a1b8405d1d10663f243690418ee9e1250d970c611dcf18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23c54c6f-5ab6-49b3-bd1f-a573d6d954e4", "node_type": "1", "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.43, "x1": 432.0, "y1": 606.65}]}, "hash": "534cbdaac23649388a1f2a6f2d79b5661551451e7876f1dfe27f709114614dd0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "duration of every request and the TTFT, and we generate the overall metric statistics \n(see Figure 8-16). <br><br><br><br>Figure 8-16. LLM benchmarking workflow <br><br>The implementation consists of plenty of boilerplate code, so we\u2019ll refrain from \ndiscussing it here. However, you may be interested in testing your LLM setup with \nthe benchmarking tool. (We generated Figure 8-14 and Figure 8-15 with it.) If so, you \ncan use the tool by running the example script (which tests an OpenAI endpoint) or \ncustomize the example code to your needs: <br><br>$ python llm_benchmark_openai.py**\\**\n --requests-per-user 25**\\**\n --num-users 100 <br><br>Running the example code will generate your statistics, which should be similar to the \nfollowing results: <br><br>Starting OpenAI API benchmark**with**configuration: \n Model: gpt-4o-mini \n Users: 100 <br><br>**Pattern 27: Degradation Testing** **|** **401**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23c54c6f-5ab6-49b3-bd1f-a573d6d954e4": {"__data__": {"id_": "23c54c6f-5ab6-49b3-bd1f-a573d6d954e4", "embedding": null, "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.43, "x1": 432.0, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "96fe2b0c-e2d5-4e99-a061-b6b441d8a076", "node_type": "1", "metadata": {"bbox": [{"page": 418, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "a953b8a4dc6c6769fce98d9222d4d32b297619c6804a53227d37e8866bcbf2e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df1a903e-1c80-42fa-b362-c3d0947931fd", "node_type": "1", "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.43, "x1": 432.01, "y1": 139.33}]}, "hash": "f9a220df44a7c16da7974d61782e1e8cf37c263fb793c6102660d9f89505bda5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " Requests per user: 25 \n Max tokens: 150 \n Temperature: 0.7 \n Prompt: Explain quantum computing**in**simple terms. <br><br>Starting benchmark**with**100 users, 25 requests per user... \nBenchmark completed**in**190.57 seconds \nSuccessful requests: 2499 \nFailed requests: 1 <br><br>======================================================== \nBENCHMARK SUMMARY \n======================================================== \nEndpoint: https://api.openai.com/v1/chat/completions \nModel: gpt-4o-mini \nUsers: 100 \nRequests per user: 25 \nTotal requests: 2500 \nSuccessful: 2499 \nFailed: 1 \nSuccess rate: 100.0% \nTotal duration: 190.57s <br><br>PERFORMANCE METRICS: \nAverage TTFT: 3.556s \n95th percentile TTFT: 4.206s \nAverage tokens/sec: 24.1 \n95th percentile tokens/sec: 31.5 \nOverall throughput: 1914.5 tokens/sec \n======================================================== <br><br>**Performance-monitoring tools**\nMore sophisticated tools can help you simulate more complex usage patterns. These \ninclude the following:LLMPerf is a library that was developed <br><br>LLMPerf <br><br>by the Ray Project for validating and benchmarking LLMs. It provides capabili\u2010 \nties for performance benchmarking. The tool can spawn load tests by submitting \nconcurrent requests to the LLM API, and it can measure the token latency and \ngeneration throughput per request and across concurrent requests. It focuses on \nmeasuring technical performance metrics like response times and throughput, \nrather than evaluating content quality. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df1a903e-1c80-42fa-b362-c3d0947931fd": {"__data__": {"id_": "df1a903e-1c80-42fa-b362-c3d0947931fd", "embedding": null, "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.43, "x1": 432.01, "y1": 139.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "23c54c6f-5ab6-49b3-bd1f-a573d6d954e4", "node_type": "1", "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 148.43, "x1": 432.0, "y1": 606.65}]}, "hash": "534cbdaac23649388a1f2a6f2d79b5661551451e7876f1dfe27f709114614dd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "690389aa-3813-47d7-a3b6-8dd1625bd368", "node_type": "1", "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 203.84, "x1": 432.01, "y1": 486.14}]}, "hash": "956be51e0a5a969defdcaaf9f8f5e47206a8a516124ca6c2a96ba9dfa12a9616", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LangSmith <br><br>LangSmith is an observability and evaluation platform where teams can debug, \ntest, and monitor AI app performance. It\u2019s part of the LangChain project, but \nperformance testing doesn\u2019t require LangChain. Its other capabilities include \ninference tracing, dataset capturing and management, and model evaluations, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "690389aa-3813-47d7-a3b6-8dd1625bd368": {"__data__": {"id_": "690389aa-3813-47d7-a3b6-8dd1625bd368", "embedding": null, "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 203.84, "x1": 432.01, "y1": 486.14}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "df1a903e-1c80-42fa-b362-c3d0947931fd", "node_type": "1", "metadata": {"bbox": [{"page": 419, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.43, "x1": 432.01, "y1": 139.33}]}, "hash": "f9a220df44a7c16da7974d61782e1e8cf37c263fb793c6102660d9f89505bda5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93ff9b69-514a-45f7-9ba8-5daa806b6007", "node_type": "1", "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.43, "x1": 432.01, "y1": 195.74}]}, "hash": "80902d716083876605f9924922984d4950b70aed98df6580a3acafd16534c558", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LLMs do not natively maintain information across extended user interactions. Each \ncall to an LLM is stateless and independent of previous calls. LLM-based conversa\u2010 \ntional applications such as chatbots simulate maintaining state by prepending the \nprevious conversational turns to the prompt. For example, in PydanticAI, to give \nagent access to previous messages, you explicitly pass them in: <br><br>result1 = agent.run_sync(...) \nresult2 = agent.run_sync(..., message_history=result1.new_messages()) <br><br>Workflow agents maintain context by asking for the current state as input and \nreturning an augmented state as output. For example, in LangGraph, each node has a \nsignature similar to the following: <br><br>**def**some_node(current_state: CurrentState) -> NextState: \n next_state = current_state.copy() \n ... \n next_state['confirmed_amount'] = 3450 \n**return**next_state <br><br>The problem with both of these approaches is that they essentially involve adding the \nentire conversation history to the prompt. Although LLMs today have relatively large \ncontext windows\u2014Gemini, at the time of writing, supports 1 million tokens in its \ncontext window\u2014it\u2019s cost-prohibitive to prepend the entirety of a user\u2019s interaction \nhistory to each prompt. The cost problem stems from the transformer architecture\u2019s \nquadratic scaling with sequence length, which makes context extension computation\u2010 \nally prohibitive. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93ff9b69-514a-45f7-9ba8-5daa806b6007": {"__data__": {"id_": "93ff9b69-514a-45f7-9ba8-5daa806b6007", "embedding": null, "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.43, "x1": 432.01, "y1": 195.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "690389aa-3813-47d7-a3b6-8dd1625bd368", "node_type": "1", "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 203.84, "x1": 432.01, "y1": 486.14}]}, "hash": "956be51e0a5a969defdcaaf9f8f5e47206a8a516124ca6c2a96ba9dfa12a9616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ff563cd-e753-428d-9ef3-3664d88309cb", "node_type": "1", "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 293.55, "x1": 432.01, "y1": 605.54}]}, "hash": "fabfafff237b2376ce0366ad6a45e1fd9995ad6b457f5bfc9c713faf3bc58d4a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How can you manage the memory of past interactions without overflowing the \ncontext window or incurring unmanageable costs? <br><br>**Solution**\nLong-term memory, in LLM applications, is the capacity to store and retrieve infor\u2010 \nmation that extends beyond the immediate conversational context of the current \nsession. This capacity is essential to overcoming LLMs\u2019 inherent limitations, since by \ndesign, LLMs typically treat each interaction as a fresh, isolated request. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ff563cd-e753-428d-9ef3-3664d88309cb": {"__data__": {"id_": "0ff563cd-e753-428d-9ef3-3664d88309cb", "embedding": null, "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 293.55, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "93ff9b69-514a-45f7-9ba8-5daa806b6007", "node_type": "1", "metadata": {"bbox": [{"page": 421, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 89.43, "x1": 432.01, "y1": 195.74}]}, "hash": "80902d716083876605f9924922984d4950b70aed98df6580a3acafd16534c558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db1dbd59-68bd-4d34-b6e2-5bfaf15366b2", "node_type": "1", "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 199.35, "x1": 432.01, "y1": 285.45}]}, "hash": "97bab4e317e821ed5707fcd9a2e7c3b80a51741c85c7a792e5a9e2936f12dc4e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, in LangChain, to limit the message history to 1,000 tokens, you\u2019d do the \nfollowing: <br><br>**from langchain_core.messages import**trim_messages \ntrim_messages( \n messages, \n strategy=\"last\", \n token_counter=ChatOpenAI(MODEL_ID), \n max_tokens=1000, \n start_on=\"human\", \n end_on=(\"human\", \"tool\"), \n include_system=**True**, \n) <br><br>The reason to specify start_on and end_on is that most chat models expect a conver\u2010 \nsation history to start with a human message. The last parameter in this code snippet \nspecifies that LangChain should retain the system instruction from one call to the \nnext. <br><br>**Episodic memory**<br><br>Suppose the user asks your LLM application, \u201cWhat can I do in Iceland?\u201d It may \nbe important for you to maintain continuity with messages from previous sessions. \nPerhaps this user asked your system this question two days ago and the AI suggested \nhot baths, but the user shot that idea down. It would be helpful to not repeat that \nsuggestion. <br><br>The working memory is the set of messages retained from the current session, but \nhow do you get relevant messages from previous user conversations? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db1dbd59-68bd-4d34-b6e2-5bfaf15366b2": {"__data__": {"id_": "db1dbd59-68bd-4d34-b6e2-5bfaf15366b2", "embedding": null, "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 199.35, "x1": 432.01, "y1": 285.45}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0ff563cd-e753-428d-9ef3-3664d88309cb", "node_type": "1", "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 293.55, "x1": 432.01, "y1": 605.54}]}, "hash": "fabfafff237b2376ce0366ad6a45e1fd9995ad6b457f5bfc9c713faf3bc58d4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f076dd-dffd-4397-aaae-74bce5b5bea8", "node_type": "1", "metadata": {"bbox": [{"page": 425, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "7de0a805f2290589755875290f441040ebf42fd706b1b61d435fcbc7e9ef1903", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To do this, you\u2019d store all the messages in a persistent store such as a database and \nthen search that database to find messages that are relevant to the current query or \nconversation (see Figure 8-18). The capacity to store and retrieve such messages is \ncalled episodic memory. Make sure to search not only for the content but also for \nrelevant metadata. In this use case, for example, you might want the messages to be \nfrom this user, to be recent (from the past week, perhaps), and to be about travel. Any \nrelevant messages found are used to populate the episodic memory. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46f076dd-dffd-4397-aaae-74bce5b5bea8": {"__data__": {"id_": "46f076dd-dffd-4397-aaae-74bce5b5bea8", "embedding": null, "metadata": {"bbox": [{"page": 425, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "db1dbd59-68bd-4d34-b6e2-5bfaf15366b2", "node_type": "1", "metadata": {"bbox": [{"page": 423, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 199.35, "x1": 432.01, "y1": 285.45}]}, "hash": "97bab4e317e821ed5707fcd9a2e7c3b80a51741c85c7a792e5a9e2936f12dc4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f0cf87f-35fb-4ebf-99e7-130b29df2193", "node_type": "1", "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 250.54, "x1": 432.01, "y1": 605.54}]}, "hash": "bf57df5a3eea32f7d7db70d6139f7fdba54e4e31d92778c97dc2ee2959ec3d3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, you don\u2019t need to rely on users to tell you about themselves in system \nprompts. You can monitor messages from each user and extract relevant facts into \na profile. For example, if the user mentions that they are allergic to nuts, you can \nadd that to the profile information that was used to construct the system prompt \nfor that user (see Figure 8-20). Of course, you can also incorporate the user profile \ninformation into the context of the prompt, rather than into the system prompt. That \nmeans you can construct a user-specific system prompt and thus control what parts \nof the user profile are included in it. <br><br><br><br>Figure 8-20. Updating the user profile based on user messages <br><br>**Semantic memory**<br><br>Key facts you extract from previous messages need not be just from user profile \ninformation. For example, say the current user asks, \u201cWhat can I do on the day I land \nin Iceland?\u201d It would be helpful to retrieve information on the most recent trip that \nthey told you about. The capacity to store and retrieve information that\u2019s primarily \nbased on content is semantic memory. It\u2019s different from episodic memory, which is \nprimarily based on recency. <br><br>**408** **|** **Chapter 8: Addressing Constraints**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f0cf87f-35fb-4ebf-99e7-130b29df2193": {"__data__": {"id_": "9f0cf87f-35fb-4ebf-99e7-130b29df2193", "embedding": null, "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 250.54, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "46f076dd-dffd-4397-aaae-74bce5b5bea8", "node_type": "1", "metadata": {"bbox": [{"page": 425, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "7de0a805f2290589755875290f441040ebf42fd706b1b61d435fcbc7e9ef1903", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c33b3bd-0066-4d00-916e-e32401fe9650", "node_type": "1", "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.25, "x1": 432.0, "y1": 242.44}]}, "hash": "587c9984fc63e39eb26e1ea48caddf6efa16f8a95dfd318a6075738f4bf8d5ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In LangGraph, to store memories, you do the following: <br><br>store = PostgresStore( \n connection_string=\"postgresql://.../dbname\", \n) \ntrip_memories_ns = (user_id, \"trip_memories\")*# namespace*\nmemory = {\"trip\": {\"from\": \"SEA\", \"to\": \"KEF\", \"depart_time\": ...}} \nmemory_id = hash(json.dumps(memory, sort_keys=**True**)) \nstore.put(trip_memories_ns, memory_id, memory) <br><br>Then, if the user asks a question about a trip, the application can search for their most \nrecent trip in the memory store: <br><br>most_recent_trip = in_memory_store.search(trip_memories_ns)[-1] <br><br>It might instead look at all their recent trips within a certain timeframe or at ones that \ninvolve Iceland. The logic of what\u2019s relevant is up to you. <br><br>**Example**\nSuppose you\u2019re working on an AI assistant that assists users in booking their future \ntravel. Long-term memory will help it to remember relevant context like seat prefer\u2010 \nences, preferred airlines or hotel chains, and already visited destinations. You can find \nthe complete example in our Github repository. <br><br>**Mem0**<br><br>For long-term memory, you\u2019ll use Mem0, which implements the low-latency memory \nmanagement approach introduced by Chhikara et al. in 2025 in their paper Mem0: \nBuilding Production-Ready AI Agents with Scalable Long-Term Memory. Mem0 \ndynamically extracts, consolidates, and retrieves relevant information from ongoing \nconversations into a graph-based memory representation. It provides the ability to \nrepresent all four types of memory, as discussed in the previous \u201cSolution\u201d section. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c33b3bd-0066-4d00-916e-e32401fe9650": {"__data__": {"id_": "0c33b3bd-0066-4d00-916e-e32401fe9650", "embedding": null, "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.25, "x1": 432.0, "y1": 242.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9f0cf87f-35fb-4ebf-99e7-130b29df2193", "node_type": "1", "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 250.54, "x1": 432.01, "y1": 605.54}]}, "hash": "bf57df5a3eea32f7d7db70d6139f7fdba54e4e31d92778c97dc2ee2959ec3d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cbc6b0e-d6c8-4117-9419-5bedeb3936eb", "node_type": "1", "metadata": {"bbox": [{"page": 427, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 105.88, "x1": 432.0, "y1": 256.35}]}, "hash": "d4da9590440c371a88c3ef8176ce81b562726edffd19904f694dd5c0b81a8d69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, you have to configure Mem0 with a vector store, an embedding model, an LLM, \nand a database. (We\u2019ll discuss how Mem0 uses these components shortly.) <br><br>*# Mem0 Configuration*\nconfig = { \n \"vector_store\": { \n \"provider\": \"chroma\",*# Database for semantic search*\n \"config\": { \n \"collection_name\": \"mem0_basic_example\", \n \"path\": \"/tmp/chroma_db\",*# Where to store memories*\n } \n }, \n \"llm\": { \n \"provider\": \"openai\",*# OPENAI_API_KEY needs to be set in env*\n \"config\": { \n \"model\": \"gpt-4o-mini\", \n \"temperature\": 0.1, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cbc6b0e-d6c8-4117-9419-5bedeb3936eb": {"__data__": {"id_": "0cbc6b0e-d6c8-4117-9419-5bedeb3936eb", "embedding": null, "metadata": {"bbox": [{"page": 427, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 105.88, "x1": 432.0, "y1": 256.35}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0c33b3bd-0066-4d00-916e-e32401fe9650", "node_type": "1", "metadata": {"bbox": [{"page": 426, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.25, "x1": 432.0, "y1": 242.44}]}, "hash": "587c9984fc63e39eb26e1ea48caddf6efa16f8a95dfd318a6075738f4bf8d5ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf420b2-2366-4251-a5cf-741686d1578a", "node_type": "1", "metadata": {"bbox": [{"page": 428, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.37, "x1": 432.0, "y1": 289.96}, {"page": 429, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.97, "x1": 432.01, "y1": 276.65}]}, "hash": "5a0b1274fd0410b98978e38c2542a9eb80606a6031297672f7fa8aed59d74c2a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can now inject the returned memories from the search into the request context \nfor LLM requests once. <br><br>That\u2019s it! As you can see, memory frameworks like Mem0 (LangMem is another) \nabstract away the details of how exactly the memories are retrieved and ranked. Let\u2019s \ntake a look at what\u2019s happening behind the scenes. <br><br>**How adding memories works**<br><br>When you call memory.add(messages, user_id=\"megan\"), the long-term memory is \nassociated with this user and persists across sessions. You can also specify other types \nof metadata: <br><br>client.add(messages, user_id=\"megan\", metadata={\"food\": \"vegan\"}) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdf420b2-2366-4251-a5cf-741686d1578a": {"__data__": {"id_": "bdf420b2-2366-4251-a5cf-741686d1578a", "embedding": null, "metadata": {"bbox": [{"page": 428, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.37, "x1": 432.0, "y1": 289.96}, {"page": 429, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.97, "x1": 432.01, "y1": 276.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0cbc6b0e-d6c8-4117-9419-5bedeb3936eb", "node_type": "1", "metadata": {"bbox": [{"page": 427, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 105.88, "x1": 432.0, "y1": 256.35}]}, "hash": "d4da9590440c371a88c3ef8176ce81b562726edffd19904f694dd5c0b81a8d69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea4a1bbb-846d-4f7e-8bba-c006efce7256", "node_type": "1", "metadata": {"bbox": [{"page": 430, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.15, "x1": 467.25, "y1": 337.34}]}, "hash": "0f8c1dfe090c9df31afb090b68a18b2de8b16b5a1a67bb66e619bb1f54fc97bf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main goal of the preprocessing is to keep relevant information like personal facts \n(\u201cI like to travel to Reykjavik.\u201d), preferences (\u201cI love pizza.\u201d), plans (\u201cI\u2019m traveling \nto Iceland.\u201d), and relationships (\u201cMegan is a customer.\u201d). At the same time, the LLM \nwill ignore irrelevant information such as small talk (\u201cNice day today.\u201d), common \ngreetings (\u201cHello, how are you?\u201d) and information that can easily be reconstructed, \nsuch as general knowledge (\u201cParis is the capital of France.\u201d) and math problems \n(\u201cWhat\u2019s 2 + 2?\u201d). <br><br>The relevant information is then embedded using an embedding model: <br><br>*# Convert memory text to numerical vector*\nembedding_response = openai_embeddings.create( \n model=\"text-embedding-3-small\", \n input=\"User wants to travel from Seattle to Reykjavik\" \n) <br><br>*# Gets back a 1536-dimensional vector like:*\n*# [0.023, -0.891, 0.445, ..., 0.123]*<br><br>*# Potentially Key-Value store for quick lookups*\nkv_store[\"user:megan:destination\"] = \"Reykjavik\" \nkv_store[\"user:megan:origin\"] = \"Seattle\" <br><br>At this point, the framework has stored a knowledge artifact about the cus\u2010 \ntomer Megan in the long-term memory. But what happens when you call mem \nory.search(query, user_id=\"megan\") during a future conversation? <br><br>**How searching memories works**<br><br>The memory.search operation retrieves memories by searching the vector database \nfor recent messages that are similar to the input query. Mem0 also identifies refer\u2010 \nenced entities and searches for related messages and user preferences in the database. \nIn this way, by calling memory.search, you can retrieve episodic, semantic, and \nprocedural memories. These results are ranked based on their relevance to the query, \nand you can also specify metadata filters (see the Mem0 documentation for details). <br><br>Let\u2019s look at this in detail. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea4a1bbb-846d-4f7e-8bba-c006efce7256": {"__data__": {"id_": "ea4a1bbb-846d-4f7e-8bba-c006efce7256", "embedding": null, "metadata": {"bbox": [{"page": 430, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.15, "x1": 467.25, "y1": 337.34}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bdf420b2-2366-4251-a5cf-741686d1578a", "node_type": "1", "metadata": {"bbox": [{"page": 428, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 98.37, "x1": 432.0, "y1": 289.96}, {"page": 429, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.97, "x1": 432.01, "y1": 276.65}]}, "hash": "5a0b1274fd0410b98978e38c2542a9eb80606a6031297672f7fa8aed59d74c2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d0f0250-d790-4f6e-bb02-65617a283d31", "node_type": "1", "metadata": {"bbox": [{"page": 431, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.22, "x1": 432.01, "y1": 293.73}]}, "hash": "0dc45ddecd5d836f0da4baf0290be26b2e2951cb8fa2a84d4fee162b8df877d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Procedural memory can be stored in a KV database like Redis, a graph database \nlike Neo4j, or a relational database like Postgres or SQLite. Depending on how you \nconfigure Mem0, it uses the appropriate querying mechanism: <br><br>*# Key-Value Database: Fast exact lookups*\nkv_store[\"user:megan:name\"] \u2192 \"Megan\" \nkv_store[\"user:megan:destination\"] \u2192 \"Reykjavik\" \nkv_store[\"user:megan:origin\"] \u2192 \"Seattle\" \n*# Graph Database: Relationship queries*\ngraph_db.query(\"MATCH (megan:Person)-[:WORKS_AT]->(company:Company) RETURN company\") \n*# \u2192 Finds Megan's workplace connections*\n*# Relational Database: Audit trail*\nhistory_db.query(\"SELECT*FROM memory_history WHERE user_id='megan' ORDER BY timestamp\") \n*# \u2192 Shows how Megan's memories evolved over time*<br><br>You can control the returned memories by using filters\u2014which are extra conditions \nthat you place on the memories. For example, you could set a filter so that the model \nextracts memories related to food preferences whenever it books a flight for a user: <br><br>query = \"Book a flight from SEA to KEF on June 6\" \nfilters = { \n \"AND\": [ \n { \n \"categories\": { \n \"contains\": \"food_preferences\" \n } ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d0f0250-d790-4f6e-bb02-65617a283d31": {"__data__": {"id_": "5d0f0250-d790-4f6e-bb02-65617a283d31", "embedding": null, "metadata": {"bbox": [{"page": 431, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.22, "x1": 432.01, "y1": 293.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ea4a1bbb-846d-4f7e-8bba-c006efce7256", "node_type": "1", "metadata": {"bbox": [{"page": 430, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.15, "x1": 467.25, "y1": 337.34}]}, "hash": "0f8c1dfe090c9df31afb090b68a18b2de8b16b5a1a67bb66e619bb1f54fc97bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d32703a-be8c-4c0b-9395-14e294fa723e", "node_type": "1", "metadata": {"bbox": [{"page": 432, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 143.25, "x1": 425.9, "y1": 360.54}]}, "hash": "30c84cf221104e20cee6b6fd82e922f6a4c4404b37cf728c77b41d16d0836993", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, you should use semantic memory rather than episodic memory. Episodic \nmemory stores all messages as-is and relies on finding relevant messages at runtime, \nand while storage is inexpensive, retrieving relevant messages can add considerable \nlatency. Memory errors due to cache misses are also quite hard to troubleshoot, so \ninstead of storing the messages as-is, it\u2019s often better to extract memorable informa\u2010 \ntion from the messages and store only those memories, which will be fewer and can \nbe searched more deterministically. <br><br>**References**\nThe types of memory were introduced by Sumers et al. (2023). Wang et. al. (2023) \nintroduced a latent-space approach to augmenting LLMs with long-term memory. \nThey used an adaptive residual side-network as a memory coretriever while keeping \nthe original LLM unchanged. At the time of writing, the leading edge of memory \nmanagement is the scalable Mem0 approach developed by Chhikara et al. (2025), \nwhich dynamically extracts and updates information in a labeled graph. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d32703a-be8c-4c0b-9395-14e294fa723e": {"__data__": {"id_": "3d32703a-be8c-4c0b-9395-14e294fa723e", "embedding": null, "metadata": {"bbox": [{"page": 432, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 143.25, "x1": 425.9, "y1": 360.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5d0f0250-d790-4f6e-bb02-65617a283d31", "node_type": "1", "metadata": {"bbox": [{"page": 431, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 99.22, "x1": 432.01, "y1": 293.73}]}, "hash": "0dc45ddecd5d836f0da4baf0290be26b2e2951cb8fa2a84d4fee162b8df877d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "369a3b8a-a71e-4be0-870c-51a739d1c073", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 351.5, "x1": 350.05, "y1": 604.6}]}, "hash": "73392f19dfc0ad53b033b28ee4f2319392e947ae37a4e1e23df683d404cfc5fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 8-2 summarizes the patterns we\u2019ve discussed in this chapter. <br><br>Table 8-2. Patterns for Meeting Constraints <br><br>**Patterns**\n**Small**\n**Language**\n**Model (SLM)**\n**(Pattern 24)**\n**Problems**\nThe foundational model \nyou\u2019re using is introducing \ntoo much latency or cost. <br><br>**Solutions**\nUse a small foundational model to fit \nwithin cost and latency constraints without \ncompromising unduly on quality by employing \nquantization (reducing the precision of \nmodel parameters), distillation (narrowing \nthe knowledge scope), or speculative coding \n(backstopping with a larger model). <br><br>**Usage scenarios**\nNarrow-scoped \nknowledge applications, \ncost reduction, edge \ndevice deployment, \nfaster inference \nrequirements, and GPU- \nconstrained \nenvironments. \nApplications with \nrepeated queries, cost \noptimization, interactive \napplications requiring \nfast responses, and \nmultitenant systems. <br><br>**Prompt**\n**Caching**\n**(Pattern 25)**<br><br>User requests follow \npatterns with repeated \nqueries. Recomputing the \nsame responses wastes \nresources and increases \ncosts. \nReuse previously generated responses (in the \ncase of client-side caching) and/or model \ninternal states (in the case of server-side \ncaching) for the same or similar prompts. The \nsimilarity can be based on prompt meaning \n(with semantic cache) or overlap (with prefix \ncaching). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "369a3b8a-a71e-4be0-870c-51a739d1c073": {"__data__": {"id_": "369a3b8a-a71e-4be0-870c-51a739d1c073", "embedding": null, "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 351.5, "x1": 350.05, "y1": 604.6}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3d32703a-be8c-4c0b-9395-14e294fa723e", "node_type": "1", "metadata": {"bbox": [{"page": 432, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 143.25, "x1": 425.9, "y1": 360.54}]}, "hash": "30c84cf221104e20cee6b6fd82e922f6a4c4404b37cf728c77b41d16d0836993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aa2b078-6037-47fa-930c-37af2349e0cb", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 357.66, "y0": 351.5, "x1": 427.56, "y1": 481.55}]}, "hash": "d3ff9960a8dc777dd1e0881caccd266e114ea535a0d28848ef539687c9d69035", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Patterns**\n**Inference**\n**Optimization**\n**(Pattern 26)**<br><br>**Problems**\nSelf-hosting LLMs brings \nwith it GPU constraints \nand hardware utilization \nchallenges. Real-time \napplications need faster \nresponse times. <br><br>**Solutions**\nThe pattern improves the efficiency of model \ninference by employing continuous batching \n(pulling requests from a queue and slotting \nthem into GPU cores as soon as they become \navailable), speculative decoding (efficiently \ncomputing the next set of tokens whenever \nthe smaller model is able to do so and \nbackstopping this with a large model), and/or \nprompt compression (preprocessing prompts to \nmake them shorter). \nA set of core metrics \u2014time to first token \n(TTFT), end-to-end request latency (EERL), \ntokens per second (TPS), and requests per \nsecond (RPS)\u2014and a variety of scalability and \nresilience metrics can help identify degradation \nof service quality. Targeted interventions can \nhelp improve specific metrics. \nLLM applications use various types of memory \n\u2014working, episodic, procedural, and semantic \n\u2014to maintain context, recall past interactions, \npersonalize responses, and retain key facts, \nrespectively. <br><br>**Degradation**\n**Testing**\n**(Pattern 27)**<br><br>You need metrics to help \nyou identify when service \nquality degrades and the \nconstraints under which the \napplication is bounded. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3aa2b078-6037-47fa-930c-37af2349e0cb": {"__data__": {"id_": "3aa2b078-6037-47fa-930c-37af2349e0cb", "embedding": null, "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 357.66, "y0": 351.5, "x1": 427.56, "y1": 481.55}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "369a3b8a-a71e-4be0-870c-51a739d1c073", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 351.5, "x1": 350.05, "y1": 604.6}]}, "hash": "73392f19dfc0ad53b033b28ee4f2319392e947ae37a4e1e23df683d404cfc5fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef20d96b-784d-4d7d-9060-405a11c3e355", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 308.3, "x1": 208.32, "y1": 403.7}]}, "hash": "9645421262d576eff64e05497f9ac6f4077772825c8be584541c4b93df1e5e82", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Preproduction testing, \nperformance validation, \nbottleneck identification, \ncapacity planning, \nongoing monitoring, and \noptimization. <br><br>Chatbots, multistep \nworkflows, \npersonalization, and \nprocessing large \ndocuments. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef20d96b-784d-4d7d-9060-405a11c3e355": {"__data__": {"id_": "ef20d96b-784d-4d7d-9060-405a11c3e355", "embedding": null, "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 308.3, "x1": 208.32, "y1": 403.7}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3aa2b078-6037-47fa-930c-37af2349e0cb", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 357.66, "y0": 351.5, "x1": 427.56, "y1": 481.55}]}, "hash": "d3ff9960a8dc777dd1e0881caccd266e114ea535a0d28848ef539687c9d69035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09d1b8a0-094b-4def-9a8b-38e6bfee07ca", "node_type": "1", "metadata": {"bbox": [{"page": 434, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 136.27, "x1": 432.01, "y1": 582.33}]}, "hash": "9e19fb1bef06584bc70c53585d7ab0c52fa2db869cc7af007382252bd3810a15", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Long-Term**\n**Memory**\n**(Pattern 28)**<br><br>LLM applications need \nto simulate memories \nof past interactions by \nprepending relevant history \nto each prompt, but \nthis approach can become \ncostly and inefficient with \nlong conversations, due to \ncontext window limitations. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09d1b8a0-094b-4def-9a8b-38e6bfee07ca": {"__data__": {"id_": "09d1b8a0-094b-4def-9a8b-38e6bfee07ca", "embedding": null, "metadata": {"bbox": [{"page": 434, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 136.27, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ef20d96b-784d-4d7d-9060-405a11c3e355", "node_type": "1", "metadata": {"bbox": [{"page": 433, "page_height": 661.5, "page_width": 504.0, "x0": 75.6, "y0": 308.3, "x1": 208.32, "y1": 403.7}]}, "hash": "9645421262d576eff64e05497f9ac6f4077772825c8be584541c4b93df1e5e82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc41c678-f51c-4920-ac6a-8b0f05e47c2d", "node_type": "1", "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 222.45, "x1": 432.0, "y1": 605.54}]}, "hash": "1294ab5fd46512b63e57732fcba38cbf9dbbd2df9243e2438bab411093e13440", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 9**\n**Setting Safeguards**<br><br>There\u2019s always a level of risk associated with GenAI applications. That\u2019s because they \nare built on top of foundational models, which are a nondeterministic technology \nthat has the potential to provide inaccurate or hallucinated answers. Foundational \nmodels are also a general-purpose technology, so their responses may not always \nalign with what you want them to do. <br><br>In this chapter, we discuss four patterns that can help you set safeguards around your \nGenAI applications. Template Generation (Pattern 29) is useful in situations where \nthe risk involved in sending content without human review is very high but human \nreview will not scale to the volume of communications. Assembled Reformat (Pattern \n30) helps in situations where content needs to be presented in an appealing way but \nthe risk posed by dynamically generated content is too high. Self-Check (Pattern 31) \nhelps you identify potential hallucinations cost-effectively. Finally, Guardrails (Pat\u2010 \ntern 32) are a catchall way to apply safeguards around your core GenAI applications \nto ensure that they operate within ethical, legal, and functional parameters. <br><br>**Pattern 29: Template Generation**\nThe Template Generation pattern reduces the number of items that need human \nreview by pregenerating templates that can be reviewed offline. At inference time, all \nthe application needs to do is deterministic string replacement on the reviewed tem\u2010 \nplate. This makes the final responses safe to send to consumers without additional \nreview. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc41c678-f51c-4920-ac6a-8b0f05e47c2d": {"__data__": {"id_": "cc41c678-f51c-4920-ac6a-8b0f05e47c2d", "embedding": null, "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 222.45, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "09d1b8a0-094b-4def-9a8b-38e6bfee07ca", "node_type": "1", "metadata": {"bbox": [{"page": 434, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 136.27, "x1": 432.01, "y1": 582.33}]}, "hash": "9e19fb1bef06584bc70c53585d7ab0c52fa2db869cc7af007382252bd3810a15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f26a159-0b16-4f5f-8520-db00ebf39179", "node_type": "1", "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.56, "x1": 2252.25, "y1": 213.64}]}, "hash": "476f9746bc3c6e19619a8cb84b022cd514583d5cfd958aea1d829a2788fc8f34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At inference time, the application just needs to plug values into the template and send \nout the notes. Because this process is deterministic and constrained, the chances of \nintroducing factual errors or toxic content at this stage are minimal. <br><br>**Example**\nYou operate a package tours company, and shortly after each booking, you send out a \npersonalized thank-you note from the guide who will lead the tour because it tends to \nreduce the number of cancellations. The code for this example is on GitHub. <br><br>**Pregeneration loop**<br><br>You can pregenerate a templated thank-you note for every combination of tour, \npackage type, and language that you offer and store these templates in a database for \neasy retrieval: <br><br>DESTINATIONS=[ \n \"Toledo, Spain\", \n \"Avila & Segovia\", \n \"Escorial Monastery\" \n] \nPACKAGE_TYPES=[ \n \"Family\", \n \"Individual\", \n \"Group\", \n \"Singles\" \n] \nLANGUAGES=[ \n \"English\", \n \"Polish\" \n] \n**for**dest**in**DESTINATIONS: \n**for**package_type**in**PACKAGE_TYPES: \n**for**lang**in**LANGUAGES: \n template = create_template(dest, package_type, lang) \n db.insert(dest, package_type, lang, template) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f26a159-0b16-4f5f-8520-db00ebf39179": {"__data__": {"id_": "0f26a159-0b16-4f5f-8520-db00ebf39179", "embedding": null, "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.56, "x1": 2252.25, "y1": 213.64}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "cc41c678-f51c-4920-ac6a-8b0f05e47c2d", "node_type": "1", "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 222.45, "x1": 432.0, "y1": 605.54}]}, "hash": "1294ab5fd46512b63e57732fcba38cbf9dbbd2df9243e2438bab411093e13440", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b24950f1-5349-4740-972e-308187b05fba", "node_type": "1", "metadata": {"bbox": [{"page": 438, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 166.76, "x1": 432.0, "y1": 234.05}]}, "hash": "7e3669cc1ea80bf7ae9bcc52a1bcbfc36ba234938a83f7dde9f5e2d7e74756dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Your business offers three destinations, four package types, and tours in two lan\u2010 \nguages. So, you will generate 3 \u00d7 4 \u00d7 2 = 24 templates. To create each template, you \ncan prompt a foundational model. <br><br>**Generating a template**<br><br>Some placeholders (such as language) will be replaced by the pregeneration loop, \nbut the generated template will still contain placeholders for items like the customer \nname. These remaining placeholders will have to be filled in before sending: <br><br>**def**create_template(tour_destination, package_type, language): \n prompt=f\"\"\" \n You are a tour guide working on behalf of Tours GenAI S.L. Write a personalized letter in {language} to a customer who has purchased a {package_type} tour package to visit {tour_destination}. Sound excited to see them and lead them on the tour. Explain some of the highlights of what they will see there and some of the things they can do while there. In the letter, use [CUSTOMER_NAME] to indicate the place to be replaced by their name and [TOUR_GUIDE] to indicate the place to be replaced by your name. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b24950f1-5349-4740-972e-308187b05fba": {"__data__": {"id_": "b24950f1-5349-4740-972e-308187b05fba", "embedding": null, "metadata": {"bbox": [{"page": 438, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 166.76, "x1": 432.0, "y1": 234.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0f26a159-0b16-4f5f-8520-db00ebf39179", "node_type": "1", "metadata": {"bbox": [{"page": 437, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.56, "x1": 2252.25, "y1": 213.64}]}, "hash": "476f9746bc3c6e19619a8cb84b022cd514583d5cfd958aea1d829a2788fc8f34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49d86a18-3e70-4626-879a-f252cd44527d", "node_type": "1", "metadata": {"bbox": [{"page": 439, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 138.22, "x1": 432.01, "y1": 389.13}]}, "hash": "193df77ace2bf4a70a1b209f37acd8fec3464dcac21034e115eebbb5a759e1ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Inference**<br><br>Whenever a tour is purchased or a tour guide is confirmed for the tour, your \napplication will invoke an email service with details of the tour. The application will \nretrieve the appropriate template from the database and replace the placeholders with \nstrings from the session to obtain the body of the email: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49d86a18-3e70-4626-879a-f252cd44527d": {"__data__": {"id_": "49d86a18-3e70-4626-879a-f252cd44527d", "embedding": null, "metadata": {"bbox": [{"page": 439, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 138.22, "x1": 432.01, "y1": 389.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b24950f1-5349-4740-972e-308187b05fba", "node_type": "1", "metadata": {"bbox": [{"page": 438, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 166.76, "x1": 432.0, "y1": 234.05}]}, "hash": "7e3669cc1ea80bf7ae9bcc52a1bcbfc36ba234938a83f7dde9f5e2d7e74756dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0007d41-3ddd-49fa-bfdf-cecc4612cf0c", "node_type": "1", "metadata": {"bbox": [{"page": 440, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 140.02, "x1": 432.01, "y1": 346.53}]}, "hash": "b00025c9c6e0791f229944dec816d4f15e3eb19a66e3d585bab97ecdc2a490de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can combine Template Generation with ML in a wide variety of personalization \nand customization scenarios\u2014for example, you can build a set of personalized land\u2010 \ning pages by using pregenerated templates and then use ML to select which templates \nto employ for a given marketing campaign or user profile. For personalized recom\u2010 \nmendations, you could use a traditional recommendations engine to select which \nproducts to show the user and pull in the appropriate pregenerated content (images \nand text) for that product. <br><br>You can also use this approach of combining pregeneration with ML if you\u2019re cus\u2010 \ntomizing your website for different customer journeys. In that case, you\u2019d pregenerate \nthe landing pages and use a propensity model (which is an ML model that predicts \nthe likelihood that an event, such as a purchase, will happen) to choose the next best \naction. <br><br>**References**\nMail merge is a feature that personalizes documents or emails for mass distribution by \ncombining a main template with data from a separate source. It dates back to a 1980s \nword processor called WordStar. The idea of creating these templates, rather than the \nfinal documents, with LLMs was introduced in 2024 by Valliappa Lakshmanan in an \narticle on balancing creativity and risk in GenAI applications. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0007d41-3ddd-49fa-bfdf-cecc4612cf0c": {"__data__": {"id_": "c0007d41-3ddd-49fa-bfdf-cecc4612cf0c", "embedding": null, "metadata": {"bbox": [{"page": 440, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 140.02, "x1": 432.01, "y1": 346.53}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "49d86a18-3e70-4626-879a-f252cd44527d", "node_type": "1", "metadata": {"bbox": [{"page": 439, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 138.22, "x1": 432.01, "y1": 389.13}]}, "hash": "193df77ace2bf4a70a1b209f37acd8fec3464dcac21034e115eebbb5a759e1ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "758e4d1a-e9e5-4722-8a57-359e0b30038d", "node_type": "1", "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 364.64, "x1": 739.25, "y1": 606.65}]}, "hash": "ceb9cca9a6843ccc1b0f8435e150e6801a8ef52ddbf62229cc4fab75151ae2eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is there a way to get the benefits of LLM generation without incurring the risks posed \nby LLMs\u2019 potential to introduce inaccurate or hallucinated information? <br><br>**Solution**\nThe Assembled Reformat pattern works if you can identify the characteristics of the \nproduct that would incur unacceptable risk if hallucinated, and you can assemble \nthese characteristics by using low-risk methods, such as reading from a database or \nusing a document data extraction system. <br><br>Creating the final result by either appending all the text that corresponds to the \nproduct attributes or putting them into some predefined structure results in text \nthat\u2019s accurate but not very pleasing to read. <br><br>Once you have this accurate text, put it into the context of the prompt and ask the \nLLM to rephrase, reword, or reformat it (see Figure 9-2). Text generated by these \nLLM tasks tends to be much lower risk than text generated from scratch, and the \nresulting text will be fluent and better suited to the content\u2019s purpose. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "758e4d1a-e9e5-4722-8a57-359e0b30038d": {"__data__": {"id_": "758e4d1a-e9e5-4722-8a57-359e0b30038d", "embedding": null, "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 364.64, "x1": 739.25, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c0007d41-3ddd-49fa-bfdf-cecc4612cf0c", "node_type": "1", "metadata": {"bbox": [{"page": 440, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 140.02, "x1": 432.01, "y1": 346.53}]}, "hash": "b00025c9c6e0791f229944dec816d4f15e3eb19a66e3d585bab97ecdc2a490de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f104c9fa-404b-4f46-bfd8-92f2c5f2f2d2", "node_type": "1", "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.33, "x1": 432.07, "y1": 355.26}]}, "hash": "cfd05c7af492ea812aff089ad1cb09df14ce1b1a02f2cfeace7a08596992c880", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "part_description='The wet end of a paper machine is the section where the paper web is formed. It is arguably the most important section of the machine.' \n*failure_modes=['Web breaks', 'Uneven sheet formation', 'Poor drainage'],*\nwarranty_period=3 \nprice='$23295 <br><br>The failure modes extracted from trusted sources have been italicized. By grounding \nsubsequent generation on these three failure modes, the LLM will always include \ncaveats on these three issues and will also not include other failure modes that the \npaper machine manufacturer might not want publicized. <br><br>Once you have the extracted content, you can pass it along to a more creative \nwriting engine that will reformat it to meet your desired goal, such as search engine \noptimization and/or being persuasive to potential buyers: <br><br>Write content in Markdown that will go in the Replacement Parts part of the manufac\u2010 \nturer\u2019s website. Include a placeholder for an image and include a description of the \nimage. Optimize the content for SEO. Also make it appealing to potential buyers. <br><br>**Part Information:**<br><br>{item} <br><br>The resulting text uses the information about the three \u201cacceptable\u201d failure modes in \na way that justifies why you might want to buy the part: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f104c9fa-404b-4f46-bfd8-92f2c5f2f2d2": {"__data__": {"id_": "f104c9fa-404b-4f46-bfd8-92f2c5f2f2d2", "embedding": null, "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.33, "x1": 432.07, "y1": 355.26}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "758e4d1a-e9e5-4722-8a57-359e0b30038d", "node_type": "1", "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 364.64, "x1": 739.25, "y1": 606.65}]}, "hash": "ceb9cca9a6843ccc1b0f8435e150e6801a8ef52ddbf62229cc4fab75151ae2eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "143e14f1-5368-4eba-8bd1-934e50b749db", "node_type": "1", "metadata": {"bbox": [{"page": 443, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 190.22, "x1": 432.0, "y1": 288.92}]}, "hash": "7ba1f8130ecf6456ff10732aa929af271049d3fef933794b308200189082c09b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The wet end is where the magic happens\u2014the initial formation of the paper web. A \npoorly functioning wet end can lead to a cascade of problems, including: <br><br>***Web Breaks:**Frequent web breaks are a major source of downtime and waste of \nmaterials. <br><br>***Uneven Sheet Formation:**Inconsistent sheet formation results in lower-quality \npaper and potential customer complaints. <br><br>***Poor Drainage:**Inefficient drainage reduces machine speed and increases energy \nconsumption. <br><br>Investing in genuine replacement parts for your wet end is an investment in the overall \nefficiency and quality of your paper production. <br><br>**Considerations**\nEven though the two steps (assembly and reformatting) are low risk, you should \nvalidate the results. For example, if possible, extract the data in two ways to see if you \nget the same result. You can also use Pattern 31, Self-Check (which we cover next), for \nthis purpose. Similarly, even though reformatting is low-risk, you should validate that \nthe generated content retains the raw data that should not be hallucinated. You can \nuse Pattern 17, LLM-as-Judge, for this. <br><br>If you\u2019re considering using Assembled Reformat, first consider whether Pattern 29, \nTemplate Generation, will suit your needs\u2014its ability to review all templates provides \nan extra safeguard. Choose Assembled Reformat only in situations where you can\u2019t ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "143e14f1-5368-4eba-8bd1-934e50b749db": {"__data__": {"id_": "143e14f1-5368-4eba-8bd1-934e50b749db", "embedding": null, "metadata": {"bbox": [{"page": 443, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 190.22, "x1": 432.0, "y1": 288.92}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f104c9fa-404b-4f46-bfd8-92f2c5f2f2d2", "node_type": "1", "metadata": {"bbox": [{"page": 442, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 81.33, "x1": 432.07, "y1": 355.26}]}, "hash": "cfd05c7af492ea812aff089ad1cb09df14ce1b1a02f2cfeace7a08596992c880", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "008def5d-53f9-47ef-a967-b103713b7417", "node_type": "1", "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 394.04, "x1": 432.01, "y1": 605.54}]}, "hash": "312ca17abc20d58e9cd8d6f94ada8e95823e8a147b14477d05534b81437b3cb7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As LLM providers have improved the coverage of their training data and method\u2010 \nologies, hallucination rates for common tasks and queries have dropped steadily. \nFor example, Vectara measured the top 25 LLMs\u2019 hallucination rates on a text \nsummarization task, as shown in Figure 9-3. In December 2024, the best LLM was \nhallucinating at a rate of 1.3% and the 25th best was hallucinating at a rate of 4.1%. \nWhen Vectara tested the same measure on the same task in April 2025, hallucination \nrates had dropped by 40% to 50% across the board\u2014the best LLM\u2019s hallucination rate \nwas now 0.7% and that of the 25th was 2.4%. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "008def5d-53f9-47ef-a967-b103713b7417": {"__data__": {"id_": "008def5d-53f9-47ef-a967-b103713b7417", "embedding": null, "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 394.04, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "143e14f1-5368-4eba-8bd1-934e50b749db", "node_type": "1", "metadata": {"bbox": [{"page": 443, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 190.22, "x1": 432.0, "y1": 288.92}]}, "hash": "7ba1f8130ecf6456ff10732aa929af271049d3fef933794b308200189082c09b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5856735-225f-44fa-bd38-13238b5a1600", "node_type": "1", "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 74.88, "x1": 432.0, "y1": 382.47}]}, "hash": "913fbfda451e425e1f5e91e9d2099fa9d25847233db04a5d1a76408876894e42", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "extracted from invoices, receipts, and the like are hallucinated! Moreover, if you now \ntake the output of the data extraction and pass it as the input context of other LLM \ncalls, the chances of errors occurring compound\u2014even if you have error detection \nand correction in place. The more complex your LLM-calling chain, the more likely it \nis that the LLM responses will be hallucinated. <br><br>Is there a way to determine whether an LLM is hallucinating? Suppose you have three \nLLMs that were trained in very different ways on nonoverlapping datasets. You could \nuse these three LLMs to generate responses and compare their responses. When \nthe responses differ, there is a high likelihood of hallucination\u2014and while it might \nbe unclear what the correct answer is, at least you\u2019d know that there\u2019s a potential \nproblem. Intuitively, then, you can use the variability of LLM responses to identify \npotential hallucination trouble spots. <br><br>However, the frontier models\u2019 training datasets overlap quite a lot, so it would be \ndifficult to source three such nonoverlapping LLMs. Also, inference with multiple \nLLMs will multiply the costs. Is there a way to look at an LLM response and identify \npotential hallucination trouble spots? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5856735-225f-44fa-bd38-13238b5a1600": {"__data__": {"id_": "b5856735-225f-44fa-bd38-13238b5a1600", "embedding": null, "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 74.88, "x1": 432.0, "y1": 382.47}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "008def5d-53f9-47ef-a967-b103713b7417", "node_type": "1", "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 394.04, "x1": 432.01, "y1": 605.54}]}, "hash": "312ca17abc20d58e9cd8d6f94ada8e95823e8a147b14477d05534b81437b3cb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa1a463d-73cc-44e2-b387-a214ec70d339", "node_type": "1", "metadata": {"bbox": [{"page": 448, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "540aabc10e06e1c7d700c9e20d2e3508273b329592a2b5bb7a7e85dce1914fa9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Solution**\nAs we discussed in Chapter 1 and Chapter 2, many LLMs provide, along with the \ngenerated tokens, those tokens\u2019 logprobs (also called logits). From the logprobs, you \ncan calculate the probability of a specific token being the correct one as follows: <br><br>elogit <br><br>In situations where the generated token is the overwhelming favorite, this probabil\u2010 \nity will be close to 100%. In situations where there are multiple possibilities (see \nFigure 1-5), the probability of the \u201cwinning\u201d token being the correct one will be \nlower. <br><br>**Requesting and processing logprobs**<br><br>You can ask OpenAI to return the logprobs of each token along with the response \ntext. The following code also asks for the five leading candidates at each step (this \ncode is on Github): <br><br>message = client.chat.completions.create( \n model=\"gpt-3.5-turbo\", \n messages=[ \n ... \n ], \n logprobs=**True**, \n top_logprobs=5 \n ) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa1a463d-73cc-44e2-b387-a214ec70d339": {"__data__": {"id_": "fa1a463d-73cc-44e2-b387-a214ec70d339", "embedding": null, "metadata": {"bbox": [{"page": 448, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b5856735-225f-44fa-bd38-13238b5a1600", "node_type": "1", "metadata": {"bbox": [{"page": 445, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 74.88, "x1": 432.0, "y1": 382.47}]}, "hash": "913fbfda451e425e1f5e91e9d2099fa9d25847233db04a5d1a76408876894e42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98cd11e6-2a19-4f4b-9a1d-241f673ea579", "node_type": "1", "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 318.84, "x1": 432.01, "y1": 605.54}]}, "hash": "f8d3953a3850d898526bc1d318f018dca6af93e7ddbb4f61e58159a87c595a21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model responds with this: <br><br>John Cole Howard is a fictional character from the TV show**The**Office, portrayed by \nactor**Ed**Helms. <br><br>The tokens that were selected even though they didn\u2019t reach a 50% probability were \nThe and Ed (which are bolded in the preceding response and shown in Figure 9-5). <br><br><br><br>Figure 9-5. Logprobs at low-confidence tokens <br><br>This is likely because Ed Helms\u2019 character in The Office is Andy Bernard and there is \nno famous person named John Cole Howard. Hence the low probability on the name \nof the show and on the name of the actor: the model is pretty much guessing at this \npoint. <br><br>**Identifying hallucinations**<br><br>Given that the model emits tokens with low values of logprobs when it has to choose \nfrom many equally likely options, you could threshold the logprobs and use low \nprobabilities as indicators of possible hallucinations. However, you have to be careful \nabout false positives. As the Atat\u00fcrk example indicates, there are valid reasons for a \ncorrect answer to include tokens that have low probabilities. <br><br>There are several approaches that you can use to limit the false positives (also see \nFigure 9-6): <br><br>Identify tokens of interest <br><br>To limit false positives, you can focus on checking the logprobs only on specific \ntokens. This is often possible when you\u2019re generating structured output because \nit\u2019s possible to determine the positions of the key values to validate and check the \nlogprobs of only those tokens. We illustrate this in the following Example section. <br><br>**Pattern 31: Self-Check** **|** **431**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98cd11e6-2a19-4f4b-9a1d-241f673ea579": {"__data__": {"id_": "98cd11e6-2a19-4f4b-9a1d-241f673ea579", "embedding": null, "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 318.84, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fa1a463d-73cc-44e2-b387-a214ec70d339", "node_type": "1", "metadata": {"bbox": [{"page": 448, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 40.5, "x1": 432.0, "y1": 605.54}]}, "hash": "540aabc10e06e1c7d700c9e20d2e3508273b329592a2b5bb7a7e85dce1914fa9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ecf0323-cb7e-4855-a4d5-73fd3325119d", "node_type": "1", "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 217.64, "x1": 432.01, "y1": 309.74}]}, "hash": "6d77a33ba5ba18c1384fc381a936d971b78f8b88a2bfe7e25dc4f39ee79b9ed8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample generated sequences <br><br>Another way to limit false positives is to use the sequence generation approach \nwhenever you encounter low-probability tokens (see the \u201cPattern 1: Logits Mask\u2010 \ning\u201d section in Chapter 2 for a detailed walkthrough of sequence generation). \nThe idea is that you can generate multiple sequences and validate that they all \nagree on the answer. <br><br>In the case of the Atat\u00fcrk example, even though the sentences may start differ\u2010 \nently, both generations would have agreed on his birth year being 1881. You \ncan compare whether the answers are substantially the same by comparing the \nembeddings of the two generations. <br><br>Normalize statistics over all tokens <br><br>Calculating aggregate statistics over long answers can underestimate (in the case \nof averages) or overestimate (in the case of minimums) the hallucination poten\u2010 \ntial. An aggregate statistic that normalizes the logits for sequences of different \nlengths is perplexity, which is defined as follows: <br><br>N logitsi \n\u03a3i \n1 \nN \nperplexity = e\u2212 <br><br>So, the perplexity is the number of alternatives between which the model is \nchoosing. The lower the perplexity, the more confident the model is in the \ngenerated sequence. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ecf0323-cb7e-4855-a4d5-73fd3325119d": {"__data__": {"id_": "5ecf0323-cb7e-4855-a4d5-73fd3325119d", "embedding": null, "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 217.64, "x1": 432.01, "y1": 309.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98cd11e6-2a19-4f4b-9a1d-241f673ea579", "node_type": "1", "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 318.84, "x1": 432.01, "y1": 605.54}]}, "hash": "f8d3953a3850d898526bc1d318f018dca6af93e7ddbb4f61e58159a87c595a21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a62e534c-dc0a-4f23-8ff5-c02fd7227d2a", "node_type": "1", "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.28, "x1": 432.01, "y1": 605.54}]}, "hash": "d916ddd44583cbb3ca6a4a2cd9c3bc640ba470676b845777f7c2ca6f36f7cd51", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Build an ML model <br><br>You can treat the token probabilities of specific probabilities, the distance \nbetween embeddings of generated sequences, aggregate and normalized statistics, \nand contextual features as input features into an ML model that\u2019s trained on your \ndata and specific use case to detect hallucination. <br><br>Using a bespoke ML model is the most robust approach because it builds on all of \nthese methods. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a62e534c-dc0a-4f23-8ff5-c02fd7227d2a": {"__data__": {"id_": "a62e534c-dc0a-4f23-8ff5-c02fd7227d2a", "embedding": null, "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.28, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5ecf0323-cb7e-4855-a4d5-73fd3325119d", "node_type": "1", "metadata": {"bbox": [{"page": 449, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 217.64, "x1": 432.01, "y1": 309.74}]}, "hash": "6d77a33ba5ba18c1384fc381a936d971b78f8b88a2bfe7e25dc4f39ee79b9ed8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b8156ca-1b27-4cf4-a97d-1f7db75c1c48", "node_type": "1", "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 144.91, "x1": 432.0, "y1": 316.7}]}, "hash": "2702af43bc2f210ff1491972c55769e1ff95fd0141e9f1c5b40f15b7e69b1f23", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "you\u2019ll need to extract four numbers from each receipt: the billed amount, the tax, the \ntip, and the total amount. <br><br>Suppose your extraction code returns the following four values: <br><br>312.32,28.76,60,401.08 <br><br>If you extract all four numbers, the total amount acts as a checksum\u2014you can calcu\u2010 \nlate it from the other three numbers to confirm that the extracted value is correct. <br><br>But suppose you can extract only three of the numbers (perhaps the fourth image is \nsmudged). You get this back: <br><br>312.32,28.76,,400 <br><br>You want the LLM to impute the number that could not be extracted, and you can do \nthat with a prompt (the full code is on GitHub):3 <br><br>You are a helpful AI assistant that helps parse restaurant receipts. <br><br>I will give you a set of parsed values containing the following on each line: <br><br>billed_amount, tax, tip, paid_amount <br><br>If tax is missing, calculate it as 9.21% of the billed_amount. <br><br>If the tip is missing, calculate it as (paid_amount - billed_amount - tax). <br><br>If the paid_amount is missing, calculate it as (billed_amount + tax + tip). <br><br>Do not add any headers or explanations. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b8156ca-1b27-4cf4-a97d-1f7db75c1c48": {"__data__": {"id_": "8b8156ca-1b27-4cf4-a97d-1f7db75c1c48", "embedding": null, "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 144.91, "x1": 432.0, "y1": 316.7}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a62e534c-dc0a-4f23-8ff5-c02fd7227d2a", "node_type": "1", "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.28, "x1": 432.01, "y1": 605.54}]}, "hash": "d916ddd44583cbb3ca6a4a2cd9c3bc640ba470676b845777f7c2ca6f36f7cd51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "977e71d7-d3d9-4a86-9cbd-9b0d74875be5", "node_type": "1", "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 181.38, "x1": 432.0, "y1": 465.06}]}, "hash": "7848330e2c051ab5a2163e10666f3889bd1815599d2a46b72ade51735be73308", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can parse the LLM result as follows: <br><br>parse_result(response_text=message.choices[0].message.content, \n logprobs=message.choices[0].logprobs) <br><br>The content can be parsed into a Pandas DataFrame: <br><br>**def**parse_result(response_text, logprobs) -> pd.DataFrame: \n csv_file = StringIO(response_text) \n result_df = pd.read_csv(csv_file, header=**None**, \n names=['billed_amount', 'tax', 'tip', 'paid_amount']) <br><br>Along with the content, the model sends back logprobs. You can associate each \nreturned token with the line it appears in and compute the lowest-confidence token \nbased on that line: <br><br>line_no = 0 \nconfidence_of_line = 1.0 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "977e71d7-d3d9-4a86-9cbd-9b0d74875be5": {"__data__": {"id_": "977e71d7-d3d9-4a86-9cbd-9b0d74875be5", "embedding": null, "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 181.38, "x1": 432.0, "y1": 465.06}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8b8156ca-1b27-4cf4-a97d-1f7db75c1c48", "node_type": "1", "metadata": {"bbox": [{"page": 451, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 144.91, "x1": 432.0, "y1": 316.7}]}, "hash": "2702af43bc2f210ff1491972c55769e1ff95fd0141e9f1c5b40f15b7e69b1f23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a49ea3f-f97e-48bb-8e60-522669e5a6ce", "node_type": "1", "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.78, "x1": 432.01, "y1": 173.28}]}, "hash": "021a4716a05950532c03e79f6f0bf6ae063786c95715342b15ded101b9419f9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "312.32,28.76,60,401.08 <br><br>312.32,28.76,,400 <br><br>312.32,28.76,60, <br><br>312.21,,50, <br><br>312.43,,,400 <br><br>300,27.63,60,387.63 <br><br>You might get back the Pandas DataFrame shown in Table 9-1: <br><br>Table 9-1. Pandas DataFrame generated by GPT-40-mini <br><br>**billed_amount**\n312.32 \n312.32 \n312.32 \n312.21 \n312.43 \n300.00  **Tax**\n28.76 \n28.76 \n28.76 \n28.84 \n28.80 \n27.63  **Tip**\n60.0 \n60.0 \n60.0 \n50.0 \n60.0 \n60.0  **paid_amount**\n401.08 \n400.00 \n400.08 \n391.05 \n401.23 \n387.63  **Confidence**\n0.962668 \n0.551552 \n0.562172 \n0.172516 \n0.170295 \n0.999290 <br><br>Note how, in the last column, the confidence is high only for the two rows where \nnothing was imputed. The confidence is moderate (around 0.55) for the two rows \nwhere only one value had to be imputed, and it\u2019s low (0.17) for the columns where \ntwo values were imputed. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a49ea3f-f97e-48bb-8e60-522669e5a6ce": {"__data__": {"id_": "2a49ea3f-f97e-48bb-8e60-522669e5a6ce", "embedding": null, "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.78, "x1": 432.01, "y1": 173.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "977e71d7-d3d9-4a86-9cbd-9b0d74875be5", "node_type": "1", "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 181.38, "x1": 432.0, "y1": 465.06}]}, "hash": "7848330e2c051ab5a2163e10666f3889bd1815599d2a46b72ade51735be73308", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "647c441a-317b-44ae-ad0a-d3b676414870", "node_type": "1", "metadata": {"bbox": [{"page": 453, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.44, "x1": 432.01, "y1": 370.49}]}, "hash": "e268def1c1563fe91cb93441504807482bbe7c7503862b11b2eb90fe50038416", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can identify, solely by looking at the confidence value, which rows of the \nparsed table are problematic. Indeed, if you calculate the checksum error in the \npaid_amount, the rows with no error have a confidence above 0.9. In a complex chain \nwhere thousands of tokens are being generated in a constrained way, you can use the \ngenerating LLM\u2019s own confidence scores to identify potentially problematic outputs. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "647c441a-317b-44ae-ad0a-d3b676414870": {"__data__": {"id_": "647c441a-317b-44ae-ad0a-d3b676414870", "embedding": null, "metadata": {"bbox": [{"page": 453, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.44, "x1": 432.01, "y1": 370.49}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2a49ea3f-f97e-48bb-8e60-522669e5a6ce", "node_type": "1", "metadata": {"bbox": [{"page": 452, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 111.78, "x1": 432.01, "y1": 173.28}]}, "hash": "021a4716a05950532c03e79f6f0bf6ae063786c95715342b15ded101b9419f9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a6ac874-6cee-4cb3-b708-393e7f9c2b4c", "node_type": "1", "metadata": {"bbox": [{"page": 454, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.64, "x1": 432.01, "y1": 287.74}]}, "hash": "5f3774428689fc14e9a891ee2cb54dd517e41bbd23e30da2167062bb04db4356", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nManakul, Liusie, and Gales (2023) introduced the idea of using logprobs for \nhallucination detection. They used sequence generation, called their detector Self\u2010 \nCheckGPT, and suggested using a separate LLM solely to generate logprobs when the \nproprietary LLM doesn\u2019t itself provide logprobs. Quevedo et al. (2024) trained an ML \nclassifier on the token probabilities output by the LLM to detect hallucinations. Val\u2010 \nentin et al. (2024) compared various ways of using logprobs to detect hallucinations. <br><br>**Pattern 32: Guardrails**\nGuardrails are layers of code that operate on an LLM\u2019s inputs, outputs, context, \nand tool parameters to safeguard the LLM from malicious actors and ensure that it \noperates within allowed parameters. <br><br>**Problem**\nWhen building AI applications, you\u2019ll often need to ensure that they operate within \nspecific ethical, legal, and functional parameters. For example, you may need to \nsafeguard your AI applications in the following areas: <br><br>Security <br><br>Protecting your AI systems from malicious inputs typically requires you to guard \nagainst prompt injection attacks and jailbreaking. Prompt injections exploit the \nfact that LLMs process both system prompts and user inputs as text, which ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a6ac874-6cee-4cb3-b708-393e7f9c2b4c": {"__data__": {"id_": "6a6ac874-6cee-4cb3-b708-393e7f9c2b4c", "embedding": null, "metadata": {"bbox": [{"page": 454, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.64, "x1": 432.01, "y1": 287.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "647c441a-317b-44ae-ad0a-d3b676414870", "node_type": "1", "metadata": {"bbox": [{"page": 453, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 70.44, "x1": 432.01, "y1": 370.49}]}, "hash": "e268def1c1563fe91cb93441504807482bbe7c7503862b11b2eb90fe50038416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67ba3954-8bcc-498d-af0f-622c8f06da03", "node_type": "1", "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 332.84, "x1": 432.01, "y1": 606.65}]}, "hash": "f9359d4ff6e68cb9a958fabfdb19ef6f58c14e7898ef1e84609159f34280fa0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Alignment <br><br>You may need to ensure that LLM outputs adhere to specific guidelines, company \npolicies, or ethical principles. For instance, your organization may require that all \ncompany communications adhere to its specific policies, guidelines, and brand \nvoice, or that outputs avoid mentioning competitors or topics such as politics \nor religion. You may also operate in a jurisdiction or industry that imposes \nethical boundaries to prevent bias and discrimination and ensure fairness among \ndifferent demographic groups. <br><br>While these factors are important, you don\u2019t want to sprinkle your application code \nwith security, privacy, and content checks. A large error-handling surface area will be \nhard to maintain and enforce. Can you safeguard your AI applications in all these \nareas while keeping the maintenance overhead low? ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67ba3954-8bcc-498d-af0f-622c8f06da03": {"__data__": {"id_": "67ba3954-8bcc-498d-af0f-622c8f06da03", "embedding": null, "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 332.84, "x1": 432.01, "y1": 606.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6a6ac874-6cee-4cb3-b708-393e7f9c2b4c", "node_type": "1", "metadata": {"bbox": [{"page": 454, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.64, "x1": 432.01, "y1": 287.74}]}, "hash": "5f3774428689fc14e9a891ee2cb54dd517e41bbd23e30da2167062bb04db4356", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f23b029c-2575-46b2-bddc-0e113bde4e22", "node_type": "1", "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.26, "x1": 458.75, "y1": 324.74}]}, "hash": "a81a269ea6becbc9261c57465ecfc6d854616fc976e6c1f232037e7725b022fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " ) \n) <br><br>Frameworks such as Nvidia\u2019s NeMo, Guardrails AI, and LLM Guard provide prebuilt \nguardrails for common functionalities such as checking for jailbreaks, masking sensi\u2010 \ntive data in the input, and hallucinations. <br><br>For example, here\u2019s how to use LLM Guard to scan a prompt for toxic language: <br><br>**from llm_guard.input_scanners import**Toxicity <br><br>scanner = Toxicity(threshold=0.5, match_type=MatchType.SENTENCE) \nsanitized_prompt, is_valid,_= scanner.scan(prompt) <br><br>The scanner takes a string input and returns a sanitized version of the input (which \nis useful for guardrails that redact PII, for example) and decides whether the input \nshould be allowed. <br><br>Protecting against prompt injection is very similar: <br><br>scanner = PromptInjection(threshold=0.5, match_type=MatchType.FULL) \nsanitized_prompt, is_valid,_= scanner.scan(prompt) <br><br>In both cases, the framework uses post-trained small language models (SLMs) to \ncarry out the task. For toxicity, it uses the unitary/unbiased-toxic-roberta library, \nand for prompt injection, it uses the ProtectAI/deberta-v3-base-prompt-injection-v2 \nlibrary. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f23b029c-2575-46b2-bddc-0e113bde4e22": {"__data__": {"id_": "f23b029c-2575-46b2-bddc-0e113bde4e22", "embedding": null, "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.26, "x1": 458.75, "y1": 324.74}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "67ba3954-8bcc-498d-af0f-622c8f06da03", "node_type": "1", "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 332.84, "x1": 432.01, "y1": 606.65}]}, "hash": "f9359d4ff6e68cb9a958fabfdb19ef6f58c14e7898ef1e84609159f34280fa0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f25bf879-6e54-40b0-9093-b72b50ee1594", "node_type": "1", "metadata": {"bbox": [{"page": 457, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.1, "x1": 552.25, "y1": 223.87}]}, "hash": "79bf397c39e6ca6f6170969e8ef9a57e25969a6c4ebda3d165dcee19a8e981c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, LLM Guard also supports rejecting strings that match a regular expression: <br><br>scanner = Regex( \n patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"],*# List of regex patterns*\n is_blocked=**True**,*# If True, patterns are treated as 'bad'*\n match_type=MatchType.SEARCH,*# Can be SEARCH or FULL_MATCH*\n redact=**True**,*# Enable or disable redaction*\n) \nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt) <br><br>**Custom guardrails**<br><br>You can build custom guardrails by implementing the logic in your code, prompting \nfoundational models, or post-training an SLM. Here\u2019s a guardrail that illustrates how \nto use Pattern 17, LLM-as-Judge, to reject prompts on specific topics: <br><br>banned_topics = [ \n \"religion\", \"politics\", \"sexual innuendo\" \n] \nsystem_prompt=f\"\"\" \nI will give you a piece of text. Check whether the text touches on any of these topics. <br><br> {banned_topics} <br><br>Return True or False, with no preamble or special markers. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f25bf879-6e54-40b0-9093-b72b50ee1594": {"__data__": {"id_": "f25bf879-6e54-40b0-9093-b72b50ee1594", "embedding": null, "metadata": {"bbox": [{"page": 457, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.1, "x1": 552.25, "y1": 223.87}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f23b029c-2575-46b2-bddc-0e113bde4e22", "node_type": "1", "metadata": {"bbox": [{"page": 456, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.26, "x1": 458.75, "y1": 324.74}]}, "hash": "a81a269ea6becbc9261c57465ecfc6d854616fc976e6c1f232037e7725b022fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d92fb4cd-5b10-4116-8bb1-f8050aa6a773", "node_type": "1", "metadata": {"bbox": [{"page": 458, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 273.65}]}, "hash": "161afa0a45d728749a50c76144819511bb5aa5b413204ed2372fbc3a936f0d7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Implementing guardrails**<br><br>Suppose you want the system to prevent users from sending you PII. You want any \nproper names in prompts to be replaced by generic identifiers, and you can do that \nwith a custom guardrail: <br><br>**def**guardrail_replace_names(to_scan: str): \n llm = ... \n system_prompt=\"\"\" \nI will give you a piece of text. In that piece of text, replace any personal names with a generic identifier. <br><br>Example: \n Input: \n I met Sally in the store. \n Output: ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d92fb4cd-5b10-4116-8bb1-f8050aa6a773": {"__data__": {"id_": "d92fb4cd-5b10-4116-8bb1-f8050aa6a773", "embedding": null, "metadata": {"bbox": [{"page": 458, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 273.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f25bf879-6e54-40b0-9093-b72b50ee1594", "node_type": "1", "metadata": {"bbox": [{"page": 457, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 72.1, "x1": 552.25, "y1": 223.87}]}, "hash": "79bf397c39e6ca6f6170969e8ef9a57e25969a6c4ebda3d165dcee19a8e981c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7edaee02-5ed8-4618-8055-0e64f95ccd43", "node_type": "1", "metadata": {"bbox": [{"page": 459, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.56, "x1": 432.01, "y1": 328.22}]}, "hash": "a157d3d6e82c5e039530709f26f6f12dae38a628b3b25f7bff8c2d59733a6671", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**return**{ \n \"guardrail_type\": \"Banned Topic\", \n \"activated\": is_banned, \n \"should_stop\": is_banned, \n \"sanitized_output\": to_scan, \n } <br><br>To make it easier to apply multiple guardrails, all of your guardrail functions should \nhave the same signature. <br><br>**Wrapping the query engine**<br><br>You can wrap the query engine provided by LlamaIndex with a set of guardrails: <br><br>**class GuardedQueryEngine**(RetrieverQueryEngine): \n**def** __init__(self, query_engine: RetrieverQueryEngine): \n self._query_engine = query_engine <br><br>**def**query(self, query): \n*# apply guardrails to inputs*", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7edaee02-5ed8-4618-8055-0e64f95ccd43": {"__data__": {"id_": "7edaee02-5ed8-4618-8055-0e64f95ccd43", "embedding": null, "metadata": {"bbox": [{"page": 459, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.56, "x1": 432.01, "y1": 328.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d92fb4cd-5b10-4116-8bb1-f8050aa6a773", "node_type": "1", "metadata": {"bbox": [{"page": 458, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.46, "x1": 432.0, "y1": 273.65}]}, "hash": "161afa0a45d728749a50c76144819511bb5aa5b413204ed2372fbc3a936f0d7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fe6c804-8fd5-4168-bd4e-4a95a469d7bf", "node_type": "1", "metadata": {"bbox": [{"page": 460, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.63, "x1": 432.01, "y1": 292.47}]}, "hash": "9df5a071052a4bed53c0751bc4f4fccf70f5652ed12fc2b6c7f9fdcb7761dc02", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The modified version would be as follows: <br><br>Would a man be an appealing match if he were not wealthy? <br><br>**Considerations**\nGuardrails introduce considerable engineering complexity and latency into your \napplication architecture. The most complex way in which you could choose to \nimplement a GenAI application is by deploying it alongside custom post-trained \nguardrails, so make sure that this complexity is warranted. Perhaps there are less \nexpensive ways you can safeguard against the key risks you\u2019re concerned about. <br><br>Even though the guardrails in the example were implemented by calling out to a \nfrontier model API, this need not be the case: it\u2019s possible to use SLMs to keep latency \nwithin manageable limits. <br><br>It is not necessary to run the guardrails and LLM code sequentially\u2014for example, you \ncould run input and/or retrieval guardrails in parallel with the incoming request to \navoid slowing down the user\u2019s request: <br><br>**try**: \ninput_guardrail_results, turn_result =**await**asyncio.gather( \n apply_guardrails( \n ... ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fe6c804-8fd5-4168-bd4e-4a95a469d7bf": {"__data__": {"id_": "2fe6c804-8fd5-4168-bd4e-4a95a469d7bf", "embedding": null, "metadata": {"bbox": [{"page": 460, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.63, "x1": 432.01, "y1": 292.47}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7edaee02-5ed8-4618-8055-0e64f95ccd43", "node_type": "1", "metadata": {"bbox": [{"page": 459, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 69.56, "x1": 432.01, "y1": 328.22}]}, "hash": "a157d3d6e82c5e039530709f26f6f12dae38a628b3b25f7bff8c2d59733a6671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a420fffc-cdbe-4a75-b515-55452e7d50ab", "node_type": "1", "metadata": {"bbox": [{"page": 461, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 344.05, "x1": 426.8, "y1": 605.63}]}, "hash": "d481e301cfaf89ef43768c75bfdc08ad8fedf7f68e7399999d6483d19e0675f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**References**\nDong et al. (2024) explain why point solutions for guardrails don\u2019t work and a \ncomprehensive approach is needed. The OWASP security project classified prompt \ninjection scenarios. <br><br>QED42 built prompt-based guardrails with policies, exceptions, and few-shot exam\u2010 \nples around an LLM-powered search application at a legal entity. The guardrails \nfiltered out out-of-domain (nonlegal) query inputs and ensured that the outputs were \nrelevant. Acrolinx uses AI guardrails that are implemented with LLM-as-Judge to \nmaintain brand voice consistency across content. <br><br>**Summary**\nIn this chapter, we explored four patterns for implementing safety mechanisms in \nAI applications, and we addressed critical concerns regarding security, data privacy, \ncontent moderation, hallucination prevention, and ethical alignment. Table 9-2 sum\u2010 \nmarizes these patterns. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a420fffc-cdbe-4a75-b515-55452e7d50ab": {"__data__": {"id_": "a420fffc-cdbe-4a75-b515-55452e7d50ab", "embedding": null, "metadata": {"bbox": [{"page": 461, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 344.05, "x1": 426.8, "y1": 605.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2fe6c804-8fd5-4168-bd4e-4a95a469d7bf", "node_type": "1", "metadata": {"bbox": [{"page": 460, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 76.63, "x1": 432.01, "y1": 292.47}]}, "hash": "9df5a071052a4bed53c0751bc4f4fccf70f5652ed12fc2b6c7f9fdcb7761dc02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21ba9a70-50d9-45c6-8c11-2c6e37237db5", "node_type": "1", "metadata": {"bbox": [{"page": 462, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.87, "x1": 432.01, "y1": 582.33}]}, "hash": "482f2a040abfc147d485d5338da7fbb57d476b25798d73045d3b511e6eae372b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 9-2. Patterns for teaching capability <br><br>**Patterns**\n**Template**\n**Generation**\n**(Pattern 29)**\n**Problems**\nThe risk of sending content without \nhuman review is very high, but \nhuman review will not scale to the \nvolume of communications. <br><br>**Solutions**\nPregenerate templates that are \nreviewed beforehand. Inference time \nrequires only deterministic string \nreplacement, and it\u2019s therefore safe to \ndirectly send to consumers. \nReduce the risk of generating \ninaccurate or hallucinated content \nby separating the task of content \ncreation into two low-risk steps\u2014first, \nassembling data in low-risk ways and \nsecond, formatting the content based \non that data. \nUse token probabilities to detect \nhallucination in LLM responses. <br><br>**Assembled**\n**Reformat**\n**(Pattern 30)**\nContent needs to be presented in \nan appealing way, but the risk \nposed by dynamically generated \ncontent is too high. <br><br>Any situation where factual \n(as opposed to creative) \nresponses are needed. \nAnytime your application \ncould be subject to attacks \nby malicious adversaries. \n**Guardrails**\n**(Pattern 32)**<br><br>You need safeguards for security, \ndata privacy, content moderation, \nhallucination, and alignment to \nensure that AI applications operate \nwithin ethical, legal, and functional \nparameters.  Wrap the LLM calls with a layer of \ncode that preprocesses the information \ngoing into the model and/or post- \nprocesses the output of the model. \nKnowledge retrieval and tool use will \nalso need to be protected. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21ba9a70-50d9-45c6-8c11-2c6e37237db5": {"__data__": {"id_": "21ba9a70-50d9-45c6-8c11-2c6e37237db5", "embedding": null, "metadata": {"bbox": [{"page": 462, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.87, "x1": 432.01, "y1": 582.33}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a420fffc-cdbe-4a75-b515-55452e7d50ab", "node_type": "1", "metadata": {"bbox": [{"page": 461, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 344.05, "x1": 426.8, "y1": 605.63}]}, "hash": "d481e301cfaf89ef43768c75bfdc08ad8fedf7f68e7399999d6483d19e0675f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5730284-ad9a-4bd4-b93f-8fea33c716d2", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 371.74, "x1": 432.0, "y1": 605.54}]}, "hash": "52eebedf14d2fde6a367c675f002a49289833de8ab88d1c51d1b6109c69cc9ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CHAPTER 10**\n**Composable Agentic Workflows**<br><br>In this chapter, we pull together the patterns from the first nine chapters into an \napplication that demonstrates how patterns interact with one another to enable you \nto build production-ready agentic applications that get better over time. <br><br>The full code for this chapter can be found in the GitHub repository of this book. \nPlease open the code in your favorite Python IDE and follow along with us. <br><br>**Agentic Workflow**\nRather than build out a full application in this chapter, we\u2019ll demonstrate a vertical \nslice of an application that corresponds to a customer use case. In GenAI, vertical \nslices like this often correspond to a user-driven (manual) workflow that is in the \nprocess of becoming AI assisted or fully automated. AI assistants are called copilots, \nand autonomous AI are termed agents. Any application on this spectrum is termed \nagentic. Thus, what we are demonstrating in the GitHub repository is an agentic \nworkflow. <br><br>The workflow is intended to create educational content; this is the same use case we \nexplored in the description of Pattern 23, Multiagent Collaboration. It is depicted in \nFigure 10-1. Note that unlike in Figure 7-5, there are two stages of review (instead of \njust one) and the data being transferred is depicted. Also, there are a few more writers \nin the content-writing team, including one who will write on GenAI topics based on \nthe content of this book. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5730284-ad9a-4bd4-b93f-8fea33c716d2": {"__data__": {"id_": "d5730284-ad9a-4bd4-b93f-8fea33c716d2", "embedding": null, "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 371.74, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "21ba9a70-50d9-45c6-8c11-2c6e37237db5", "node_type": "1", "metadata": {"bbox": [{"page": 462, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 142.87, "x1": 432.01, "y1": 582.33}]}, "hash": "482f2a040abfc147d485d5338da7fbb57d476b25798d73045d3b511e6eae372b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e481ac7a-d91a-46ac-bb45-ae6fc8209ba9", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 80.24, "y0": 151.63, "x1": 423.75, "y1": 351.72}]}, "hash": "62bdd67e93702a0e69f97521c63d08a81b2ad937ce3c0e09f7a5785326c799ae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unlike in Pattern 23, here, we\u2019ll build the workflow without using any multiagent \nframeworks. An influential article from Anthropic on building effective agents notes \nthat \u201cthe most successful implementations use simple, composable patterns rather \nthan complex frameworks,\u201d which is reminiscent of the Unix philosophy. Now that \nwe\u2019ve explored a set of composable patterns in this book, let\u2019s use them to build an \neffective multiagent, multistep workflow in an LLM-agnostic and cloud-agnostic way. <br><br>**Running the Application**\nThere are two ways to run the code: as an AI assistant (copilot) or as a fully autono\u2010 \nmous application (agent). Regardless of how you run the application, both require the \nsame setup. <br><br>**Setup**<br><br>In a virtual environment (see sidebar), install the necessary dependencies: <br><br>python -m pip install -r requirements.txt <br><br>Then, edit the keys.env file and add your Gemini API key to it. If you don\u2019t have a \nGemini API key, you can get one from Google\u2019s AI Studio. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e481ac7a-d91a-46ac-bb45-ae6fc8209ba9": {"__data__": {"id_": "e481ac7a-d91a-46ac-bb45-ae6fc8209ba9", "embedding": null, "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 80.24, "y0": 151.63, "x1": 423.75, "y1": 351.72}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d5730284-ad9a-4bd4-b93f-8fea33c716d2", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 371.74, "x1": 432.0, "y1": 605.54}]}, "hash": "52eebedf14d2fde6a367c675f002a49289833de8ab88d1c51d1b6109c69cc9ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "109031cb-3ece-435c-b7bb-92916a291764", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.99, "x1": 432.0, "y1": 128.29}]}, "hash": "6046f66ba246151d220f4b04697b7773ae53dacfc6b839a96e455a0b9fa8d63a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Virtual Environments for Python**\nYou\u2019ll usually need a wide variety of external libraries in any Python project. Often, \nthese will require specific versions that may clash with what you\u2019ve used for other \nprojects. If you were to have a single Python installation, these libraries would conflict \nand cause all sorts of problems. Therefore, we recommend that you use a virtual \nenvironment, which is a sandboxed Python environment that maintains independent \nversions of Python libraries. <br><br>You can create a virtual environment by using the venv module: <br><br>python -m venv agentic_ai/ <br><br>Next, activate the virtual environment by running the script that\u2019s included in its \ninstallation. <br><br>$ source agentic_ai/bin/activate \n(agentic_ai) $ <br><br>The rest of the commands in this chapter will be executed within the shell. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "109031cb-3ece-435c-b7bb-92916a291764": {"__data__": {"id_": "109031cb-3ece-435c-b7bb-92916a291764", "embedding": null, "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.99, "x1": 432.0, "y1": 128.29}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e481ac7a-d91a-46ac-bb45-ae6fc8209ba9", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 80.24, "y0": 151.63, "x1": 423.75, "y1": 351.72}]}, "hash": "62bdd67e93702a0e69f97521c63d08a81b2ad937ce3c0e09f7a5785326c799ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7035445f-bc91-45cd-b6e1-cb5dbe990b24", "node_type": "1", "metadata": {"bbox": [{"page": 466, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 607.79}]}, "hash": "23e48766633870656d7f122c6d357031a92bc51cff6b5274abd7aae10ed5d6eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, you don\u2019t need to use Gemini: you can change which LLM(s) the applica\u2010 \ntion uses by editing the appropriate models in utils/llms.py. You can see a list of \nsupported models on PydanticAI\u2019s documentation page. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7035445f-bc91-45cd-b6e1-cb5dbe990b24": {"__data__": {"id_": "7035445f-bc91-45cd-b6e1-cb5dbe990b24", "embedding": null, "metadata": {"bbox": [{"page": 466, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 607.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "109031cb-3ece-435c-b7bb-92916a291764", "node_type": "1", "metadata": {"bbox": [{"page": 464, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 91.99, "x1": 432.0, "y1": 128.29}]}, "hash": "6046f66ba246151d220f4b04697b7773ae53dacfc6b839a96e455a0b9fa8d63a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "524c5041-4330-48e9-8291-afb62bca493e", "node_type": "1", "metadata": {"bbox": [{"page": 467, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.250003}]}, "hash": "d578e9102300cfe6fd42508dd054dea5b351cc3188fde8cce92c6e7a096356b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Agent mode**<br><br>To run the application as an autonomous agent, run this code: <br><br>python cmdline_app.py <br><br>When the autonomous agent executes a workflow, the result is what will happen in \ncopilot mode if the user accepts all the AI recommendations, makes no changes, and \nsimply clicks the Next button on each page. Therefore, as we discuss how the copilot \nmode behaves, you can infer how the autonomous application will behave. <br><br>**What the Application Does**\nThe copilot mode workflow starts with the webpage shown in Figure 10-2. <br><br>**The first step of the workflow**<br><br>The end user specifies a topic, such as \u201cBattle of the Bulge,\u201d and clicks Next. The \nfirst step of the workflow involves the Task Assigner agent processing the topic (see \nFigure 10-1). This agent chooses the best writer to tackle the topic (see Figure 10-3 \nand Figure 10-4). <br><br><br><br>Figure 10-3. The Task Assigner agent finds the best writer for the given topic <br><br>**Agentic Workflow** **|** **449**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "524c5041-4330-48e9-8291-afb62bca493e": {"__data__": {"id_": "524c5041-4330-48e9-8291-afb62bca493e", "embedding": null, "metadata": {"bbox": [{"page": 467, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.250003}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7035445f-bc91-45cd-b6e1-cb5dbe990b24", "node_type": "1", "metadata": {"bbox": [{"page": 466, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 607.79}]}, "hash": "23e48766633870656d7f122c6d357031a92bc51cff6b5274abd7aae10ed5d6eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62f07fc0-a4b1-4c2a-9265-322d606af6c0", "node_type": "1", "metadata": {"bbox": [{"page": 468, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "hash": "fe44f800741e7e259a034a4dd4376848f9f073582637d7b2a25a843a08722887", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 10-4. The Task Assigner agent determines that the history content-writing agent \nis the best writer for an article on the Battle of the Bulge <br><br>**Human feedback**<br><br>By default, the task of writing on the \u201cBattle of the Bulge\u201d topic is assigned to the \nhistory content\u2013writing agent. This agent will get the task if the end user clicks the \nNext button. However, the user has the option to change to a different writing agent \nthan the recommended one, as shown in Figure 10-5. If they do so, then their human \nfeedback is logged, and you get a line of feedback that\u2019s similar to what is shown in \nFigure 10-6. <br><br>**450** **|** **Chapter 10: Composable Agentic Workflows**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62f07fc0-a4b1-4c2a-9265-322d606af6c0": {"__data__": {"id_": "62f07fc0-a4b1-4c2a-9265-322d606af6c0", "embedding": null, "metadata": {"bbox": [{"page": 468, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "524c5041-4330-48e9-8291-afb62bca493e", "node_type": "1", "metadata": {"bbox": [{"page": 467, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.0, "y1": 602.250003}]}, "hash": "d578e9102300cfe6fd42508dd054dea5b351cc3188fde8cce92c6e7a096356b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccd841cc-111d-4d65-86f0-92bcf98d657a", "node_type": "1", "metadata": {"bbox": [{"page": 469, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "9a962eb392a76896d4fe1f5ee9d1d9a364949e7ab348ad2bc7645b27911d9e50", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "<br><br>Figure 10-5. End user changing the writer to something other than the AI\u2019s recommen\u2010 \ndation, which is logged as human feedback <br><br>Figure 10-6. Viewing human feedback in the logs <br><br>Every piece of user-generated content should incorporate this pattern of providing \nAI recommendations and implicitly obtaining human feedback through the UI. Your \nproject team should include a good UX designer to ensure comprehensive, unobtru\u2010 \nsive feedback collection. <br><br>**Agentic Workflow** **|** **451**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccd841cc-111d-4d65-86f0-92bcf98d657a": {"__data__": {"id_": "ccd841cc-111d-4d65-86f0-92bcf98d657a", "embedding": null, "metadata": {"bbox": [{"page": 469, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "62f07fc0-a4b1-4c2a-9265-322d606af6c0", "node_type": "1", "metadata": {"bbox": [{"page": 468, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 602.249995}]}, "hash": "fe44f800741e7e259a034a4dd4376848f9f073582637d7b2a25a843a08722887", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3d6b1b3-5cfd-49de-9f74-1b6ca1bbbfd1", "node_type": "1", "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 162.94, "x1": 432.01, "y1": 605.54}]}, "hash": "039c4d2477ac94c428402f8fb0f012617886a8048252ef0332b9545eb1c60dc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are three options for the next page of the workflow, which presents the draft \narticle generated by the history-writing agent to the user. (Anthropic, which intro\u2010 \nduced the concept, calls this kind of editable entity an artifact.) In the first option, \nwhich we\u2019ve gone with here (see Figure 10-7), the draft article is displayed in text \nboxes that allow for easy editing. The user can edit the draft text and list of keywords \ndirectly. <br><br><br><br>Figure 10-7. Users can modify the generated draft either directly or by using a chat \ninterface <br><br>A second option would be to display the draft as regular text but provide an Edit \nbutton to allow users to modify it directly. <br><br>A third option for allowing users to change the artifact, which is also shown in \nFigure 10-7, is through commands in a chat interface. Users can ask the agent to add \nmore keywords that are related to the location of the battle. Such commands add to \nthe prompts that the agent in question is already employing, so to enable targeted \nediting, it\u2019s important for the application to keep track of each user\u2019s context in order: \nthat is, what the user has said before, what they\u2019re currently seeing, what the current \nprompt is, and whether any of the user\u2019s previous instructions are relevant to the \ncurrent artifact. Pattern 28, Long-Term Memory, is helpful for this purpose. <br><br>**452** **|** **Chapter 10: Composable Agentic Workflows**", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3d6b1b3-5cfd-49de-9f74-1b6ca1bbbfd1": {"__data__": {"id_": "d3d6b1b3-5cfd-49de-9f74-1b6ca1bbbfd1", "embedding": null, "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 162.94, "x1": 432.01, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ccd841cc-111d-4d65-86f0-92bcf98d657a", "node_type": "1", "metadata": {"bbox": [{"page": 469, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 40.5, "x1": 432.01, "y1": 605.54}]}, "hash": "9a962eb392a76896d4fe1f5ee9d1d9a364949e7ab348ad2bc7645b27911d9e50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da88303b-6d89-4c91-a3b0-94d45156a116", "node_type": "1", "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.05, "x1": 432.0, "y1": 154.84}]}, "hash": "763c4a0f1aa245dfcde4c8f9291acee9967f1e67946f9ae440aad929840ad6b0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Your UX design should change depending on how much context the prompt needs \nto include. If only the current step of the workflow is needed, put the chat prompt \nwithin the page. If the context needs to include the entire workflow, place the prompt \nin a side panel. <br><br>**System Architecture**\nThe system architecture involves five interacting components: <br><br>\u2022 \n\u2022 Agents that implement each step of the workflow <br><br>\u2022 \n\u2022 A multiagent architecture that advances end users through the workflow <br><br>\u2022 \n\u2022 Governance, monitoring, and security components <br><br>\u2022 \n\u2022 A learning pipeline to continuously improve AI capability <br><br>\u2022 \n\u2022 An ongoing data creation, collection, and curation program <br><br>Let\u2019s look at each of these five pieces in turn. <br><br>**Agent patterns**<br><br>To let users train agents to do the tasks that form the workflow, the UX should allow \nusers to complete the work without referencing any external system. They can do this \nby using tools or APIs to bring any necessary data into the application. If a user has \nto go outside the application to hunt for information to enter, then the agent won\u2019t \nhave the context it needs to recommend decisions, learn when recommendations are \nincorrect, and eventually learn to make decisions autonomously. <br><br>Each step of the workflow is executed by one or more agents, and each agent can \nbe implemented independently of the others. These agents can follow a plan of \naction (with Chain of Thought [Pattern 13]), retrieve necessary data (with Basic RAG \n[Pattern 6] and Index-Aware Retrieval [Pattern 9]), and call tools as needed (with \nTool Calling [Pattern 21]). In all this, they can recover from errors (with Reflection \n[Pattern 18] and Self-Check [Pattern 31]) or be implemented to trade off risk and \ncreativity (with Template Generation [Pattern 29] and Assembled Reformat [Patterns \n30]). It\u2019s important to choose an abstraction level and framework for each agent that \nlets you easily implement the patterns that are necessary for it. For example, for an \nagent that requires RAG, you might start with LlamaIndex as the base framework. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da88303b-6d89-4c91-a3b0-94d45156a116": {"__data__": {"id_": "da88303b-6d89-4c91-a3b0-94d45156a116", "embedding": null, "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.05, "x1": 432.0, "y1": 154.84}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d3d6b1b3-5cfd-49de-9f74-1b6ca1bbbfd1", "node_type": "1", "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 162.94, "x1": 432.01, "y1": 605.54}]}, "hash": "039c4d2477ac94c428402f8fb0f012617886a8048252ef0332b9545eb1c60dc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f96fe2d-77d6-4f19-917a-992b886f5596", "node_type": "1", "metadata": {"bbox": [{"page": 471, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.04, "x1": 552.25, "y1": 312.65}]}, "hash": "63e37cfaca602b3e204a3766dae5a741b999441074bc450e2a162a205ceec065", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our application, the summary step is carried out by a Panel Secretary agent (see \nthis code) that is implemented by using PydanticAI and is set up by means of a \nsystem prompt: <br><br>from pydantic_ai import Agent \nclass PanelSecretary: \n def__init__(self): \n system_prompt = PromptService.render_prompt(\"secretary_system_prompt\") ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f96fe2d-77d6-4f19-917a-992b886f5596": {"__data__": {"id_": "5f96fe2d-77d6-4f19-917a-992b886f5596", "embedding": null, "metadata": {"bbox": [{"page": 471, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.04, "x1": 552.25, "y1": 312.65}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "da88303b-6d89-4c91-a3b0-94d45156a116", "node_type": "1", "metadata": {"bbox": [{"page": 470, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 73.05, "x1": 432.0, "y1": 154.84}]}, "hash": "763c4a0f1aa245dfcde4c8f9291acee9967f1e67946f9ae440aad929840ad6b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c27fa68-8aba-478a-ab1d-e20d13509793", "node_type": "1", "metadata": {"bbox": [{"page": 472, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.67, "x1": 432.0, "y1": 346.94}]}, "hash": "760181659fe5bdf05ee6e23a2708bf190d5c16a80d39b12c3e71bc44d690a1a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "async def write_response(self, topic: str, prompt: str) -> Article: \n*# semantic RAG*\n nodes = self.retriever.retrieve(topic) \n ... <br><br>Once the agent has been set up, it can consolidate a set of article reviews into a single \nset of summarized instructions: <br><br>async def consolidate(self, topic: str, article: Article, reviews_so_far: List[Tuple[Reviewer, str]]) -> str: \n reviews_text = [] \n**for**reviewer, review**in**reviews_so_far: \n reviews_text.append(f\"BEGIN review by {reviewer.name}:\\n{review}\\nEND review\\n\") <br><br> prompt = PromptService.render_prompt(\"Secretary_consolidate_reviews\", \n topic=topic, \n article=article, \n reviews=reviews_text) \n result = await self.agent.run(prompt) \n**return**result.output <br><br>In this code, you can see the state of the workflow (the topic, the article being \nreviewed, and the reviews so far) being carried through to the context of the prompt. \nYou can also see that it uses await and async to allow this code to be concurrent. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c27fa68-8aba-478a-ab1d-e20d13509793": {"__data__": {"id_": "2c27fa68-8aba-478a-ab1d-e20d13509793", "embedding": null, "metadata": {"bbox": [{"page": 472, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.67, "x1": 432.0, "y1": 346.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5f96fe2d-77d6-4f19-917a-992b886f5596", "node_type": "1", "metadata": {"bbox": [{"page": 471, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 79.04, "x1": 552.25, "y1": 312.65}]}, "hash": "63e37cfaca602b3e204a3766dae5a741b999441074bc450e2a162a205ceec065", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1278f72-b643-4d56-8ebd-c87c44d07812", "node_type": "1", "metadata": {"bbox": [{"page": 473, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.16, "x1": 501.25, "y1": 324.15}]}, "hash": "1a85fea852356c595598bd409c90c93f841ea2ad0208a7926f0c8337c5bd9e1a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, whenever \nagents/ \ngeneric_writer_agent.py), it searches for relevant instructions in the long-term mem\u2010 \nory and adds them to the prompt: <br><br>the writer  creates  the  initial  draft  (see <br><br>prompt_vars = { \n \"prompt_name\": f\"GenericWriter_write_about\", \n \"content_type\": get_content_type(self.writer), \n \"additional_instructions\": ltm.search_relevant_memories( \n f\"{self.writer.name}, write about {topic}\"), \n \"topic\": topic \n} \nprompt = PromptService.render_prompt(**prompt_vars) <br><br>**Multiagent architecture**<br><br>These individual agents have to be orchestrated into a workflow. The agents can be \ninvoked one after the other in an agent mode (see this code): <br><br>**async def**write_about(self, topic: str) -> Article: \n*# Step 1: Identify who can write on this topic*\n writer = WriterFactory.create_writer(**await**self.find_writer(topic)) <br><br>*# Step 2: Ask the writer to create an initial draft*\n logger.info(f\"Assigning {topic} to {writer.name()}\") \n draft =**await**writer.write_about(topic) <br><br>*# Step 3: Get the review panel to review the article*", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1278f72-b643-4d56-8ebd-c87c44d07812": {"__data__": {"id_": "e1278f72-b643-4d56-8ebd-c87c44d07812", "embedding": null, "metadata": {"bbox": [{"page": 473, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.16, "x1": 501.25, "y1": 324.15}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2c27fa68-8aba-478a-ab1d-e20d13509793", "node_type": "1", "metadata": {"bbox": [{"page": 472, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 71.67, "x1": 432.0, "y1": 346.94}]}, "hash": "760181659fe5bdf05ee6e23a2708bf190d5c16a80d39b12c3e71bc44d690a1a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de80ca07-9d74-43c0-949f-63d0cd41d499", "node_type": "1", "metadata": {"bbox": [{"page": 474, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.11, "x1": 432.01, "y1": 346.93}]}, "hash": "0c48b217f9ffdb211ef84e188eb99fc2fcec65842ece539e99ba356f33a13d21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The page invokes the next agent when the user clicks the Next button: <br><br>**if**st.button(\"Next\"): \n ... \n st.switch_page(\"pages/3_PanelReview1.py\") <br><br>In cases where there is a choice to be made or logic to be carried out, you should \nimplement the logic directly. This direct control is the main advantage of not using a \nmultiagent framework. For example, whenever the user modifies the text of the draft, \nyou can invoke the appropriate writer agent to rewrite the article: <br><br>**def**modify_draft(): \n modify_instruction = st.session_state.modify_instruction \n logger.info(f\"Updating draft to instructions: {modify_instruction}\") \n draft = asyncio.run(writer.revise_article(topic, st.session_state.draft, modify_instruction)) \n logger.info(draft.full_text) \n st.session_state.draft = draft*# but keep the original as \"ai_generated_draft\"*\n*# because this is a callback, it redraws the page*<br><br>**with**st.form(\"Modification form\", clear_on_submit=**True**): \n st.text_input(label=\"Modification instructions\", value=\"\", key=\"modify_instruction\") \n st.form_submit_button(label=\"Modify\",**on_click=modify_draft**) ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de80ca07-9d74-43c0-949f-63d0cd41d499": {"__data__": {"id_": "de80ca07-9d74-43c0-949f-63d0cd41d499", "embedding": null, "metadata": {"bbox": [{"page": 474, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.11, "x1": 432.01, "y1": 346.93}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e1278f72-b643-4d56-8ebd-c87c44d07812", "node_type": "1", "metadata": {"bbox": [{"page": 473, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.16, "x1": 501.25, "y1": 324.15}]}, "hash": "1a85fea852356c595598bd409c90c93f841ea2ad0208a7926f0c8337c5bd9e1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2026a230-eb08-4765-abd0-2547f6414398", "node_type": "1", "metadata": {"bbox": [{"page": 475, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.14, "x1": 432.01, "y1": 299.16}]}, "hash": "458ec7ce2bd1122f40c221de74ab2496c89747870622a734b607b2700e80514c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CONDITION**\n{{ condition }} <br><br>The guardrail raises an exception if the input is unacceptable: <br><br>**async def**is_acceptable(self, prompt: str, raise_exception=**False**) -> bool: \n result =**await**self.agent.run(prompt) \n**if not**result.output: \n**raise**InputGuardrailException(f\"{self.id} failed on {prompt}\") \n**return True**<br><br>By making sure to invoke the guardrail anytime it is given a topic, the Task Assigner \nensures that an exception is thrown and the workflow is stopped if the input topic is \nunacceptable: <br><br>*# guardrail is applied in parallel; it will raise an exception*\n_, result =**await**asyncio.gather( \n self.topic_guardrail.is_acceptable(topic), \n self.agent.run(prompt) \n) \n**return**result.output <br><br>This code uses an asynchrony trick: it prevents the guardrail from slowing down \nthe original operation by starting both tasks at once. If the guardrail check fails, the \nsecond call is also terminated. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2026a230-eb08-4765-abd0-2547f6414398": {"__data__": {"id_": "2026a230-eb08-4765-abd0-2547f6414398", "embedding": null, "metadata": {"bbox": [{"page": 475, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.14, "x1": 432.01, "y1": 299.16}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "de80ca07-9d74-43c0-949f-63d0cd41d499", "node_type": "1", "metadata": {"bbox": [{"page": 474, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 97.11, "x1": 432.01, "y1": 346.93}]}, "hash": "0c48b217f9ffdb211ef84e188eb99fc2fcec65842ece539e99ba356f33a13d21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c175af1f-26d5-4f67-964c-f8ead1fb0da3", "node_type": "1", "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 305.84, "x1": 432.0, "y1": 605.54}]}, "hash": "f3ada342d3fef59b208a839e46f3f5c1d2aaf5225ec29d76326f4f32096151b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": " st.switch_page(\"pages/3_PanelReview1.py\") <br><br>In copilot mode, you can switch to the admin console to view log files. You saw an \nexample of feedback in Figure 10-6, and the first line of the feedback depicted there \nis a notification that the AI suggested the wrong writing agent for the input \u201cSquaring \nthe circle.\u201d It suggested the math-writing agent, but the human changed that to the \nhistory-writing agent. The second line shows that an initial draft was modified. Both \nthe AI-generated draft and the human-modified draft are saved. In this way, through \nthe normal operation of the user interface, human feedback data is collected (see \nFigure 10-8). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c175af1f-26d5-4f67-964c-f8ead1fb0da3": {"__data__": {"id_": "c175af1f-26d5-4f67-964c-f8ead1fb0da3", "embedding": null, "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 305.84, "x1": 432.0, "y1": 605.54}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2026a230-eb08-4765-abd0-2547f6414398", "node_type": "1", "metadata": {"bbox": [{"page": 475, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 183.14, "x1": 432.01, "y1": 299.16}]}, "hash": "458ec7ce2bd1122f40c221de74ab2496c89747870622a734b607b2700e80514c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74b10294-e51d-4de2-bac6-10e8f9de2922", "node_type": "1", "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.95, "x1": 432.0, "y1": 293.03}]}, "hash": "80395312320317d60434bb2e18f1a9cc70d4ffcd1014e0c83e032364a7f23ec4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "good the keywords are, you need to log the keywords. This happens whenever an \ninitial draft is created because that code issues the following call: <br><br>**from composable_app.utils import**save_for_eval**as**evals \n... \nevals.record_ai_response(\"initial draft\", \n ai_input=prompt_vars, \n ai_response=initial_draft) <br><br>An example of an offline evaluation using the generated logs is shown in evals/evalu\u2010 \nate_keywords.py. <br><br>You often will not know how good the outcomes of an activity are until many days \nlater. It\u2019s important to be able to tie together the workflow steps and compare them \nagainst outcomes to derive evaluation metrics that are relevant to the business. For \nexample, the metrics on this education use case might be appeal (such as the number \nof teachers who include the topic in their lesson plans), engagement (such as the \nnumber of students who read through till the end), and functional performance (such \nas the fraction of students who correctly answer a national exam question on the \ntopic). <br><br>The human feedback and offline evaluations can be used in patterns such as Content \nOptimization (Pattern 5), Adapter Tuning (Pattern 15) and Prompt Optimization \n(Pattern 20) to more closely align the AI agent with human preferences and business \noutcomes. In consumer-facing applications that need to handle millions of requests \na day, logging every prompt may slow down performance unacceptably. If this hap\u2010 \npens, you may need to put a sampling strategy in place. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74b10294-e51d-4de2-bac6-10e8f9de2922": {"__data__": {"id_": "74b10294-e51d-4de2-bac6-10e8f9de2922", "embedding": null, "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.95, "x1": 432.0, "y1": 293.03}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c175af1f-26d5-4f67-964c-f8ead1fb0da3", "node_type": "1", "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 305.84, "x1": 432.0, "y1": 605.54}]}, "hash": "f3ada342d3fef59b208a839e46f3f5c1d2aaf5225ec29d76326f4f32096151b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bbdb5f1-333e-4f38-b4e8-97566a6b5fc1", "node_type": "1", "metadata": {"bbox": [{"page": 478, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 93.83, "x1": 432.0, "y1": 319.93}]}, "hash": "cb0a3e67657d45507d710b27be3bef2e27f18a5f1eb288988488e9a01531975e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Data program**<br><br>In many practical situations, the learning pipeline as described in the previous section \nmay be insufficient,due to the following factors: <br><br>Data size <br><br>Often, you won\u2019t get enough human corrections to let you rely solely on organic \nhuman feedback to improve the AI generations. <br><br>Data complexity <br><br>Most workflows will involve relatively simple operations, whereas high-value \nactivities will involve more complex but rarer inputs. <br><br>Detailed feedback <br><br>Experts often dramatically change the output in the very last step instead of at \nthe point where the AI makes a mistake, thus obscuring which agent needs to be \ntrained on the feedback. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bbdb5f1-333e-4f38-b4e8-97566a6b5fc1": {"__data__": {"id_": "2bbdb5f1-333e-4f38-b4e8-97566a6b5fc1", "embedding": null, "metadata": {"bbox": [{"page": 478, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 93.83, "x1": 432.0, "y1": 319.93}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "74b10294-e51d-4de2-bac6-10e8f9de2922", "node_type": "1", "metadata": {"bbox": [{"page": 477, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 103.95, "x1": 432.0, "y1": 293.03}]}, "hash": "80395312320317d60434bb2e18f1a9cc70d4ffcd1014e0c83e032364a7f23ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65f279e3-1db4-486b-98ec-48a5931b7f14", "node_type": "1", "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 79.41, "x1": 424.79, "y1": 444.81}]}, "hash": "884599b7a3a66886b072721508e18148ec4e73533e27a4b7f70f01b1d63a4ef3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Modularity and reusability <br><br>Each component can be reused in different applications and readily reconfigured \nor versioned to adapt to evolving business needs or different clients. This brings \nenhanced flexibility, scalability, and efficiency. You can use Dependency Injection \n(Pattern 19) to allow agents to be developed and tested independently, even if \nthey rely on the outputs of previous agents in the workflow. <br><br>Technical flexibility <br><br>Composable architectures allow organizations to select the best tools for each \nspecific need. This lets them adapt rapidly to changing market conditions and \ntechnology requirements without major system overhauls. <br><br>Standard protocols, tools, and packages <br><br>Using patterns doesn\u2019t mean that you need to implement everything from \nscratch. You can leverage standard protocols and libraries because of the open\u2010 \nness of the design. At the time of writing, these includes frameworks like Pydan\u2010 \nticAI that let you be LLM-agnostic, packages like LllamaIndex and Mem0 that \nsimplify common needs like the building of RAG systems and managing long- \nterm memory, and protocols like MCP and A2A that standardize the way you ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65f279e3-1db4-486b-98ec-48a5931b7f14": {"__data__": {"id_": "65f279e3-1db4-486b-98ec-48a5931b7f14", "embedding": null, "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 79.41, "x1": 424.79, "y1": 444.81}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2bbdb5f1-333e-4f38-b4e8-97566a6b5fc1", "node_type": "1", "metadata": {"bbox": [{"page": 478, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 93.83, "x1": 432.0, "y1": 319.93}]}, "hash": "cb0a3e67657d45507d710b27be3bef2e27f18a5f1eb288988488e9a01531975e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d8173a7-f6f9-4224-89d8-6f9b8eced1f8", "node_type": "1", "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.21, "x1": 431.99, "y1": 586.28}]}, "hash": "f12631723b242af8b0619cbf90e014435e3e333834e7c378aeef015598fdc7e1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "benefits of RAG, 109 \nextending capabilities (see model capabili\u2010 \nties extended) \nPattern 6: Basic RAG, 108-122 <br><br>about, 108, 206 \nconsiderations, 119-121 \nexample: indexing and retrieval, 118 \ngeneration, 117 \ngrounding LLM responses, 110-112 \nindexing, 112-115 \nlimitations of RAG, 121 \nmetadata, 114 \n(see also metadata) <br><br>PDF document ingestion, 118 \npipelines, 112-117 \nproblem, 108 \nreferences, 121 \nretrieval, 112-114, 115-117 \nsolution, 109-118 \nPattern 7: Semantic Indexing, 122-140 <br><br>about, 122, 206 \nconsiderations, 137-139 \ncontextual retrieval, 132 \nembeddings, 123-127 \nexample: indexing and querying tables, \n135-137 <br><br>hierarchical chunking, 133 \nimages and videos, 128 \njargon, 131 \nproblem, 122 \nreferences, 139 \nsemantic chunking, 127 \nsolution, 123-134 \ntables, 130 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d8173a7-f6f9-4224-89d8-6f9b8eced1f8": {"__data__": {"id_": "5d8173a7-f6f9-4224-89d8-6f9b8eced1f8", "embedding": null, "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.21, "x1": 431.99, "y1": 586.28}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "65f279e3-1db4-486b-98ec-48a5931b7f14", "node_type": "1", "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 79.41, "x1": 424.79, "y1": 444.81}]}, "hash": "884599b7a3a66886b072721508e18148ec4e73533e27a4b7f70f01b1d63a4ef3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3298f657-361e-4144-a50c-2419a38baab2", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 251.22, "x1": 357.91, "y1": 605.82}]}, "hash": "158eccee38d52ef8e7e62e3b324a48ee7392d4edd94003dcafb7c741ad498815", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Index**<br><br>**Symbols**\n| (pipe) separator <br><br>response items separated by, 56 \nsynonym expansion in prompt, 131 <br><br>**A**\nA/B testing <br><br>Pattern 5: Content Optimization, 85-92 \ntraditional, 84 <br><br>A2A (Agent2Agent) protocol, 349 \nAdapter Tuning (Pattern 15), 240-256 <br><br>about, 240, 243, 273 \nconsiderations, 254-256 \nalternatives, 256 \nvariations, 254-256 \nexample: image input, text output, 249-254 <br><br>adapter tuning, 252 \ndataset preparation, 251 \ngoal of anatomical descriptions, 249 \ninference, 252-254 <br><br>not for jargon or new facts, 243, 244 \nproblem, 240 \nreferences, 256 \nsolution, 241-249 <br><br>architecture, 241-247 \ninference, 249 \nparameter-efficient fine-tuning, 245 \ntraining, 247-249 \nwhen prompt engineering not viable, 250 \nadding knowledge to foundational models <br><br>about patterns, 105, 206 \nabout RAG, 105, 119 \nabout world knowledge of foundational \nmodels, 108 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3298f657-361e-4144-a50c-2419a38baab2": {"__data__": {"id_": "3298f657-361e-4144-a50c-2419a38baab2", "embedding": null, "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 251.22, "x1": 357.91, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5d8173a7-f6f9-4224-89d8-6f9b8eced1f8", "node_type": "1", "metadata": {"bbox": [{"page": 480, "page_height": 661.5, "page_width": 504.0, "x0": 71.99, "y0": 73.21, "x1": 431.99, "y1": 586.28}]}, "hash": "f12631723b242af8b0619cbf90e014435e3e333834e7c378aeef015598fdc7e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8189b9a-1db2-48c8-aa2f-35088739c43b", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 234.71, "y1": 249.42}]}, "hash": "c6e05348d35cddbb9d30baa3b0893d3e1446671133a23cedec026fed6b6afcf5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 8: Indexing at Scale, 140-150 <br><br>about, 140, 206 \nconsiderations, 149 \nexample: metadata filtering, 147-149 \nmetadata, 143-147 \nmetadata alternatives, 149 \nmetadata limitations, 149 \nmetadata types, 143 \nproblem, 140-142 \nreferences, 150 \nsolution, 143-147 \nPattern 9: Index-Aware Retrieval, 151-163 <br><br>about, 153, 206 \nconsiderations, 162 \nexample: Semantic Indexing versus, <br><br>159-162 \nproblem, 151 \nreferences, 163 \nsolution, 153-159 \nPattern 10: Node Postprocessing, 163-172 <br><br>about, 163, 206 \nconsiderations, 172 \nexample: semantic RAG versus, 170-172 \nproblem, 164 \nreferences, 172 \nreranking, 166 \nsolution, 165-188 \nPattern 11: Trustworthy Generation, <br><br>173-193 \nabout, 173, 206 \ncitations, 175-178 \nconsiderations, 192 \ncorrective retrieval-augmented genera\u2010 <br><br>agents <br><br>about, 2 \nabout patterns, 313, 351 \nagentic AI, 12-14 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8189b9a-1db2-48c8-aa2f-35088739c43b": {"__data__": {"id_": "a8189b9a-1db2-48c8-aa2f-35088739c43b", "embedding": null, "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 234.71, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3298f657-361e-4144-a50c-2419a38baab2", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 251.22, "x1": 357.91, "y1": 605.82}]}, "hash": "158eccee38d52ef8e7e62e3b324a48ee7392d4edd94003dcafb7c741ad498815", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddeb8c32-f31b-4f21-959e-6b9d899586c4", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 422.3, "y1": 260.22}]}, "hash": "534a3c8139d3acf1d7157ea29719e5eaf915544c97a98afe4fe9698610b9724f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tion, 182-184 <br><br>example: adding citations, 188 \nhuman feedback, 181 \nobservability, 181 \nout-of-domain detection, 174 \nproblem, 173 \nreferences, 193 \nself-RAG, 185 \nsolution, 165-188 \nuser interface design, 186-188 \nPattern 12: Deep Search, 193-205 <br><br>about, 193, 195, 206 \nconsiderations, 205 \ndeep research versus Deep Search, 198 \nexample: Wikipedia, 200-205 \nproblem, 194 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddeb8c32-f31b-4f21-959e-6b9d899586c4": {"__data__": {"id_": "ddeb8c32-f31b-4f21-959e-6b9d899586c4", "embedding": null, "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 422.3, "y1": 260.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a8189b9a-1db2-48c8-aa2f-35088739c43b", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 234.71, "y1": 249.42}]}, "hash": "c6e05348d35cddbb9d30baa3b0893d3e1446671133a23cedec026fed6b6afcf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8685baea-bfdf-40f1-8c75-d1aefac285b8", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 230.57, "y1": 605.82}]}, "hash": "419616973179891b273623b3f4eaff0c0682b77c25d011c1f48dbda128ea3bda", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "aspirational goal currently, 14 <br><br>agentic applications, 2, 313 \ncharacteristics of, 13 \nautonomy, 13 \nexample: inventory-managing agent, 12 \n(see also agentic workflow demonstra\u2010 \ntion) \nPattern 21: Tool Calling, 313-329 <br><br>about, 313, 351 \nabout the name, 316 \nconsiderations, 326-329 \nexample in OpenAI: booking a flight, \n316-321 <br><br>example: weather tool, 324-326 \nfunction calling in OpenAI, 316 \ngeocoding tool called, 325 \nhow tool calling works, 314-316 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8685baea-bfdf-40f1-8c75-d1aefac285b8": {"__data__": {"id_": "8685baea-bfdf-40f1-8c75-d1aefac285b8", "embedding": null, "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 230.57, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ddeb8c32-f31b-4f21-959e-6b9d899586c4", "node_type": "1", "metadata": {"bbox": [{"page": 481, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 422.3, "y1": 260.22}]}, "hash": "534a3c8139d3acf1d7157ea29719e5eaf915544c97a98afe4fe9698610b9724f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91ba294d-4b11-4091-917f-e6fbf27ba745", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 242.37, "y1": 227.82}]}, "hash": "624e530005110e4c782cda1678f6b472660d9a004cdfde43f98081de97c30b48", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LangGraph and MCP, 321-324 \nMCP client, 325 \nMCP server, 325 \nproblem, 313 \nreferences, 329 \nsolution, 314 \nsupport for tool calling in LLMs, 319 \nPattern 22: Code Execution, 329-333 <br><br>about, 329, 351 \nconsiderations, 332 \nexample: graphing basketball tourna\u2010 <br><br>ment, 331 \nproblem, 329 \nreferences, 333 \nsandboxing the code execution, 332 \nsolution, 330 \nPattern 23: Multiagent Collaboration, <br><br>333-351 \nabout, 333, 351 \nAgent2Agent protocol, 349 \nconsiderations, 348-350 \nexample: creating content with AG2, \n341-348 <br><br>multiagent architectures, 336-340 \nproblem, 333 \nreferences, 351 \nsolution, 334-341 \nuse cases, 340 \nAI engineering, 2 \nabout foundational models, xi, 1 <br><br>agents, 2 \ncreating a foundational model, 7-10 \nfine-grained control, 15-20 \nmodel ratings versus cost, 10-12 \nbuilding on foundational models, 3-12 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91ba294d-4b11-4091-917f-e6fbf27ba745": {"__data__": {"id_": "91ba294d-4b11-4091-917f-e6fbf27ba745", "embedding": null, "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 242.37, "y1": 227.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8685baea-bfdf-40f1-8c75-d1aefac285b8", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 230.57, "y1": 605.82}]}, "hash": "419616973179891b273623b3f4eaff0c0682b77c25d011c1f48dbda128ea3bda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad31daba-7fe3-48f1-ba8b-818e3f1ba463", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 74.72, "x1": 426.58, "y1": 250.25}]}, "hash": "455e3164bb13ff3362bb1b9b9c60d660b633ea68ca14e8f7746a8009b9aefd23", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "agnostic to model, framework, hyper\u2010 \nscaler, 3, 6 <br><br>books on building GenAI applications, 3 \nLLM-agnostic framework, 6 \nmodel provider\u2019s API, 5 \nprompts and context, 3-5 \nrunning your model locally, 6, 12 <br><br>(see also Pattern 24: Small Language \nModel (SLM); Pattern 26: Infer\u2010 \nence Optimization) <br><br>AI Engineering (Huyen), 2 \n\u201cair-gapped\u201d systems, 12 \nAlexander, Christopher, 1 \nAllen-Zhu, Zeyuah, 360 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad31daba-7fe3-48f1-ba8b-818e3f1ba463": {"__data__": {"id_": "ad31daba-7fe3-48f1-ba8b-818e3f1ba463", "embedding": null, "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 74.72, "x1": 426.58, "y1": 250.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "91ba294d-4b11-4091-917f-e6fbf27ba745", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 242.37, "y1": 227.82}]}, "hash": "624e530005110e4c782cda1678f6b472660d9a004cdfde43f98081de97c30b48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63dade9e-0bdc-4ef9-a2f5-f7f607c21064", "node_type": "1", "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.3, "y1": 605.82}]}, "hash": "18bb27a60b76a0abb3331cccbe4457c4be42da8ba79e1738a73afb1cc0eda330", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**B**\nback translation, 83 \nBackus-Naur form (BNF), 51 <br><br>Pydantic approach versus, 59 \nBasic RAG (Pattern 6), 108-122 <br><br>about, 108, 206 \nabout world knowledge of foundational \nmodels, 108 <br><br>benefits of RAG, 109, 119 \nconsiderations, 119-121 \nalternatives, 119 \nlarger context window versus RAG, 120 \nlimitations, 121 <br><br>example: indexing and retrieval, 118 \nfew-shot CoT versus RAG, 216 \nPDF document ingestion, 118 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63dade9e-0bdc-4ef9-a2f5-f7f607c21064": {"__data__": {"id_": "63dade9e-0bdc-4ef9-a2f5-f7f607c21064", "embedding": null, "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.3, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ad31daba-7fe3-48f1-ba8b-818e3f1ba463", "node_type": "1", "metadata": {"bbox": [{"page": 482, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 74.72, "x1": 426.58, "y1": 250.25}]}, "hash": "455e3164bb13ff3362bb1b9b9c60d660b633ea68ca14e8f7746a8009b9aefd23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3793458-fa6f-4e13-8af6-b9a6063536a6", "node_type": "1", "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 423.37, "y1": 325.02}]}, "hash": "19fafb6d9b5cd0dc5e5abf0745ef6c47c99c64df381686ff4dc0ab2bef776040", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "solution <br><br>generation, 117 \ngrounding LLM responses, 110-112 \nindexing, 112-115 \nmetadata, 114 \n(see also metadata) <br><br>pipelines, 112-117 \nretrieval, 112-114, 115-117 <br><br>basketball tournament graphed, 331 \nbeam search, 19 <br><br>beam search width, 20 \nlength penalties, 20 \nrepetition penalties, 19 <br><br>Bench.io for personalizing system prompt, 407 \nbenchmarking tool for LLMs, 400 \nLLMPerf for benchmarking, 402 <br><br>biases of LLMs, 285 \nBitNet, 362 \nBitsAndBytesConfig Python library, 362 \nBM25 algorithm, 116 <br><br>building in LlamaIndex, 116 \nhybrid search with vector searches, 156 \nmatching keywords exactly, 121 \nTF-IDF modified by, 116 <br><br>Chhikara, P., 409 \ncitations, 175-178 <br><br>classification-based citations, 177 \nexample of adding citations, 188 \npretrained foundational models lacking, 109 <br><br>BNF (see Backus-Naur form (BNF)) \nbook supplemental material online, xiv \nbook web page, xv \nBooks2 as training data, 108 \nbranding <br><br>RAG enabling attribution, 109 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3793458-fa6f-4e13-8af6-b9a6063536a6": {"__data__": {"id_": "c3793458-fa6f-4e13-8af6-b9a6063536a6", "embedding": null, "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 423.37, "y1": 325.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "63dade9e-0bdc-4ef9-a2f5-f7f607c21064", "node_type": "1", "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.3, "y1": 605.82}]}, "hash": "18bb27a60b76a0abb3331cccbe4457c4be42da8ba79e1738a73afb1cc0eda330", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba272cfa-53b5-4ff9-bd17-8525f9732c0a", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.14, "y1": 605.82}]}, "hash": "439df7d26dc4f0ac3d1dbc8310cc181cb4a0048f0b1d61540702ff87a3f23d28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "source-level tracking, 175 \ntoken-level attribution, 177 \nclassified ads generated, 93-97 <br><br>Pattern 1: Logits Masking, 32 \nPattern 4: Reverse Neutralization, 75, 78-82 <br><br>inference, 96 \nLLM as judge, 94 \nLLM providing reason for choice, 94 \npreference tuning, 96 \ntemperature change for different ads, 93 \ntraining dataset, 95 <br><br>budget forcing, 240 <br><br>**C**\ncapabilities of models \nCode Execution (Pattern 22), 329-333 <br><br>extending (see model capabilities extended) \nlimits of LLM reasoning, 209-212 \nabout, 329, 351 \nconsiderations, 332 <br><br>known capabilities, 210 \nunknown capabilities, 211 \nChain of Thought (Pattern 13), 212-226 \nsandboxing the code execution, 332 \nexample: graphing basketball tournament, \n331 <br><br>about, 212, 273 \nconsiderations, 222 \nalternatives, 224 \nlimitations, 222-224 \nproblem, 329 \nreferences, 333 \nsolution, 330 <br><br>cold start in creating a foundational model, 9 \nComet Opik, 181 \nCommon Crawl as training data, 108 \nconstrained decoding, 62 <br><br>examples <br><br>few-shot CoT, 220 \nzero-shot CoT, 220 \nLLM reason for choice in Content Optimi\u2010 \n(see also Grammar (Pattern 2)) \nzation, 94 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba272cfa-53b5-4ff9-bd17-8525f9732c0a": {"__data__": {"id_": "ba272cfa-53b5-4ff9-bd17-8525f9732c0a", "embedding": null, "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.14, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c3793458-fa6f-4e13-8af6-b9a6063536a6", "node_type": "1", "metadata": {"bbox": [{"page": 483, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 423.37, "y1": 325.02}]}, "hash": "19fafb6d9b5cd0dc5e5abf0745ef6c47c99c64df381686ff4dc0ab2bef776040", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27478eb3-a3f1-41be-88ab-7b8d9ec9cabe", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.86, "y1": 238.62}]}, "hash": "79299d752d19515e4fca07dddc35ad725951e2ac31ae3c851320e1492be2bcd8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "considerations, 59-62 \nexample: arithmetic expressions, 54-56 \nexample: extracting invoice information, \n58 <br><br>grammar as logits masking, 50 \ngrammar as rules constraining tokens, \n49, 59 \ngrammar-constrained logits processor, \n50-52 <br><br>JSON output format example, 57 \nLangChain XML parser warning, 52 \npipe (|) separator, 56 \nproblem, 49 \nreferences, 62 \nsolution, 49-53 \nstandard data format option, 52 \nuser-specified schema option, 52 \nvarious approaches to constraint, 59-61 \nPattern 3: Style Transfer, 63-74 <br><br>about, 63, 102 \nconsiderations, 73 \nexample: image, 71-73 \nexample: text, 70 \nfew-shot learning option, 66 \nmodel fine-tuning option, 68-73 \nproblem, 63-66 \nreferences, 74 \nsolution, 66-73 \nPattern 4: Reverse Neutralization, 74-83 <br><br>about, 74, 102 \nconsiderations, 82 \nexample: generating legal text, 77 \nexample: personal style, 78-82 \nproblem, 74 \nreferences, 83 \nsolution, 75 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27478eb3-a3f1-41be-88ab-7b8d9ec9cabe": {"__data__": {"id_": "27478eb3-a3f1-41be-88ab-7b8d9ec9cabe", "embedding": null, "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.86, "y1": 238.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ba272cfa-53b5-4ff9-bd17-8525f9732c0a", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.14, "y1": 605.82}]}, "hash": "439df7d26dc4f0ac3d1dbc8310cc181cb4a0048f0b1d61540702ff87a3f23d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ae42d6a-ee99-4f04-b0df-971eb61f4b94", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.69, "y1": 217.02}]}, "hash": "949dfd23e8c416cca0811830ec58bdc0474a2fe08d0651dd6234d6c9a4055e67", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "about patterns, 31, 102 \nabout stochastic responses, 31 \nPattern 1: Logits Masking, 32-48 <br><br>about, 32, 102 \naccess to logits required, 48 \nconsiderations, 46-48 \nexample: sequence regeneration, 41-46 \nexample: sequence selection, 40-41 \nproblem, 32-34 \nreferences, 48 \nsolution, 34-39 \ntry-and-try-again antipattern, 33 \nPattern 2: Grammar, 49-63 <br><br>about, 49, 102 \nBackus-Naur form of grammar, 51 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ae42d6a-ee99-4f04-b0df-971eb61f4b94": {"__data__": {"id_": "3ae42d6a-ee99-4f04-b0df-971eb61f4b94", "embedding": null, "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.69, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "27478eb3-a3f1-41be-88ab-7b8d9ec9cabe", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.86, "y1": 238.62}]}, "hash": "79299d752d19515e4fca07dddc35ad725951e2ac31ae3c851320e1492be2bcd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aae88323-c36d-42c7-b865-1a206b7cdb31", "node_type": "1", "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 422.57, "y1": 605.82}]}, "hash": "dc45956958648150fcdfb1869006381184046de63613739630743533672749e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 5: Content Optimization, 83-102 <br><br>about, 83, 97, 102 \nconsiderations, 97-102 \ndirect preference optimization, 92 \nexample: creating images in preferred \nstyle, 99 <br><br>example: generating classified ads, 93-97 \nimproving quality, 100-102 \npreference dataset creation, 86-92 \nproblem, 83-85 \nreferences, 102 \nsolution, 85-92 \nPython dataclass, 53 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aae88323-c36d-42c7-b865-1a206b7cdb31": {"__data__": {"id_": "aae88323-c36d-42c7-b865-1a206b7cdb31", "embedding": null, "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 422.57, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3ae42d6a-ee99-4f04-b0df-971eb61f4b94", "node_type": "1", "metadata": {"bbox": [{"page": 484, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.69, "y1": 217.02}]}, "hash": "949dfd23e8c416cca0811830ec58bdc0474a2fe08d0651dd6234d6c9a4055e67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fc33c58-d086-47a8-b0e2-d920f745c2bd", "node_type": "1", "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.87, "y1": 217.02}]}, "hash": "444fa2687408ca16bc97889090d7ad462afd4fba01794a306685b184a1e0e674", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "thinking, 203 <br><br>problem, 194 \nreferences, 205 \nsolution, 195 <br><br>evaluation metrics, 199 \nfoundational models for, 198 \ninformation integration, 200 \niterative refinement, 199 <br><br>DeepEval, 180 \nDeepSeek <br><br>creating a foundational model, 7-10 \ndistilled versions for modest hardware, 10 \nmixture of experts model, 9 \nDegradation Testing (Pattern 27), 390-403 <br><br>about, 390, 415 \nexample: LLM benchmarking tool, 400 \nperformance-monitoring tools, 402 \nproblem, 390 \nreferences, 403 \nsolution, 391-400 <br><br>core metrics, 391-397 \nload testing, 398-400 \nscalability, 397 \nstress analysis, 398 \nDependency Injection (Pattern 19), 295-302 <br><br>about, 295, 311 \nconsiderations, 301 \nexample: marketing text, 296-299 \nproblem developing GenAI applications, \n295 <br><br>references, 302 \nsolution, 299-301 <br><br>depth map for Style Transfer (Pattern 3), 72 \ndesign patterns, 1 \nabout, xi, xii, 3 \nabout foundational models, xi, 1 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fc33c58-d086-47a8-b0e2-d920f745c2bd": {"__data__": {"id_": "8fc33c58-d086-47a8-b0e2-d920f745c2bd", "embedding": null, "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.87, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "aae88323-c36d-42c7-b865-1a206b7cdb31", "node_type": "1", "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 422.57, "y1": 605.82}]}, "hash": "dc45956958648150fcdfb1869006381184046de63613739630743533672749e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e8f0e54-9088-4315-8aa5-efc0551297ec", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 272.82, "x1": 244.03, "y1": 605.82}]}, "hash": "d263790396a76f2061309e0f390e40d0b250db8c6b35e5fa60226e029e64f92d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "agents, 2 \ncreating a foundational model, 7-10 \nfine-grained control, 15-20 \nmodel ratings versus cost, 10-12 <br><br>Andrew Ng\u2019s four categories, 239 \nbuilding on foundational models, 3-12 \nagnostic to model, framework, hyper\u2010 \nscaler, 3, 6 <br><br>books on building GenAI applications, 3 \nLLM-agnostic framework, 6 \nmodel provider\u2019s API, 5 \nprompts and context, 3-5 \nrunning your model locally, 6, 12 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e8f0e54-9088-4315-8aa5-efc0551297ec": {"__data__": {"id_": "5e8f0e54-9088-4315-8aa5-efc0551297ec", "embedding": null, "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 272.82, "x1": 244.03, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8fc33c58-d086-47a8-b0e2-d920f745c2bd", "node_type": "1", "metadata": {"bbox": [{"page": 485, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.87, "y1": 217.02}]}, "hash": "444fa2687408ca16bc97889090d7ad462afd4fba01794a306685b184a1e0e674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fc36dfb-8014-43a0-8fa6-348dd333052a", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.42, "x1": 426.72, "y1": 379.85}]}, "hash": "20baa5ad9708053bebbd55f07957ebc2923c81ba6a316d591b7335dd697c57b1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(see also Pattern 24: Small Language \nModel (SLM); Pattern 26: Infer\u2010 \nence Optimization) <br><br>content style (see controlling content style) \nDesign Patterns: Elements of Reusable Object- \nOriented Software (Gamma, Helm, John\u2010 \nson, and Vlissides), 1 <br><br>Devin, 351 \ndiffusers library (Hugging Face) for Style <br><br>Transfer (Pattern 3), 72 \ndepth estimation for depth map, 72 \nDiffusionDPO, 99 \nimproving quality, 100-102 \ndirect preference optimization (DPO), 92 <br><br>DiffusionDPO for images, 99 \nimproving quality, 100-102 \ntraining a model, 92 \nTransformer Reinforcement Learning \nlibrary, 92 <br><br>Direct Preference Optimization: Your Language \nModel Is Secretly a Reward Model (Raifalov \net al.), 92 \ndisambiguation \nneed for, 140 \nPattern 8: Indexing at Scale metadata, <br><br>143-147 \nexample: metadata filtering, 147-149 \nusing metadata, 143 <br><br>Pattern 10: Node Postprocessing, 169 \nquery ambiguity, 194 \nPattern 12: Deep Search, 197 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fc36dfb-8014-43a0-8fa6-348dd333052a": {"__data__": {"id_": "7fc36dfb-8014-43a0-8fa6-348dd333052a", "embedding": null, "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.42, "x1": 426.72, "y1": 379.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5e8f0e54-9088-4315-8aa5-efc0551297ec", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 272.82, "x1": 244.03, "y1": 605.82}]}, "hash": "d263790396a76f2061309e0f390e40d0b250db8c6b35e5fa60226e029e64f92d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98684ed0-f1be-470d-b6b8-d3dae59984bf", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 85.52, "x1": 418.38, "y1": 148.52}]}, "hash": "e2eb6a4eded649cdaa6ca5fa0911e60f4779eaf9dba0258c3fcfdd59ff54cca3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**E**\nEdgar as training data, 108 \neducational content creation in AG2, 341-348 <br><br>agents and workflow set up, 343 \nassigning topic to writer, 344 \ncreating initial draft, 344 \nreview panel created, 346 \nreview panel discussion, 346 \nsummarizing discussion into feedback, 347 \nwriting final content, 347 \ndistilled versions of models, 10 <br><br>EERL (end-to-end request latency), 393-395 \nemails personalized (see personal-style emails <br><br>about distillation, 354, 356, 357 \nensemble distillation, 360 \nfrontier models, 12 \niterative process, 360 \nlimitations, 369 \nPattern 24: Small Language Model, 357-360 \nexample: Python code documenter, <br><br>generated) \nembeddings <br><br>GraphRAG, 157-159 \nhybrid searches of keyword and meaning, \n156 <br><br>model lifecycle, 146 \nPattern 7: Semantic Indexing, 123-127 \n367-369 <br><br>full code online, 358 \nKL divergence, 359 \nexample: indexing and querying tables, \n135-137 \n\u201cDistilling the Knowledge in a Neural Network\u201d \n(Hinton, Vinyals, and Dean), 357 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98684ed0-f1be-470d-b6b8-d3dae59984bf": {"__data__": {"id_": "98684ed0-f1be-470d-b6b8-d3dae59984bf", "embedding": null, "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 85.52, "x1": 418.38, "y1": 148.52}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7fc36dfb-8014-43a0-8fa6-348dd333052a", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 132.42, "x1": 426.72, "y1": 379.85}]}, "hash": "20baa5ad9708053bebbd55f07957ebc2923c81ba6a316d591b7335dd697c57b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "721fbe0c-9385-429c-b33c-c6c7766426c1", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.66, "y1": 605.82}]}, "hash": "3711cce1f8dc15bc542c1cde827b80868752cf8f2457842ac13425a970eb743c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ranking of embedding models, 146 \nswitching embedding models, 147 <br><br>end-to-end request latency (EERL), 393-395 \nenterprise tasks by pretrained models, 257 \n(see also Evol-Instruct (Pattern 16)) \nepisodic memory, 406 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "721fbe0c-9385-429c-b33c-c6c7766426c1": {"__data__": {"id_": "721fbe0c-9385-429c-b33c-c6c7766426c1", "embedding": null, "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.66, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98684ed0-f1be-470d-b6b8-d3dae59984bf", "node_type": "1", "metadata": {"bbox": [{"page": 486, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 85.52, "x1": 418.38, "y1": 148.52}]}, "hash": "e2eb6a4eded649cdaa6ca5fa0911e60f4779eaf9dba0258c3fcfdd59ff54cca3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acd5824d-4f50-4ae0-b31e-546f7bddf8c2", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 89.22, "x1": 244.39, "y1": 217.02}]}, "hash": "411d86180fed49a0020be8bdc43ada7a6445e1896d5a27ab13b1e83670894213", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "errors fixed quickly in foundational models, \n212 \nEuropean Parliament proceedings as training \ndata, 108 \nevaluation by models (see Pattern 17: LLM-as- \nJudge) \nEvol-Instruct (Pattern 16), 257-273 <br><br>about, 257, 273 \nconsiderations, 272 \nexample, 265-272 \nanswers, 269 \nevaluating examples, 270 \ngoal: business strategy consultant, 265 \nquestions, 266-269 \ntraining, 270 <br><br>problem, 257 \nreferences, 273 \nsolution, 258-265 \ninference, 262 \ninstruction tuning, 260-262 \ninstruction-tuning dataset prep, 262-265 \nPeFT, 261 <br><br>example store, 218 \nexamples <br><br>agent to manage inventory, 12 \nPattern 1: Logits Masking, 39-46 \nsequence regeneration, 41-46 \nsequence selection, 40-41 \nPattern 2: Grammar, 54-59 <br><br>arithmetic expressions, 54-56 \nextracting invoice information, 58 \nPattern 3: Style Transfer <br><br>images, 71-73 \nimages documentation online, 72 \ntext, 70 \nPattern 4: Reverse Neutralization ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acd5824d-4f50-4ae0-b31e-546f7bddf8c2": {"__data__": {"id_": "acd5824d-4f50-4ae0-b31e-546f7bddf8c2", "embedding": null, "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 89.22, "x1": 244.39, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "721fbe0c-9385-429c-b33c-c6c7766426c1", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.66, "y1": 605.82}]}, "hash": "3711cce1f8dc15bc542c1cde827b80868752cf8f2457842ac13425a970eb743c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91253c81-ed39-4ff6-887d-ce286d3c574e", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.82, "y1": 281.82}]}, "hash": "694c42774245b4b67b4c61f8eae7a79b064c9fa226e56e056ce2a9657260df32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "generating legal text, 77 \npersonal style, 78-82 \nPattern 5: Content Optimization <br><br>creating images in preferred style, 99 \ngenerating classified ads, 93-97 \nPattern 6: Basic RAG indexing and retrieval, \n118 <br><br>Pattern 7: Semantic Indexing tables, 135-137 \nPattern 8: Indexing at Scale metadata filter\u2010 \ning, 147-149 \nPattern 9: Index-Aware Retrieval versus \nSemantic Indexing, 159-162 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91253c81-ed39-4ff6-887d-ce286d3c574e": {"__data__": {"id_": "91253c81-ed39-4ff6-887d-ce286d3c574e", "embedding": null, "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.82, "y1": 281.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "acd5824d-4f50-4ae0-b31e-546f7bddf8c2", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 89.22, "x1": 244.39, "y1": 217.02}]}, "hash": "411d86180fed49a0020be8bdc43ada7a6445e1896d5a27ab13b1e83670894213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7577febb-6ed3-4406-a535-ff96a6ceee1f", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 251.22, "x1": 424.57, "y1": 605.82}]}, "hash": "d985bef5bda9282a8a3c026ae07e4f993ca67a4f1bb226ecc416fb8c09107a71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "code documenter, 367-369 \nPattern 25: Prompt Caching client side, \n377-380 <br><br>Pattern 26: Inference Optimization, 388-390 \nPattern 27: Degradation Testing bench\u2010 \nmarking tool, 400 <br><br>Pattern 28: Long-Term Memory, 409-414 \nPattern 29: Template Generation thank-you \nnotes, 420-421 \nPattern 30: Assembled Reformat product \ncatalog, 424 \nPattern 31: Self-Check extracting values \nfrom receipts, 433-435 \nPattern 32: Guardrails for RAG system, \n440-442 \nextending model capabilities (see model capa\u2010 \nbilities extended) \nextracting invoice information, 58 \nPattern 31: Self-Check example, 433-435 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7577febb-6ed3-4406-a535-ff96a6ceee1f": {"__data__": {"id_": "7577febb-6ed3-4406-a535-ff96a6ceee1f", "embedding": null, "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 251.22, "x1": 424.57, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "91253c81-ed39-4ff6-887d-ce286d3c574e", "node_type": "1", "metadata": {"bbox": [{"page": 487, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.82, "y1": 281.82}]}, "hash": "694c42774245b4b67b4c61f8eae7a79b064c9fa226e56e056ce2a9657260df32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62b5ef8b-eb43-4c8e-8cd2-cd89b95fef09", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.13, "y1": 249.42}]}, "hash": "80101eb6f8a70b15c33debec47107fc7e63a0603322056a2359e4b9b16070ac6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "chain-of-thought reasoning, 10 \ncold start, 9 \nnext-token predictors, 9 \npretraining stage, 9 \npure reinforcement learning, 9 \nreinforcement learning stage, 9 \nrejection sampling, 9 \nsupervised fine-tuning, 9 \ntrained on tokens, 8 \ndistilled versions for modest hardware, 10 <br><br>frontier models, 12 \nerrors fixed quickly, 212 \nfine-grained control, 15-20 <br><br>about fine-grained control, 15 \nbeam search, 19 \nlogits, 15-16 \nnucleus sampling, 18 \ntemperature, 16-18 \ntop-K sampling, 18 \nGenAI applications, xi, 1 \nin-context learning, 20-22 <br><br>about, 20 \nAdapter Tuning as alternative, 74 \nfew-shot learning, 21, 63, 66 \nfew-shot learning considerations, 73 \nzero-shot learning, 20 \nlimits of LLM reasoning, 209-212 \nextending capabilities (see model capa\u2010 <br><br>bilities extended) \nknown capabilities, 210 \nunknown capabilities, 211 \nmodel ratings versus cost, 10-12 \nfrontier models, 11 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62b5ef8b-eb43-4c8e-8cd2-cd89b95fef09": {"__data__": {"id_": "62b5ef8b-eb43-4c8e-8cd2-cd89b95fef09", "embedding": null, "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.13, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7577febb-6ed3-4406-a535-ff96a6ceee1f", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 251.22, "x1": 424.57, "y1": 605.82}]}, "hash": "d985bef5bda9282a8a3c026ae07e4f993ca67a4f1bb226ecc416fb8c09107a71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0d8967e-4a10-499e-82cb-5437a44be844", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 75.92, "x1": 242.37, "y1": 257.72}]}, "hash": "05e75eeac2fdda0c5633ff71c5e17d88b627b4a8718cba65bfeb312fd5b2cd0d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "post-training, 22-27 <br><br>dataset structure, 24 \ndetermining if post-training possible, 24 \nmethods, 22 \nPattern 15: Adapter Tuning, 241-249 \nPattern 16: Evol-Instruct, 258-265 \nquantization, 362 \npretrained, xi, 1, 7 <br><br>citations for sources lacking, 109 \nDeepSeek base LLM, 8 \nenterprise tasks not in training, 257 \n(see also Pattern 16: Evol-Instruct) <br><br>next-token predictors, 9 \ntrained on tokens, 8 \nworld knowledge training data, 108 \nstochastic responses, 31 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0d8967e-4a10-499e-82cb-5437a44be844": {"__data__": {"id_": "d0d8967e-4a10-499e-82cb-5437a44be844", "embedding": null, "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 75.92, "x1": 242.37, "y1": 257.72}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "62b5ef8b-eb43-4c8e-8cd2-cd89b95fef09", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 429.13, "y1": 249.42}]}, "hash": "80101eb6f8a70b15c33debec47107fc7e63a0603322056a2359e4b9b16070ac6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef5679dc-0a2c-412d-bb8e-e6fb99666db2", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 272.82, "x1": 431.84, "y1": 605.82}]}, "hash": "e54eb001dcc88256bf5dbf5f8fa7d42e954dd7a5bb531434d868d55865db1b4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "adding knowledge, 105 \n(see also adding knowledge to founda\u2010 \ntional models) \nagents, 2 \n(see also agents) \nbuilding on, 3-12 \nagnostic to model, framework, hyper\u2010 \nscaler, 3, 6 <br><br>books on building GenAI applications, 3 \nLLM-agnostic framework, 6 \nmodel provider\u2019s API, 5 \nprompts and context, 3-5 \nrunning your model locally, 6, 12 <br><br>(see also Pattern 24: Small Language \nModel (SLM); Pattern 26: Infer\u2010 \nence Optimization) \ncreating a foundational model, 7-10 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef5679dc-0a2c-412d-bb8e-e6fb99666db2": {"__data__": {"id_": "ef5679dc-0a2c-412d-bb8e-e6fb99666db2", "embedding": null, "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 272.82, "x1": 431.84, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d0d8967e-4a10-499e-82cb-5437a44be844", "node_type": "1", "metadata": {"bbox": [{"page": 488, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 75.92, "x1": 242.37, "y1": 257.72}]}, "hash": "05e75eeac2fdda0c5633ff71c5e17d88b627b4a8718cba65bfeb312fd5b2cd0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af4fbd4c-ec03-4b75-8b65-5a4fe8a9aad8", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 128.72, "x1": 242.37, "y1": 466.25}]}, "hash": "b53ecf43906a13baf5bec72010cf3db693e663bc93abb8fd967a0f7ae1ccc8c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 4: Reverse Neutralization <br><br>generating legal text, 77 \ngenerating personalized email, 80 \nPattern 6: Basic RAG <br><br>Document object from text, 114 \nindexing and retrieval, 118 \ntax return into prompt for queries, 120 \nPattern 9: Index-Aware Retrieval example \nSemantic Indexing comparison, 159 \nPattern 11: Trustworthy Generation <br><br>citations added, 192 \ncitations via source-level tracking, 176 <br><br>Pattern 12: Deep Search example, 200 \nnotebook on thinking step, 203 \nPattern 14: Tree of Thoughts <br><br>random sentences example, 227 \nsupply chain optimization, 236 \nPattern 16: Evol-Instruct training, 270 \nPattern 19: Dependency Injection marketing \ntext, 296 \nPattern 20: Prompt Optimization marketing \ntext, 305, 308 \nPattern 22: Code Execution graphing bas\u2010 \nketball tournament, 331 \nPattern 23: Multiagent Collaboration exam\u2010 \nple in AG2, 341 \nPattern 24: Small Language Model <br><br>distillation, 358 \nexample training dataset script, 368 \nexample: Python code documenter, 367, \n369 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af4fbd4c-ec03-4b75-8b65-5a4fe8a9aad8": {"__data__": {"id_": "af4fbd4c-ec03-4b75-8b65-5a4fe8a9aad8", "embedding": null, "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 128.72, "x1": 242.37, "y1": 466.25}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ef5679dc-0a2c-412d-bb8e-e6fb99666db2", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 272.82, "x1": 431.84, "y1": 605.82}]}, "hash": "e54eb001dcc88256bf5dbf5f8fa7d42e954dd7a5bb531434d868d55865db1b4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c309842-50db-430d-86ea-38209cf35792", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 431.74, "y1": 271.02}]}, "hash": "af35267863d4b703041ff883e2699d20a0220d176da63a5858eed8946b817342", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**G**\nGamma, Erich, 1 \nGenAI applications \nabout foundational models, xi, 1 <br><br>agents, 2 \ncreating a foundational model, 7-10 \nfine-grained control, 15-20 \nmodel ratings versus cost, 10-12 \nbuilding on foundational models, 3-12 \nagnostic to model, framework, hyper\u2010 \nscaler, 3, 6 <br><br>books on building GenAI applications, 3 \nLLM-agnostic framework, 6 \nmodel provider\u2019s API, 5 \nprompts and context, 3-5 \nrunning your model locally, 6, 12 <br><br>(see also Pattern 24: Small Language \nModel (SLM); Pattern 26: Infer\u2010 \nence Optimization) <br><br>Generative AI on AWS (Fregly et al.), 3 \nGenerative AI on Google Cloud with Lang\u2010 \nChain (Kuligin et al.), 3 \ngeocoding tool called by weather tool, 325 <br><br>MCP client, 325 \nMCP server, 325 \nGitHub, 108 <br><br>Anthropic API invoking Claude Sonnet 3, 5 \nbridge card game API call results, 211 \ninventory-managing agent code, 12 \nMCP server, 329 \nPattern 1: Logits Masking, 37 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c309842-50db-430d-86ea-38209cf35792": {"__data__": {"id_": "6c309842-50db-430d-86ea-38209cf35792", "embedding": null, "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 431.74, "y1": 271.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "af4fbd4c-ec03-4b75-8b65-5a4fe8a9aad8", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 128.72, "x1": 242.37, "y1": 466.25}]}, "hash": "b53ecf43906a13baf5bec72010cf3db693e663bc93abb8fd967a0f7ae1ccc8c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b4d770f-b378-4ce2-b2ee-b3ea92812a92", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 251.22, "x1": 431.05, "y1": 605.82}]}, "hash": "abc9155d745a2bbd951b3d81960ff593ae3a2bf8c0f8dfd802792854d3668790", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "speculative decoding, 366 \nPattern 25: Prompt Caching client-side \nexample, 380 \nPattern 26: Inference Optimization example, \n388 \nPattern 27: Degradation Testing bench\u2010 \nmarking tool, 400 \nPattern 29: Template Generation thank-you \nnotes, 420 \nPattern 30: Assembled Reformat product \ncatalog, 424 <br><br>Pattern 31: Self-Check \nexample, 434 \nsolution, 428 <br><br>Pattern 32: Guardrails example, 440 \nprime numbers, 210 \nGoogle frontier models, 11 \nfine-tuning, 24 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b4d770f-b378-4ce2-b2ee-b3ea92812a92": {"__data__": {"id_": "5b4d770f-b378-4ce2-b2ee-b3ea92812a92", "embedding": null, "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 251.22, "x1": 431.05, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6c309842-50db-430d-86ea-38209cf35792", "node_type": "1", "metadata": {"bbox": [{"page": 489, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 431.74, "y1": 271.02}]}, "hash": "af35267863d4b703041ff883e2699d20a0220d176da63a5858eed8946b817342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57677e66-7d2b-4032-8a3d-8be75bb8e301", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 210.27, "y1": 249.42}]}, "hash": "e8c84037d72b028761056734ecffe942ef182c815f886eeecc236521b3657a1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 31: Self-Check solution, 428-432 \nRAG reducing, 109 \nrisk of, 173, 426-428 \ncorrective retrieval-augmented genera\u2010 \ntion reducing, 182-184 \nRAG system in unfamiliar domain, 162 \nHands-On Generative AI with Transformers \nand Diffusion Models (Sanseviero et al.), 3 <br><br>Helm, Richard, 1 \nhierarchical chunking, 133 <br><br>Recursive Abstractive Processing for Tree- \nOrganized Retrieval (RAPTOR), 133 <br><br>hierarchical multiagent architectures, 336 \nHinton, Geoffrey, 357 \nhosted API services, 12 \nHugging Face <br><br>diffusers library, 72 \nimage style transfer documentation, 72 \ntrained model distribution, 26 \nTransformer Reinforcement Learning, 247 \nTransformers library, 37 <br><br>length penalties supported, 20 \nloading DPO-trained model, 92 \nhuman feedback, 181 \nPattern 5: Content Optimization for respon\u2010 \nses, 205 \nhuman-in-the-loop multiagent architecture, \n339 <br><br>Huyen, Chip, 2 \nhybrid searches <br><br>Pattern 9: Index-Aware Retrieval, 156 \nPattern 10: Node Postprocessing, 167 \nHyDE (hypothetical document embedding), ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57677e66-7d2b-4032-8a3d-8be75bb8e301": {"__data__": {"id_": "57677e66-7d2b-4032-8a3d-8be75bb8e301", "embedding": null, "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 210.27, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5b4d770f-b378-4ce2-b2ee-b3ea92812a92", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 251.22, "x1": 431.05, "y1": 605.82}]}, "hash": "abc9155d745a2bbd951b3d81960ff593ae3a2bf8c0f8dfd802792854d3668790", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e5ef6cc-51e6-4f59-b6c9-a64fd1ebcc3f", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 426.06, "y1": 249.42}]}, "hash": "8f2871a0acc886de3a550645926042ad24fe76b98c91d99c9340ced317e9a61f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Guardrails (Pattern 32), 436-443 <br><br>about, 443 \nabout guardrails, 436 \nconsiderations, 442 \nexample, 440-442 \nproblem, 436 \nreferences, 443 \nsolution, 438-440 <br><br>applying a set of guardrails, 440 \ncustom guardrails, 439 \nprebuilt guardrails, 438 \nGuardrails AI Python package, 180 <br><br>**H**\nhallucinations, 109, 426 \n\u201cI don\u2019t know\u201d, 436 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e5ef6cc-51e6-4f59-b6c9-a64fd1ebcc3f": {"__data__": {"id_": "2e5ef6cc-51e6-4f59-b6c9-a64fd1ebcc3f", "embedding": null, "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 426.06, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "57677e66-7d2b-4032-8a3d-8be75bb8e301", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 210.27, "y1": 249.42}]}, "hash": "e8c84037d72b028761056734ecffe942ef182c815f886eeecc236521b3657a1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97e40446-216f-464a-8358-8156fed480f5", "node_type": "1", "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 236.08, "y1": 422.22}]}, "hash": "1fb7c03a2c8e7859030f4c36d311506419c19a6e5060533aae7d18eb7973f91b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "155 \nPattern 9: Index-Aware Retrieval example, \n161 <br><br>hyperparameter temperature, 16-18 \nhyperscaler fully managed API endpoints, 12 \nhypothetical document embedding (see HyDE \n(hypothetical document embedding)) <br><br>**I**\nIDF (inverse document frequency), 116 \nin-context learning, 20-22 <br><br>about, 20 \nAdapter Tuning as alternative, 74 \nfew-shot learning, 21, 63, 66 <br><br>considerations, 73 \nzero-shot learning, 20 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97e40446-216f-464a-8358-8156fed480f5": {"__data__": {"id_": "97e40446-216f-464a-8358-8156fed480f5", "embedding": null, "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 236.08, "y1": 422.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2e5ef6cc-51e6-4f59-b6c9-a64fd1ebcc3f", "node_type": "1", "metadata": {"bbox": [{"page": 490, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 426.06, "y1": 249.42}]}, "hash": "8f2871a0acc886de3a550645926042ad24fe76b98c91d99c9340ced317e9a61f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d00e4faa-b245-4e01-9ae7-0f4b2be949cc", "node_type": "1", "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 78.12, "x1": 426.9, "y1": 433.02}]}, "hash": "1530ca1c45c2a8395c58c626f8e92adae006a4a16b39f4b6be8b5de04f637605", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "references, 163 \nsolution, 153-159 <br><br>GraphRAG, 157-159 \nhybrid searches of keyword and mean\u2010 \ning, 156 <br><br>hypothetical answers, 155 \nquery expansion, 156 \nindexing, 112-115 <br><br>keyword indexing and lookup, 122-123 \nPattern 6: Basic RAG example, 118 \nPattern 7: Semantic Indexing example, \n135-137 \nPattern 8: Indexing at Scale example, <br><br>147-149 \ntables, 130 \nIndexing at Scale (Pattern 8), 140-150 <br><br>about, 140, 206 \nconsiderations, 149 <br><br>metadata alternatives, 149 \nmetadata limitations, 149 <br><br>example: metadata filtering, 147-149 \nproblem, 140-142 <br><br>contradictory information, 141 \ndata freshness, 140 \ndisambiguation, 140 \nmodel lifecycle, 142 <br><br>references, 150 \nsolution, 143-147 <br><br>detecting contradictory content, 144 \ndetecting outdated content, 145 \nmanaging model lifecycle, 146 \nmetadata, 143-147 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d00e4faa-b245-4e01-9ae7-0f4b2be949cc": {"__data__": {"id_": "d00e4faa-b245-4e01-9ae7-0f4b2be949cc", "embedding": null, "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 78.12, "x1": 426.9, "y1": 433.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "97e40446-216f-464a-8358-8156fed480f5", "node_type": "1", "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 236.08, "y1": 422.22}]}, "hash": "1fb7c03a2c8e7859030f4c36d311506419c19a6e5060533aae7d18eb7973f91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3714b9c2-be2d-4b20-97e4-9dd58c54be8a", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 348.42, "x1": 431.69, "y1": 605.82}]}, "hash": "b7440ffc4f50601dac91fe49fe43e5bad2473fae342c6d8e84a3e9aa0bcd8da9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "internet disconnect <br><br>\u201cair-gapped\u201d systems, 12 \nlocally hostable models, 12 <br><br>invalid action masking, 48 \ninverse document frequency (IDF), 116 \ninvoice information extracted, 58 \nPattern 31: Self-Check example, 433-435 <br><br>**J**\njargon <br><br>continued pretraining, 23, 244 \nPattern 7: Semantic Indexing, 131 <br><br>Johnson, Ralph, 1 \nJSON-formatted response, 52 \nPattern 2: Grammar, 57 \njust-in-time inventory agent, 12 <br><br>**K**\nkeyword indexing and lookup, 122-123 \nKL (Kullback\u2013Leibler) divergence, 359 \nknowledge added to foundational models (see \nadding knowledge to foundational models) <br><br>knowledge graph via GraphRAG, 157-159 \nKoenig, Andrew, 302 \nKuligin, Leonid, 3 \nKullback\u2013Leibler (KL) divergence, 359 <br><br>**L**\nLakshmanan, Valliappa, xii \nLampson, Butler, 302 \nLangChain \nclient-side caching, 373 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3714b9c2-be2d-4b20-97e4-9dd58c54be8a": {"__data__": {"id_": "3714b9c2-be2d-4b20-97e4-9dd58c54be8a", "embedding": null, "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 348.42, "x1": 431.69, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d00e4faa-b245-4e01-9ae7-0f4b2be949cc", "node_type": "1", "metadata": {"bbox": [{"page": 491, "page_height": 661.5, "page_width": 504.0, "x0": 259.49, "y0": 78.12, "x1": 426.9, "y1": 433.02}]}, "hash": "1530ca1c45c2a8395c58c626f8e92adae006a4a16b39f4b6be8b5de04f637605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4bfa2ac-76b3-43fb-86d5-636405609c51", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 229.43, "y1": 422.22}]}, "hash": "bd756e048eb69acf7a4e47d82612acdc7b895eb90e5a0d76dfa93f4585cbf7e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "solution, 277 <br><br>fine-tuning approach, 281 \nML approach, 280 \nprompting approach, 279 <br><br>LLMPerf, 402 \nLLMs into production (see production systems) \nLMArena leaderboard of model ratings versus <br><br>cost, 10-12 \nload testing, 390 \nmodel fine-tuning versus few-shot learning,  Pattern 27: Degradation Testing, 398-400 \n68, 73  locally hostable models, 6, 12 <br><br>model sharing reducing, 370 \nPattern 10: Node Postprocessing, 172 \nprompt caching reducing, 371 \nsequence generation communication, 48 \nsmall language model lower latency, 356 \nspeculative decoding for small language <br><br>(see also Pattern 24: Small Language Model \n(SLM); Pattern 26: Inference Optimiza\u2010 \ntion) \nlogits, 15-16 <br><br>beam search, 19 \nlogits masking (see Logits Masking (Pattern \n1)) <br><br>nucleus sampling, 18 \nPattern 31: Self-Check solution, 428-432 \nsoftmax function for probabilities, 15 <br><br>temperature, 16 \ntop-K sampling, 18 \nzeroing out, 38 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4bfa2ac-76b3-43fb-86d5-636405609c51": {"__data__": {"id_": "a4bfa2ac-76b3-43fb-86d5-636405609c51", "embedding": null, "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 229.43, "y1": 422.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3714b9c2-be2d-4b20-97e4-9dd58c54be8a", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 348.42, "x1": 431.69, "y1": 605.82}]}, "hash": "b7440ffc4f50601dac91fe49fe43e5bad2473fae342c6d8e84a3e9aa0bcd8da9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "432c5169-c87c-4f74-944f-fc6c960ab55e", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 422.94, "y1": 346.62}]}, "hash": "3374b8c7437e7c9c55a877b7f3cc0f6876bb753c9be52255ce54c86c7afa07ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model, 364 \ntail latency, 34 \ntry-and-try-again antipattern, 34 \nleast-to-most (LtM) prompting, 239 \nlegal text generated, 77 \nLewis, Patrick, 105 \nlexicon, 131 \nLi, Yuanzhi , 360 \nLlamaIndex <br><br>BM25 and vector hybrid search, 157 \nBM25 retriever code, 116 \nconverting text to Document object, 114 <br><br>nodes, 115 \nLlamaParse, 118 \nLLM errors fixed quickly, 212 \nLLM reasoning limits, 209-212 \nknown capabilities, 210 \nunknown capabilities, 211 \nLLM running on your own hardware, 6, 12 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "432c5169-c87c-4f74-944f-fc6c960ab55e": {"__data__": {"id_": "432c5169-c87c-4f74-944f-fc6c960ab55e", "embedding": null, "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 422.94, "y1": 346.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a4bfa2ac-76b3-43fb-86d5-636405609c51", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 229.43, "y1": 422.22}]}, "hash": "bd756e048eb69acf7a4e47d82612acdc7b895eb90e5a0d76dfa93f4585cbf7e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98a27ef8-7451-47f0-bee8-b77c334f4816", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.48, "y1": 605.82}]}, "hash": "d09e52b06f248852607e5b86ab3f62f89d19269c330f5c2e16bf128aba7672bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Logits Masking (Pattern 1), 32-48 <br><br>about, 32, 102 \nconsiderations, 46-48 \naccess to logits, 48 \nalternatives, 46 \ncaveats, 48 \nlatency, 48 \nexamples, 39-46 <br><br>sequence regeneration, 41-46 \nsequence selection, 40-41 <br><br>problem, 32-34 \nreferences, 48 \nsolution, 34-39 <br><br>(see also Pattern 24: Small Language Model \n(SLM); Pattern 26: Inference Optimiza\u2010 \ntion)  Transformers library, 37 \ntry-and-try-again antipattern, 33 \nLLM-as-Judge (Pattern 17), 275-286 <br><br>logo design in Pattern 18: Reflection, 289-293 \nlogprobs (see logits) \nLong-Term Memory (Pattern 28), 403-415  about, 275, 311 \nconsiderations, 284 <br><br>bias, 285 \ncaveats, 285 \ninconsistency, 284 \nleniency, 284 \nabout, 403, 415 \nconsiderations, 414 \nexample, 409-414 <br><br>adding memories, 410 \nMem0, 409 \nsearching memories, 412-414 \nlong-term memory explained, 404-409 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98a27ef8-7451-47f0-bee8-b77c334f4816": {"__data__": {"id_": "98a27ef8-7451-47f0-bee8-b77c334f4816", "embedding": null, "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.48, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "432c5169-c87c-4f74-944f-fc6c960ab55e", "node_type": "1", "metadata": {"bbox": [{"page": 492, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 422.94, "y1": 346.62}]}, "hash": "3374b8c7437e7c9c55a877b7f3cc0f6876bb753c9be52255ce54c86c7afa07ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a2cd343-24c8-43d0-a33c-e2df37a962d3", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 414.55, "y1": 292.62}]}, "hash": "33059b2a21b89782fb86e069011b08f072bba981de1a949ed5f0bc6511905e11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "using metadata, 143 \ntypes of metadata, 143 \nmetrics \ncore metrics, 391-397 <br><br>end-to-end request latency, 393-395 \nrequests per second, 396 \ntime to first token, 391-393 \ntokens per second, 395 \nload testing, 390 \nPattern 27: Degradation Testing, 398-400 <br><br>scalability, 397 \nstress analysis, 398 <br><br>mixed-precision training, 361 \nmixture of experts (MoE) models, 9 \nmodel capabilities extended \nabout patterns, 273 \nadding knowledge (see adding knowledge to \nfoundational models) \nlimits of LLM reasoning, 209-212 <br><br>known capabilities, 210 \nunknown capabilities, 211 <br><br>Pattern 19: Dependency Injection, 296-299 \nPattern 20: Prompt Optimization, 305-310 \nMassive Text Embedding Benchmark (MTEB), \nPattern 13: Chain of Thought, 212-226 <br><br>about, 212, 273 \nauto-CoT, 218-220 \nconsiderations, 222 \nexample: few-shot CoT, 220 \nexample: zero-shot CoT, 220 \nfew-shot CoT, 215-218 \nLLM reason for choice in Content Opti\u2010 <br><br>146 \nMCP (Model Context Protocol), 321-324 <br><br>limitations, 327 \nMCP client, 323 \nPattern 21: Tool Calling, 325 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a2cd343-24c8-43d0-a33c-e2df37a962d3": {"__data__": {"id_": "7a2cd343-24c8-43d0-a33c-e2df37a962d3", "embedding": null, "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 414.55, "y1": 292.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "98a27ef8-7451-47f0-bee8-b77c334f4816", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 294.42, "x1": 431.48, "y1": 605.82}]}, "hash": "d09e52b06f248852607e5b86ab3f62f89d19269c330f5c2e16bf128aba7672bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73ddac6e-58f0-44ae-84d3-99ae3ceb1eba", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.38, "y1": 310.52}]}, "hash": "4f8bed6a6aa92714bd1d08e7162bfcce6e6889d286534a9258a2168d2799e333", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mization, 94 \nproblem, 212-214 \nreferences, 225 \nsolution, 215-220 \nTree of Thoughts versus CoT, 226 \nzero-shot CoT, 215 \nPattern 14: Tree of Thoughts, 226-240 <br><br>about, 226, 228, 273 \nalternatives, 238-240 \nchain-of-thought versus ToT, 226 \nconsiderations, 238-240 \nexample: supply chain optimization, <br><br>233-238 \nproblem, 227 \nreferences, 240 \nsolution, 228-233 \nPattern 15: Adapter Tuning, 240-256 <br><br>about, 240, 243, 273 \narchitecture, 241-247 \nconsiderations, 254-256 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73ddac6e-58f0-44ae-84d3-99ae3ceb1eba": {"__data__": {"id_": "73ddac6e-58f0-44ae-84d3-99ae3ceb1eba", "embedding": null, "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.38, "y1": 310.52}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7a2cd343-24c8-43d0-a33c-e2df37a962d3", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 78.42, "x1": 414.55, "y1": 292.62}]}, "hash": "33059b2a21b89782fb86e069011b08f072bba981de1a949ed5f0bc6511905e11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4f76778-c4af-40da-9ac2-4081f66156ee", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 297.82, "x1": 417.18, "y1": 605.82}]}, "hash": "ce533b231e634227017d17e33271cbdd52202c28dcec2a46cbb8b4691f0c38bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MCP server, 322 \nGitHub, 329 \nPattern 21: Tool Calling, 325 <br><br>medical jargon (see jargon) \nMem0, 409 \nMem0: Building Production-Ready AI Agents \nwith Scalable Long-Term Memory (Chhi\u2010 \nkara et al.), 409 \nmemoization, 373-376 \nmemories example, 409-414 \nadding memories, 410 \nlong-term memory explained, 404-409 \nMem0, 409 \nsearching memories, 412-414 \nmetadata <br><br>about, 143 \nalternatives, 149 \nlimitations, 149 \nPattern 6: Basic RAG indexing, 114 \nPattern 7: Semantic Indexing tables, 131 \nPattern 8: Indexing at Scale, 143-147 \nexample: metadata filtering, 147-149 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4f76778-c4af-40da-9ac2-4081f66156ee": {"__data__": {"id_": "e4f76778-c4af-40da-9ac2-4081f66156ee", "embedding": null, "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 297.82, "x1": 417.18, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "73ddac6e-58f0-44ae-84d3-99ae3ceb1eba", "node_type": "1", "metadata": {"bbox": [{"page": 493, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.38, "y1": 310.52}]}, "hash": "4f8bed6a6aa92714bd1d08e7162bfcce6e6889d286534a9258a2168d2799e333", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8292afc1-2ddc-4e38-9efd-eafa2f412f43", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 241.89, "y1": 303.42}]}, "hash": "ca7fd336beb0362a1ca4ca254675443ee825f34236a5f7b32f39a651ccfbc49e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "example: image input, text output, \n249-254 <br><br>not for jargon or new facts, 243, 244 \nproblem, 240 \nreferences, 256 \nsolution, 241-249 \ntraining, 247-249 \nwhen prompt engineering not viable, \n250 \nPattern 16: Evol-Instruct, 257-273 <br><br>about, 257, 273 \nconsiderations, 272 \nexample, 265-272 \ninference, 262 \ninstruction tuning, 260-262 \ninstruction-tuning dataset prep, 262-265 \nPeFT, 261 \nproblem, 257 \nreferences, 273 \nsolution, 258-265 \nModel Context Protocol (see MCP (Model <br><br>**O**\nobservability \nContext Protocol)) <br><br>model fine-tuning (see fine-tuning models) \nmodel lifecycle problem, 142 <br><br>Arize Phoenix, 181, 402 \nML libraries of tools, 180, 181 \nPattern 11: Trustworthy Generation, 181 \nmetadata for managing, 146 <br><br>model pruning, 356 \nmodel ratings versus cost, 10-12 \nobsolete information \ndetecting, 145 \nfrontier models, 11 \nimportance of detecting, 140 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8292afc1-2ddc-4e38-9efd-eafa2f412f43": {"__data__": {"id_": "8292afc1-2ddc-4e38-9efd-eafa2f412f43", "embedding": null, "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 241.89, "y1": 303.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e4f76778-c4af-40da-9ac2-4081f66156ee", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 297.82, "x1": 417.18, "y1": 605.82}]}, "hash": "ce533b231e634227017d17e33271cbdd52202c28dcec2a46cbb8b4691f0c38bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03bcc048-789d-4740-89ca-6bf50df0dee1", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 71.02, "x1": 427.17, "y1": 296.02}]}, "hash": "aa27a7394483f0b2137faadf0d2e96c6e686344ed58aeb5f38641d772a6f9087", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model sharing, 370 \nMOE (mixture of experts) models, 9 \nMultiagent Collaboration (Pattern 23), 333-351 <br><br>about, 333, 351 \nAgent2Agent protocol, 349 \nconsiderations, 348-350 \nexample: creating content with AG2, <br><br>341-348 \nagents and workflow set up, 343 \nassigning topic to writer, 344 \ncreating initial draft, 344 \nreview panel created, 346 \nreview panel discussion, 346 \nsummarizing discussion into feedback, \n347 <br><br>supplying feedback to writer agent, 347 \nwriting final content, 347 <br><br>problem, 333 \nreferences, 351 \nsolution, 334-341 \nmultiagent architectures, 336-340 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03bcc048-789d-4740-89ca-6bf50df0dee1": {"__data__": {"id_": "03bcc048-789d-4740-89ca-6bf50df0dee1", "embedding": null, "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 71.02, "x1": 427.17, "y1": 296.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8292afc1-2ddc-4e38-9efd-eafa2f412f43", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 241.89, "y1": 303.42}]}, "hash": "ca7fd336beb0362a1ca4ca254675443ee825f34236a5f7b32f39a651ccfbc49e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e8ba278-5de2-480e-9064-7ce9f28f7ce9", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 423.56, "y1": 605.82}]}, "hash": "835efe50f806f08c3f0d9a453a66605bdebe5a6ad82b5215f25d1cd8d714b483", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "filtering out <br><br>Pattern 8: Indexing at Scale, 145 \nPattern 10: Node Postprocessing, 168 <br><br>OCR (optical character recognition), 128 \nOllama client, 6 \non-premises locally hostable models, 6, 12 <br><br>(see also Pattern 24: Small Language Model \n(SLM); Pattern 26: Inference Optimiza\u2010 \ntion) <br><br>online resources (see resources online) \nopen-weight models, 12 \nfine-tuning, 25 \nOllama client, 6 <br><br>OpenAI \nAPI <br><br>invoking fine-tuned model, 70 \npersonalized email dataset, 79 \nfine-tuning GPT, 24, 70 <br><br>invoking fine-tuned model, 70 \npersonalized email dataset, 80 \nfrontier models, 11 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e8ba278-5de2-480e-9064-7ce9f28f7ce9": {"__data__": {"id_": "9e8ba278-5de2-480e-9064-7ce9f28f7ce9", "embedding": null, "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 423.56, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "03bcc048-789d-4740-89ca-6bf50df0dee1", "node_type": "1", "metadata": {"bbox": [{"page": 494, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 71.02, "x1": 427.17, "y1": 296.02}]}, "hash": "aa27a7394483f0b2137faadf0d2e96c6e686344ed58aeb5f38641d772a6f9087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2906fa52-7759-4973-9504-97fa5e666089", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.32, "x1": 238.47, "y1": 541.85}]}, "hash": "be7e918220aabe39a7d302781c5945feb82cc6b9990f67a29effd1e235e9764a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "grammar-constrained logits processor, \n50-52 <br><br>Python dataclass, 53 \nstandard data format, 52 \nuser-specified schema, 52 \nPattern 3: Style Transfer, 63-74 <br><br>about, 63, 102 \nconsiderations, 73 \ncontext engineering, 74 \nexamples <br><br>images, 71-73 \nimages documentation online, 72 \ntext, 70 \nproblem, 63-66 \nreferences, 74 \nsolution, 66-73 <br><br>few-shot learning, 66 \nmodel fine-tuning, 68-73 \nPattern 4: Reverse Neutralization, 74-83 <br><br>about, 74, 102 \nconsiderations, 82 <br><br>choosing the neutral form, 82 \ndataset considerations, 83 \nexamples <br><br>generating legal text, 77 \npersonal style, 78-82 <br><br>problem, 74 \nreferences, 83 \nsolution, 75 \nPattern 5: Content Optimization, 83-102 <br><br>about, 83, 97, 102 \nconsiderations, 97-102 <br><br>choosing between variants, 97 \nimproving quality, 100-102 \nin-distribution requirement, 98 \nexamples ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2906fa52-7759-4973-9504-97fa5e666089": {"__data__": {"id_": "2906fa52-7759-4973-9504-97fa5e666089", "embedding": null, "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.32, "x1": 238.47, "y1": 541.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9e8ba278-5de2-480e-9064-7ce9f28f7ce9", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 423.56, "y1": 605.82}]}, "hash": "835efe50f806f08c3f0d9a453a66605bdebe5a6ad82b5215f25d1cd8d714b483", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcd7947e-0db0-441a-ba90-3149669ac621", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 85.52, "x1": 234.9, "y1": 202.52}]}, "hash": "e74f41461d411912c37e7c68512e89517256c498700da85025a507a0641e0128", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**P**\nPandaLM, 285 \nparallelization, 370 \nparameter-efficient fine-tuning (PeFT), 23 <br><br>Pattern 15: Adapter Tuning, 245 \nPattern 16: Evol-Instruct, 261 <br><br>parameters for open-weight models, 12 \nPatronusAI, 285 \nPattern 1: Logits Masking, 32-48 <br><br>about, 32, 102 \nconsiderations, 46-48 \naccess to logits, 48 \nalternatives, 46 \ncaveats, 48 \nlatency, 48 \nexamples, 39-46 <br><br>sequence regeneration, 41-46 \nsequence selection, 40-41 <br><br>problem, 32-34 \nreferences, 48 \nsolution, 34-39 <br><br>Transformers library, 37 \ntry-and-try-again antipattern, 33 \nPattern 2: Grammar, 49-63 <br><br>about, 49, 102 \nconsiderations, 59-62 \nalternatives, 61 \n(see also Pattern 1: Logits Masking) <br><br>caveats, 61 \nvarious approaches to constraint, 59-61 \nexamples, 54-59 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcd7947e-0db0-441a-ba90-3149669ac621": {"__data__": {"id_": "fcd7947e-0db0-441a-ba90-3149669ac621", "embedding": null, "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 85.52, "x1": 234.9, "y1": 202.52}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2906fa52-7759-4973-9504-97fa5e666089", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 204.32, "x1": 238.47, "y1": 541.85}]}, "hash": "be7e918220aabe39a7d302781c5945feb82cc6b9990f67a29effd1e235e9764a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "913c4ee2-feb8-49ea-aca5-c0da107caf08", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 416.25, "y1": 217.02}]}, "hash": "53254df6b03e6433bf6b64fd85af2661d3567541ba4b243356b5d5e0ec810fd8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arithmetic expressions, 54-56 \nextracting invoice information, 58 \nJSON output format, 57 \ngrammar as logits masking, 50 \ngrammar as rules constraining tokens, 49, \n59 <br><br>LangChain XML parser warning, 52 \npipe (|) separator, 56 \nreferences, 62 \nsolution, 49-53 \nBackus-Naur form of grammar, 51 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "913c4ee2-feb8-49ea-aca5-c0da107caf08": {"__data__": {"id_": "913c4ee2-feb8-49ea-aca5-c0da107caf08", "embedding": null, "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 416.25, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fcd7947e-0db0-441a-ba90-3149669ac621", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 85.52, "x1": 234.9, "y1": 202.52}]}, "hash": "e74f41461d411912c37e7c68512e89517256c498700da85025a507a0641e0128", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77163a6e-71ad-44b8-b689-ee9ac6a5bffb", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 186.42, "x1": 431.39, "y1": 605.82}]}, "hash": "877507920f457ec50a6ca17fac864a08de1ab0a0a575cf72828cca06d501c13f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "creating images in preferred style, 99 \ngenerating classified ads, 93-97 <br><br>problem, 83-85 \nreferences, 102 \nsolution, 85-92 <br><br>direct preference optimization, 92 \npreference dataset creation, 86-92 \nPattern 6: Basic RAG, 108-122 <br><br>about, 108, 206 \nabout world knowledge of foundational \nmodels, 108 <br><br>benefits of RAG, 109, 119 \nconsiderations, 119-121 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77163a6e-71ad-44b8-b689-ee9ac6a5bffb": {"__data__": {"id_": "77163a6e-71ad-44b8-b689-ee9ac6a5bffb", "embedding": null, "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 186.42, "x1": 431.39, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "913c4ee2-feb8-49ea-aca5-c0da107caf08", "node_type": "1", "metadata": {"bbox": [{"page": 495, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 416.25, "y1": 217.02}]}, "hash": "53254df6b03e6433bf6b64fd85af2661d3567541ba4b243356b5d5e0ec810fd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5dec663-5ec6-4e7d-9ed9-0e4bb0f3cd68", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 227.3, "y1": 217.02}]}, "hash": "067359b0af68999c91e12ce41578f7adc3dd6ba09b814797b8ae38ac4ad93e8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "types of metadata, 143 \nusing metadata, 143 \nPattern 9: Index-Aware Retrieval, 151-163 <br><br>about, 153, 206 \nconsiderations, 162 \nexample: Semantic Indexing versus, 159-162 <br><br>HyDE approach, 161 \nquery expansion, 162 \nSemantic Indexing limitations, 159 \nproblem, 151 <br><br>answer as detail hidden in chunk, 152 \nanswer involving holistic interpretation, \n152 <br><br>question not in knowledge base, 151 \ntechnical language differences, 152 <br><br>references, 163 \nsolution <br><br>GraphRAG, 157-159 \nhybrid searches of keyword and mean\u2010 \ning, 156 <br><br>hypothetical answers, 155 \nquery expansion, 156 \nPattern 10: Node Postprocessing, 163-172 <br><br>about, 163, 206 \nconsiderations, 172 \nexample: semantic RAG versus, 170-172 \nnode postprocessing impact, 171 \nsemantic RAG limitations, 170 <br><br>problem, 164 \nreferences, 172 \nsolution, 165-188 \nreranking, 166 \nPattern 11: Trustworthy Generation, 173-193 <br><br>about, 173, 206 \nconsiderations, 192 \nexample: adding citations, 188 \nproblem, 173 \nreferences, 193 \nsolution, 165-188 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5dec663-5ec6-4e7d-9ed9-0e4bb0f3cd68": {"__data__": {"id_": "f5dec663-5ec6-4e7d-9ed9-0e4bb0f3cd68", "embedding": null, "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 227.3, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "77163a6e-71ad-44b8-b689-ee9ac6a5bffb", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 186.42, "x1": 431.39, "y1": 605.82}]}, "hash": "877507920f457ec50a6ca17fac864a08de1ab0a0a575cf72828cca06d501c13f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "172b09fc-1d8a-45b3-9bf0-ed395f2702d3", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 424.33, "y1": 184.62}]}, "hash": "a9b5d7b0815621d9ddb45b8e889a1e309ad383edf08dd7b20ced1b614d71dd06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "metadata alternatives, 149 \nmetadata limitations, 149 \nproblem, 140-142 <br><br>contradictory information, 141 \ndata freshness, 140 \ndisambiguation, 140 \nmodel lifecycle, 142 <br><br>references, 150 \nsolution, 143-147 <br><br>detecting contradictory content, 144 \ndetecting outdated content, 145 \nmanaging model lifecycle, 146 \nmetadata, 143-147 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "172b09fc-1d8a-45b3-9bf0-ed395f2702d3": {"__data__": {"id_": "172b09fc-1d8a-45b3-9bf0-ed395f2702d3", "embedding": null, "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 424.33, "y1": 184.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f5dec663-5ec6-4e7d-9ed9-0e4bb0f3cd68", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 227.3, "y1": 217.02}]}, "hash": "067359b0af68999c91e12ce41578f7adc3dd6ba09b814797b8ae38ac4ad93e8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62cc3979-57bf-4b57-9f5f-c6daceb89f7a", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 197.22, "x1": 430.16, "y1": 605.82}]}, "hash": "78294bbefa772a6675d80d6c30ea592ddc7c4f0f7dfd9a28d29c5c40bf796a17", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "citations, 175-178 \ncorrective retrieval-augmented genera\u2010 \ntion, 182-184 <br><br>guardrails, 179 \nhuman feedback, 181 \nobservability, 181 \nout-of-domain detection, 174 \nself-RAG, 185 \nuser interface design, 186-188 \nPattern 12: Deep Search, 193-205 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62cc3979-57bf-4b57-9f5f-c6daceb89f7a": {"__data__": {"id_": "62cc3979-57bf-4b57-9f5f-c6daceb89f7a", "embedding": null, "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 197.22, "x1": 430.16, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "172b09fc-1d8a-45b3-9bf0-ed395f2702d3", "node_type": "1", "metadata": {"bbox": [{"page": 496, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 424.33, "y1": 184.62}]}, "hash": "a9b5d7b0815621d9ddb45b8e889a1e309ad383edf08dd7b20ced1b614d71dd06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "090b47d2-87f4-4339-bf9b-46c8d2ccdb0a", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.89, "y1": 217.02}]}, "hash": "fa30405a612c0a167f69ad8d78f96904206c05e1a0cc96c27599950b032db2e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "path evaluation, 229, 230 \nsummary generation, 229, 232 \nthought generation, 229-230 \nPattern 15: Adapter Tuning, 240-256 <br><br>about, 240, 243, 273 \nconsiderations, 254-256 \nalternatives, 256 \nvariations, 254-256 \nexample: image input, text output, 249-254 <br><br>adapter tuning, 252 \ndataset preparation, 251 \ngoal of anatomical descriptions, 249 \ninference, 252-254 <br><br>not for jargon or new facts, 243, 244 \nproblem, 240 \nreferences, 256 \nsolution, 241-249 <br><br>architecture, 241-247 \ninference, 249 \nparameter-efficient fine-tuning, 245 \ntraining, 247-249 \nwhen prompt engineering not viable, 250 \nPattern 16: Evol-Instruct, 257-273 <br><br>about, 257, 273 \nconsiderations, 272 \nexample, 265-272 \nanswers, 269 \nevaluating examples, 270 \ngoal: business strategy consultant, 265 \nquestions, 266-269 \ntraining, 270 <br><br>problem, 257 \nreferences, 273 \nsolution, 258-265 \ninference, 262 \ninstruction tuning, 260-262 \ninstruction-tuning dataset prep, 262-265 \nPeFT, 261 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "090b47d2-87f4-4339-bf9b-46c8d2ccdb0a": {"__data__": {"id_": "090b47d2-87f4-4339-bf9b-46c8d2ccdb0a", "embedding": null, "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.89, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "62cc3979-57bf-4b57-9f5f-c6daceb89f7a", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 197.22, "x1": 430.16, "y1": 605.82}]}, "hash": "78294bbefa772a6675d80d6c30ea592ddc7c4f0f7dfd9a28d29c5c40bf796a17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deb06cd8-6c6a-417e-b762-d75830610d62", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 428.01, "y1": 195.42}]}, "hash": "f1b53543835565c84742fcac77a0100de820686cb6a7e350cff01b41db4dec99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "about, 226, 228, 273 \nchain-of-thought versus ToT, 226 \nconsiderations, 238-240 \nalternatives, 238-240 \ncomplexity, 238 \nexample: supply chain optimization, <br><br>233-238 \nexample online, 236 \nproblem, 227 \nrandom sentences example online, 227 <br><br>references, 240 \nsolution, 228-233 \nbeam search, 229, 231 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "deb06cd8-6c6a-417e-b762-d75830610d62": {"__data__": {"id_": "deb06cd8-6c6a-417e-b762-d75830610d62", "embedding": null, "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 428.01, "y1": 195.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "090b47d2-87f4-4339-bf9b-46c8d2ccdb0a", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 235.89, "y1": 217.02}]}, "hash": "fa30405a612c0a167f69ad8d78f96904206c05e1a0cc96c27599950b032db2e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea7fe245-ba09-4c02-98c7-65ba0608c36c", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 425.22, "y1": 605.82}]}, "hash": "1097b0fd9b95c423da31e7e37f1e9ecb09be1abb9bc0938ba4454f9e3a9fdaa1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 17: LLM-as-Judge, 275-286 <br><br>about, 275, 311 \nconsiderations, 284 <br><br>bias, 285 \ncaveats, 285 \ninconsistency, 284 \nleniency, 284 <br><br>example: evaluating quality of text, 282-284 \nproblem, 276 \nreferences, 286 \nsolution, 277 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea7fe245-ba09-4c02-98c7-65ba0608c36c": {"__data__": {"id_": "ea7fe245-ba09-4c02-98c7-65ba0608c36c", "embedding": null, "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 425.22, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "deb06cd8-6c6a-417e-b762-d75830610d62", "node_type": "1", "metadata": {"bbox": [{"page": 497, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 428.01, "y1": 195.42}]}, "hash": "f1b53543835565c84742fcac77a0100de820686cb6a7e350cff01b41db4dec99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d63201b5-981a-4847-aac8-15880e8f515f", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 218.07, "y1": 217.02}]}, "hash": "db786c089c0456866f496f06d0a32c066a47e9dcb8eef8afb7ddc34f38a3592b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 22: Code Execution, 329-333 <br><br>about, 329, 351 \nconsiderations, 332 <br><br>sandboxing the code execution, 332 \nexample: graphing basketball tournament, \n331 <br><br>problem, 329 \nreferences, 333 \nsolution, 330 \nPattern 23: Multiagent Collaboration, 333-351 <br><br>about, 333, 351 \nAgent2Agent protocol, 349 \nconsiderations, 348-350 \nexample: creating content with AG2, <br><br>341-348 \nagents and workflow set up, 343 \nassigning topic to writer, 344 \ncreating initial draft, 344 \nreview panel created, 346 \nreview panel discussion, 346 \nsummarizing discussion into feedback, \n347 <br><br>supplying feedback to writer agent, 347 \nwriting final content, 347 <br><br>problem, 333 \nreferences, 351 \nsolution, 334-341 <br><br>multiagent architectures, 336-340 \nmultiple specialized agents, 335 \nuse cases, 340 \nPattern 24: Small Language Model (SLM), <br><br>354-371 \nabout, 354, 415 \nconsiderations, 369-371 \nalternatives, 370 \nlimitations, 369 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d63201b5-981a-4847-aac8-15880e8f515f": {"__data__": {"id_": "d63201b5-981a-4847-aac8-15880e8f515f", "embedding": null, "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 218.07, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ea7fe245-ba09-4c02-98c7-65ba0608c36c", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 218.82, "x1": 425.22, "y1": 605.82}]}, "hash": "1097b0fd9b95c423da31e7e37f1e9ecb09be1abb9bc0938ba4454f9e3a9fdaa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cfedb9e-f339-4f9a-b7c5-4b9c6508be9b", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.2, "y1": 217.02}]}, "hash": "fccee6a478a0d1fcd2f6c94fd4642b55baefbc30e032b02cd8cdd633aa345ad0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "example in OpenAI: booking a flight, \n316-321 <br><br>example: weather tool, 324-326 \ngeocoding tool called, 325 \nMCP client, 325 \nMCP server, 325 <br><br>problem, 313 \nreferences, 329 \nsolution, 314 <br><br>function calling in OpenAI, 316 \nhow tool calling works, 314-316 \nLangGraph and MCP, 321-324 \nsupport for tool calling in LLMs, 319 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9cfedb9e-f339-4f9a-b7c5-4b9c6508be9b": {"__data__": {"id_": "9cfedb9e-f339-4f9a-b7c5-4b9c6508be9b", "embedding": null, "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.2, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d63201b5-981a-4847-aac8-15880e8f515f", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 218.07, "y1": 217.02}]}, "hash": "db786c089c0456866f496f06d0a32c066a47e9dcb8eef8afb7ddc34f38a3592b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaa1a8e4-9dd8-4806-996e-15e3e7a74433", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 231.07, "y1": 605.82}]}, "hash": "97430688c9d5565e8bb7f15efd35be428af74fa91e336fe74659d6c483ecda94", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "example: Python code documenter, 367-369 <br><br>distillation, 367-369 \nquantization, 369 \nproblem, 354-356 \nquality of response, 354 <br><br>references, 371 \nsolution, 356-367 <br><br>distillation, 357-360 \nKL divergence, 359 \nquantization, 360-363 \nspeculative decoding, 364-367 \nPattern 25: Prompt Caching, 372-381 \nabout, 371, 372, 415 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aaa1a8e4-9dd8-4806-996e-15e3e7a74433": {"__data__": {"id_": "aaa1a8e4-9dd8-4806-996e-15e3e7a74433", "embedding": null, "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 231.07, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9cfedb9e-f339-4f9a-b7c5-4b9c6508be9b", "node_type": "1", "metadata": {"bbox": [{"page": 498, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 430.2, "y1": 217.02}]}, "hash": "fccee6a478a0d1fcd2f6c94fd4642b55baefbc30e032b02cd8cdd633aa345ad0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b99c67bc-90b7-4233-897b-01f633332e0b", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 222.1, "y1": 227.82}]}, "hash": "f7fdf6034c732982c4ab1a28ed3d72d14bd0224d0edb206422919df83b115c4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "considerations, 380 \nexample: client-side caching, 377-380 \nproblem, 372 \nreferences, 381 \nsolution, 372-377 <br><br>client-side prompt caching, 373-376 \nclient-side versus server-side, 373 \nserver-side prompt caching, 376 \nPattern 26: Inference Optimization, 381-390 <br><br>about, 381, 415 \nexample, 388-390 \nproblem, 382 \nreferences, 390 \nsolution, 382-387 <br><br>continuous batching, 382-384 \nprompt compression, 385-387 \nspeculative decoding, 384 \nPattern 27: Degradation Testing, 390-403 <br><br>about, 390, 415 \nexample: LLM benchmarking tool, 400 \nperformance-monitoring tools, 402 \nproblem, 390 \nreferences, 403 \nsolution, 391-400 <br><br>core metrics, 391-397 \nload testing, 398-400 \nscalability, 397 \nstress analysis, 398 \nPattern 28: Long-Term Memory, 403-415 <br><br>about, 403, 415 \nconsiderations, 414 \nexample, 409-414 <br><br>adding memories, 410 \nMem0, 409 \nsearching memories, 412-414 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b99c67bc-90b7-4233-897b-01f633332e0b": {"__data__": {"id_": "b99c67bc-90b7-4233-897b-01f633332e0b", "embedding": null, "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 222.1, "y1": 227.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "aaa1a8e4-9dd8-4806-996e-15e3e7a74433", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 229.62, "x1": 231.07, "y1": 605.82}]}, "hash": "97430688c9d5565e8bb7f15efd35be428af74fa91e336fe74659d6c483ecda94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1ef7693-d4e6-4eb7-a09f-806e2e6b911b", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 426.32, "y1": 249.42}]}, "hash": "e9687753359630980ef6fa94c8161218a07e11b961d15eb436567b9f588cc67d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "problem, 403 \nreferences, 414 \nsolution, 404-409 \nPattern 29: Template Generation, 417-422 <br><br>about, 417, 443 \nconsiderations, 422 \nexample: thank-you notes, 420-421 \nproblem, 417 \nreferences, 422 \nsolution, 418-420 \nPattern 30: Assembled Reformat, 422-426 <br><br>about, 422, 443 \nconsiderations, 425 \nexample: product catalog, 424 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1ef7693-d4e6-4eb7-a09f-806e2e6b911b": {"__data__": {"id_": "e1ef7693-d4e6-4eb7-a09f-806e2e6b911b", "embedding": null, "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 426.32, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b99c67bc-90b7-4233-897b-01f633332e0b", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 222.1, "y1": 227.82}]}, "hash": "f7fdf6034c732982c4ab1a28ed3d72d14bd0224d0edb206422919df83b115c4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8238174-bfa6-442d-b3e0-8b705513624e", "node_type": "1", "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.82, "y1": 605.82}]}, "hash": "529f27ae457e0b3323efebf8175e6d881449e13adff5ed635cb982d2cd7146d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "personality of response, 5 \npersonalization \nPattern 4: Reverse Neutralization emails \ngenerated, 75, 78-82 \nPattern 10: Node Postprocessing enabling, \n169 <br><br>perspective differences handled by HyDE, 155 \npipe (|) separator <br><br>response items separated by, 56 \nsynonym expansion in prompt, 131 <br><br>poem generation, 41-46 \npost-training with foundational models, 22-27 <br><br>dataset structure, 24 \ndetermining if post-training possible, 24 \nmethods, 22 \ncontinued pretraining, 23 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8238174-bfa6-442d-b3e0-8b705513624e": {"__data__": {"id_": "f8238174-bfa6-442d-b3e0-8b705513624e", "embedding": null, "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.82, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e1ef7693-d4e6-4eb7-a09f-806e2e6b911b", "node_type": "1", "metadata": {"bbox": [{"page": 499, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 426.32, "y1": 249.42}]}, "hash": "e9687753359630980ef6fa94c8161218a07e11b961d15eb436567b9f588cc67d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1343be09-055e-488b-8950-c0c23143f667", "node_type": "1", "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 426.14, "y1": 271.02}]}, "hash": "17d99bdb24ba75d63a035bc48231bb8f48cebce659b5ea1c11790c080119b6dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "problem, 354-356 \nquality of response, 354 \nquantization, 360-363, 369 \nreferences, 371 \nsolution, 356-367 \nspeculative decoding, 364-367 \nPattern 25: Prompt Caching, 372-381 <br><br>about, 372, 415 \nclient-side prompt caching, 373-376 \nclient-side versus server-side, 373 \nconsiderations, 380 \nexample: client-side caching, 377-380 \nproblem, 372 \nreferences, 381 \nserver-side prompt caching, 376 \nsolution, 372-377 \nPattern 26: Inference Optimization, 381-390 <br><br>about, 381, 415 \ncontinuous batching, 382-384 \nexample, 388-390 \nproblem, 382 \nprompt compression, 385-387 \nreferences, 390 \nsolution, 382-387 \nspeculative decoding, 384 \nPattern 27: Degradation Testing, 390-403 <br><br>about, 390, 415 \ncore metrics, 391-397 \nexample: LLM benchmarking tool, 400 \nload testing, 398-400 \nperformance-monitoring tools, 402 \nproblem, 390 \nreferences, 403 \nscalability, 397 \nsolution, 391-400 \nstress analysis, 398 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1343be09-055e-488b-8950-c0c23143f667": {"__data__": {"id_": "1343be09-055e-488b-8950-c0c23143f667", "embedding": null, "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 426.14, "y1": 271.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f8238174-bfa6-442d-b3e0-8b705513624e", "node_type": "1", "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 218.82, "x1": 430.82, "y1": 605.82}]}, "hash": "529f27ae457e0b3323efebf8175e6d881449e13adff5ed635cb982d2cd7146d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcf9694-e60d-4d3b-a7bc-ab9853e49ea3", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 305.22, "x1": 422.37, "y1": 605.82}]}, "hash": "b336ce8ae27678c54c9b05fd55a436e48f236870ed4a34dfeba59560df9a6914", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG benefits, 109 \nRAG for confidential or personalized \ndata, 109 \nlocally hostable models, 12 \n(see also Pattern 24: Small Language <br><br>Model (SLM); Pattern 26: Inference \nOptimization) \nprocedural memory, 407 \nproduct catalog example, 424 \nproduction systems <br><br>Pattern 28: Long-Term Memory, 403-415 <br><br>about, 403, 415 \nconsiderations, 414 \nexample, 409-414 \nlong-term memory explained, 404-409 \nproblem, 403 \nreferences, 414 \nsolution, 404-409 \nabout patterns, 353, 415 \nPattern 24: Small Language Model, 354-371 <br><br>about, 354, 415 \nconsiderations, 369-371 \ndistillation, 357-360, 367-369 \nexample: Python code documenter,  progressive disclosure techniques, 187 \nProject Gutenberg as training data source, 108 \nPrompt Caching (Pattern 25), 372-381 \n367-369 \nabout, 371, 372, 415 \nconsiderations, 380  KL divergence, 359 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bcf9694-e60d-4d3b-a7bc-ab9853e49ea3": {"__data__": {"id_": "1bcf9694-e60d-4d3b-a7bc-ab9853e49ea3", "embedding": null, "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 305.22, "x1": 422.37, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1343be09-055e-488b-8950-c0c23143f667", "node_type": "1", "metadata": {"bbox": [{"page": 500, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 426.14, "y1": 271.02}]}, "hash": "17d99bdb24ba75d63a035bc48231bb8f48cebce659b5ea1c11790c080119b6dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2204a59-1851-49fa-8f33-3e8cdf0a2dea", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 237.69, "y1": 303.42}]}, "hash": "e902cfcceb56393ee5df2f6ecfbd3a84737bb34e5d286f48fe50a19155a84b72", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern 25: Prompt Caching client side \nexample, 377-380 <br><br>post-training supervised fine-tuning, 23 \nprompt compression, 381 <br><br>hard prompt compression, 385 \nPattern 26: Inference Optimization, \n385-387 \nsoft prompt compression, 387 \nprompt engineering <br><br>controlling response formatting, 49 \ncontrolling response style, 32 \ngrounding the LLM response, 110-112 \nquery expansion, 156 \nRAG pipelines populating prompt, \n112-118 \nwhen not a viable option, 250 <br><br>self-RAG, 185 \nsummarizing a document to prepend to \nchunk, 133 <br><br>synonym expansion, 131 \nsystem prompt, 6 <br><br>Anthropic API invoking Claude Sonnet 3, 5 \nchain-of-thought reasoning, 212, 215-220 \ncontext, 5 \nfew-shot learning, 21 \nconsiderations, 73 \nPattern 3: Style Transfer, 63, 66 \ngenerating a classified ad with zero-shot <br><br>user personalizing, 407 <br><br>user prompt, 6 \nPydantic AI framework <br><br>Agent class invoking model, 6, 12 \ndocumentation online, 6 \ninvoking foundational models, 6 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2204a59-1851-49fa-8f33-3e8cdf0a2dea": {"__data__": {"id_": "e2204a59-1851-49fa-8f33-3e8cdf0a2dea", "embedding": null, "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 237.69, "y1": 303.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1bcf9694-e60d-4d3b-a7bc-ab9853e49ea3", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 305.22, "x1": 422.37, "y1": 605.82}]}, "hash": "b336ce8ae27678c54c9b05fd55a436e48f236870ed4a34dfeba59560df9a6914", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1951d7f-32e5-4f21-bbbe-d1a1b6b9a08e", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 423.2, "y1": 314.22}]}, "hash": "99df3fc550add97e4ca5fcdd7569376769335482ab604a4170d8643bd1477843", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "learning, 93 \nin-context learning, 20-22 <br><br>about, 20 \ncontext engineering, 74 \nfew-shot learning, 21, 63, 66 \nfew-shot learning considerations, 73 \nzero-shot learning, 20 \ninstructions to model, 4 \ninstruction tuning, 257 \n(see also Pattern 16: Evol-Instruct) \npriming, 110 \n(see also RAG (retrieval-augmented \ngeneration)) <br><br>least-to-most prompting, 239 \nlong-term memory, 404 \nmultimodal prompts, 4 \nneutralizing personalized email, 78 \nPattern 17: LLM-as-Judge prompting \napproach, 279 \nPattern 20: Prompt Optimization example, \n305-310 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1951d7f-32e5-4f21-bbbe-d1a1b6b9a08e": {"__data__": {"id_": "f1951d7f-32e5-4f21-bbbe-d1a1b6b9a08e", "embedding": null, "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 423.2, "y1": 314.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e2204a59-1851-49fa-8f33-3e8cdf0a2dea", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 237.69, "y1": 303.42}]}, "hash": "e902cfcceb56393ee5df2f6ecfbd3a84737bb34e5d286f48fe50a19155a84b72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cab1e53-df29-4146-af83-8afbf51d4178", "node_type": "1", "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 240.42, "x1": 430.76, "y1": 605.82}]}, "hash": "bc80d6ca835c21b53b5dbb24cb10dc3af817620d65d27e485bf724f70fae7f2b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Python <br><br>assert statement, 301 \nBitsAndBytesConfig library, 362 \nclient-side caching, 373 \ndata classes constraining output, 53 \nGuardrails AI package, 180 \nlibraries for PDF parsing, 118 \nPattern 24: Small Language Model Python \ncode documenter, 367-369 <br><br>text chunking methods, 128 \nvirtual environments, 447 \nWikipedia package for retrieval, 201 <br><br>**Q**\nQAT (quantization-aware training), 361, 371 \nQLoRA (quantized low-rank adaptation), 23, <br><br>247, 362 \nquantization <br><br>about, 354, 356 \ndynamic quantization, 362 \nexplained, 360 \nfull-model quantization, 362 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cab1e53-df29-4146-af83-8afbf51d4178": {"__data__": {"id_": "2cab1e53-df29-4146-af83-8afbf51d4178", "embedding": null, "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 240.42, "x1": 430.76, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f1951d7f-32e5-4f21-bbbe-d1a1b6b9a08e", "node_type": "1", "metadata": {"bbox": [{"page": 501, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 423.2, "y1": 314.22}]}, "hash": "99df3fc550add97e4ca5fcdd7569376769335482ab604a4170d8643bd1477843", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0e20e99-7055-4d38-9dc4-1079b2ee96b7", "node_type": "1", "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 431.27, "y1": 369.05}]}, "hash": "fd998bc1c4e208eb1ec59b021eac9c3b14f1f50f1e55cc328e65e2beb80b5a73", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "indexing, 112-115 \nmetadata, 114 \n(see also metadata) <br><br>pipelines, 112-117 \nproblem, 108 \nreferences, 121 \nretrieval, 112-114, 115-117 \nsolution, 109-118 <br><br>PDF document ingestion, 118 \nreranking, 166 \ntable handling, 130 <br><br>Ragas, 180 \nRaifalov, R., 92 \nRAPTOR (Recursive Abstractive Processing for \nTree-Organized Retrieval), 133 <br><br>rarity of a term, 116 \nRecursive Abstractive Processing for Tree- \nOrganized Retrieval (RAPTOR), 133 <br><br>Reddit as training data, 108 \nreflection, 239 \nreflection in RAG, 185 \nReflection (Pattern 18), 286-295 <br><br>about, 286, 311 \nconsiderations, 293-295 \nexample: designing a logo, 289-293 \nPattern 17: LLM-as-Judge for evaluator, \n289 <br><br>problem, 286 \nreferences, 295 \nsolution, 287 \nconversational state, 288 \nreinforcement learning (RL) \npure RL, 9 \nchain-of-thought reasoning, 10 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0e20e99-7055-4d38-9dc4-1079b2ee96b7": {"__data__": {"id_": "a0e20e99-7055-4d38-9dc4-1079b2ee96b7", "embedding": null, "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 431.27, "y1": 369.05}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2cab1e53-df29-4146-af83-8afbf51d4178", "node_type": "1", "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 240.42, "x1": 430.76, "y1": 605.82}]}, "hash": "bc80d6ca835c21b53b5dbb24cb10dc3af817620d65d27e485bf724f70fae7f2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c86b96-25ec-4e5f-b3af-ce0537336583", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.98, "y1": 605.82}]}, "hash": "68dc3b9bf00eeb8bd7cc2e97442edb22f0f4911d150e23ae1dbd0f5908766836", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**R**\nRAG (retrieval-augmented generation) <br><br>about, 105, 119 \nabout world knowledge of foundational <br><br>models, 108 \n(see also adding knowledge to founda\u2010 \ntional models) <br><br>benefits of RAG, 109, 119 \ndetails needing attention in production, \n140-142 <br><br>few-shot CoT versus RAG, 216 \nfully managed RAG systems ingesting PDF, \nTransformer Reinforcement Learning (Hug\u2010 \n118 \nging Face), 247 \nGraphRAG, 157-159 \nreinforcement learning with human feedback \ndeepset on financial data online, 163 <br><br>(RLHF), 9 \npreference tuning, 23 \nhallucinations <br><br>reduced, 109 \nunfamiliar domain, 162 \nrejection sampling, 9 \nreliability improved <br><br>larger context window versus RAG, 120 \nlimitations, 121 \nPattern 32: Guardrails example, 440-442 \nPattern 6: Basic RAG, 108-122 <br><br>about patterns, 275, 311 \nPattern 17: LLM-as-Judge, 275-286 <br><br>about, 275, 311 \ncaveats, 285 \nconsiderations, 284 \nexample: evaluating quality of text, <br><br>about, 108 \nconsiderations, 119-121 \nexample: indexing and retrieval, 118 \ngeneration, 117 \ngrounding LLM responses, 110-112 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06c86b96-25ec-4e5f-b3af-ce0537336583": {"__data__": {"id_": "06c86b96-25ec-4e5f-b3af-ce0537336583", "embedding": null, "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.98, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a0e20e99-7055-4d38-9dc4-1079b2ee96b7", "node_type": "1", "metadata": {"bbox": [{"page": 502, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 431.27, "y1": 369.05}]}, "hash": "fd998bc1c4e208eb1ec59b021eac9c3b14f1f50f1e55cc328e65e2beb80b5a73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41d5831e-182f-4895-9834-6d4915fb5ea5", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 89.22, "x1": 430.34, "y1": 249.42}]}, "hash": "a4ea97970a75c328ccbd498ad83d0ea8ce670f4626698a52f465f1da8f9ecd24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ML approach, 280 \nproblem, 276 \nprompting approach, 279 \nreferences, 286 \nsolution, 277 \nPattern 18: Reflection, 286-295 <br><br>about, 286, 311 \nconsiderations, 293-295 \nconversational state, 288 \nexample: designing a logo, 289-293 \nproblem, 286 \nreferences, 295 \nsolution, 287 \nPattern 19: Dependency Injection, 295-302 <br><br>about, 295, 311 \nconsiderations, 301 \nexample: marketing text, 296-299 \nproblem developing GenAI applications, \n295 <br><br>references, 302 \nsolution, 299-301 \nPattern 20: Prompt Optimization, 302-311 <br><br>about, 302, 311 \nconsiderations, 310 \nexample, 305-310 \nproblem, 302 \nsolution, 302-305 \nrequests per second (RPS), 396 \nreranking, 166 \nrerankers adding costs, 172 \nresources <br><br>AI Engineering (Huyen), 2 \nbooks on building GenAI applications, 3 \nDesign Patterns: Elements of Reusable \nObject-Oriented Software (Gamma, \nHelm, Johnson, and Vlissides), 1 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41d5831e-182f-4895-9834-6d4915fb5ea5": {"__data__": {"id_": "41d5831e-182f-4895-9834-6d4915fb5ea5", "embedding": null, "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 89.22, "x1": 430.34, "y1": 249.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "06c86b96-25ec-4e5f-b3af-ce0537336583", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 218.82, "x1": 242.98, "y1": 605.82}]}, "hash": "68dc3b9bf00eeb8bd7cc2e97442edb22f0f4911d150e23ae1dbd0f5908766836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bad559f3-157c-43d2-ae2c-e6c040993564", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 239.46, "y1": 217.02}]}, "hash": "1a49eec8417b644728b627ed87224bba15d3a8c859217bffab66aa65d6be9883", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "text, 296 \nPattern 20: Prompt Optimization marketing \ntext, 305, 308 \nPattern 22: Code Execution graphing bas\u2010 \nketball tournament, 331 \nPattern 23: Multiagent Collaboration exam\u2010 \nple in AG2, 341 \nPattern 24: Small Language Model <br><br>distillation, 358 \nexample training dataset script, 368 \nexample: Python code documenter, 367, \n369 \nspeculative decoding, 366 \nPattern 25: Prompt Caching client-side \nexample, 380 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bad559f3-157c-43d2-ae2c-e6c040993564": {"__data__": {"id_": "bad559f3-157c-43d2-ae2c-e6c040993564", "embedding": null, "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 239.46, "y1": 217.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "41d5831e-182f-4895-9834-6d4915fb5ea5", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 89.22, "x1": 430.34, "y1": 249.42}]}, "hash": "a4ea97970a75c328ccbd498ad83d0ea8ce670f4626698a52f465f1da8f9ecd24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d0d5206-9b98-4fe5-8095-3dd61c69c5b2", "node_type": "1", "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.22, "x1": 239.36, "y1": 400.62}]}, "hash": "68167e61dc9155e61940b0e68f89a34bdf6997e2392f94548c3bd6ec80f3726e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine Learning Design Patterns (Laksh\u2010 \nmanan, Robinson, and Munn), xii <br><br>A Pattern Language (Alexander), 1 \nRetrieval-Augmented Generation for <br><br>Knowledge-Intensive NLP Tasks (Lewis \net al.), 105 \nresources online \nAnthropic API invoking Claude Sonnet 3 \ncode, 5 <br><br>book supplemental material, xiv \nbook web page, xv \nbridge card game API call results, 211 \ndeepset GraphRAG on financial data , 163 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d0d5206-9b98-4fe5-8095-3dd61c69c5b2": {"__data__": {"id_": "8d0d5206-9b98-4fe5-8095-3dd61c69c5b2", "embedding": null, "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.22, "x1": 239.36, "y1": 400.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "bad559f3-157c-43d2-ae2c-e6c040993564", "node_type": "1", "metadata": {"bbox": [{"page": 503, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 239.46, "y1": 217.02}]}, "hash": "1a49eec8417b644728b627ed87224bba15d3a8c859217bffab66aa65d6be9883", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f7269fd-5a60-4cb1-9b31-aeafd377e0a8", "node_type": "1", "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 427.17, "y1": 422.22}]}, "hash": "7393cf2c16fda84383032761b5384b556513e1705ac928f792022cf228968036", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "classification-based citations, 177 \nexample of adding citations, 188 \nsource-level tracking, 175 \ntoken-level attribution, 177 \ngrounding response in trusted knowledge, \n109-118 <br><br>instruction prompts, 4 \nJSON-formatted response, 52 \nmultimodal responses, 4 \njudging visual responses, 4 <br><br>personality of response, 5 \nstochastic responses, 31, 275 \n(see also controlling content style; relia\u2010 \nbility improved) <br><br>wait-injection, 240 \nretrieval, 112-114, 115-117 \nBM25 algorithm, 116 \ncontextual retrieval, 132 \nPattern 12: Deep Search example on Wiki\u2010 \npedia, 201 <br><br>Pattern 6: Basic RAG example, 118 \nPattern 9: Index-Aware Retrieval, 151-163 <br><br>about, 153, 206 \nconsiderations, 162 \nexample: Semantic Indexing versus, <br><br>159-162 \nproblem, 151 \nreferences, 163 \nsolution, 153-159 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f7269fd-5a60-4cb1-9b31-aeafd377e0a8": {"__data__": {"id_": "6f7269fd-5a60-4cb1-9b31-aeafd377e0a8", "embedding": null, "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 427.17, "y1": 422.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "8d0d5206-9b98-4fe5-8095-3dd61c69c5b2", "node_type": "1", "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 89.22, "x1": 239.36, "y1": 400.62}]}, "hash": "68167e61dc9155e61940b0e68f89a34bdf6997e2392f94548c3bd6ec80f3726e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3403c672-38c4-4407-a758-c535271cb3f4", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.02, "x1": 423.65, "y1": 605.82}]}, "hash": "fe8f1c6b01cca651db7903b95aeb36f79bab6a50810a13a7abb82864b719d376", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Robinson, Sara, xii \nRPS (requests per second), 396 \nrunning your model locally, 6, 12 <br><br>(see also Pattern 24: Small Language Model \n(SLM); Pattern 26: Inference Optimiza\u2010 \ntion) <br><br>runtime compute, 111 \nRunwayML Stable Diffusion model \nPattern 3: Style Transfer example with \nimages, 72 \nPattern 5: Content Optimization example \nwith images, 99 <br><br>**S**\nsafeguards <br><br>about patterns, 417, 443 \nPattern 29: Template Generation, 417-422 <br><br>about, 417, 443 \nconsiderations, 422 \nexample: thank-you notes, 420-421 \nproblem, 417 \nreferences, 422 \nsolution, 418-420 \nPattern 30: Assembled Reformat, 422-426 <br><br>about, 422, 443 \nconsiderations, 425 \nexample: product catalog, 424 \nproblem, 423 \nreferences, 426 \nsolution, 423 \nPattern 31: Self-Check, 426-436 \nabout, 426, 443 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3403c672-38c4-4407-a758-c535271cb3f4": {"__data__": {"id_": "3403c672-38c4-4407-a758-c535271cb3f4", "embedding": null, "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.02, "x1": 423.65, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6f7269fd-5a60-4cb1-9b31-aeafd377e0a8", "node_type": "1", "metadata": {"bbox": [{"page": 504, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 74.72, "x1": 427.17, "y1": 422.22}]}, "hash": "7393cf2c16fda84383032761b5384b556513e1705ac928f792022cf228968036", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00bf0a76-7f33-4cfe-85ec-c31b7820ef04", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 294.42, "x1": 430.2, "y1": 422.22}]}, "hash": "383ef0fe93351d8205601130918eaae85392439701e1ad628ed3a7ad1d162d8c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "considerations, 436 \nexample: extracting values from receipts, \n433-435 <br><br>contextual retrieval, 132 \nembeddings, 123-127 \nhierarchical chunking, 133 \nimages and videos, 128 \njargon, 131 \nsemantic chunking, 127 \ntables, 130 \nsemantic memory, 408 \nsequences in Pattern 1: Logits Masking, 32-48 \nsequence regeneration example, 41-46 \nsequence selection example, 40-41 <br><br>problem, 426-428 \nreferences, 436 \nsolution, 428-432 \nPattern 32: Guardrails, 436-443 <br><br>about guardrails, 436 \nabout pattern, 443 \nconsiderations, 442 \nexample, 440-442 \nproblem, 436 \nreferences, 443 \nsolution, 438-440  SFT (supervised fine-tuning), 9, 10 <br><br>sandboxing Pattern 22: Code Execution, 332 \nSanseviero, Omar, 3 \nscalability in Pattern 27: Degradation Testing, <br><br>post-training method, 23 \nTransformers library, 260 \nSmall Language Model (SLM; Pattern 24), ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00bf0a76-7f33-4cfe-85ec-c31b7820ef04": {"__data__": {"id_": "00bf0a76-7f33-4cfe-85ec-c31b7820ef04", "embedding": null, "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 294.42, "x1": 430.2, "y1": 422.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3403c672-38c4-4407-a758-c535271cb3f4", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 424.02, "x1": 423.65, "y1": 605.82}]}, "hash": "fe8f1c6b01cca651db7903b95aeb36f79bab6a50810a13a7abb82864b719d376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1e84331-fb39-4b25-99b3-fa29d70d90dc", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 175.62, "x1": 428.12, "y1": 422.22}]}, "hash": "36149bc4f3f7f1e96d95e366a2aee3e7895038e5e0f5e975f20dd726ffad6ed6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "354-371 \nabout, 354, 415 \nconsiderations, 369-371 \nalternatives, 370 \nlimitations, 369 \nexample: Python code documenter, 367-369 <br><br>distillation, 367-369 \nquantization, 369 \nproblem, 354-356 \nquality of response, 354 <br><br>references, 371 \nsolution, 356-367 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1e84331-fb39-4b25-99b3-fa29d70d90dc": {"__data__": {"id_": "f1e84331-fb39-4b25-99b3-fa29d70d90dc", "embedding": null, "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 175.62, "x1": 428.12, "y1": 422.22}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "00bf0a76-7f33-4cfe-85ec-c31b7820ef04", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 271.74, "y0": 294.42, "x1": 430.2, "y1": 422.22}]}, "hash": "383ef0fe93351d8205601130918eaae85392439701e1ad628ed3a7ad1d162d8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9d96350-2974-4dbf-b331-0363d0290cc7", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 225.04, "y1": 173.82}]}, "hash": "bbf745591f725413717443a3b570e173e35d9f6f580f8fc50cab71f9eef24239", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "397 <br><br>scaler (scaling parameter), 135 \nscreenshots of PDF pages into multimodal \nLLMs, 119 <br><br>self-bias of LLMs, 285 \nSelf-Check (Pattern 31), 426-436 <br><br>about, 426, 443 \nconsiderations, 436 \nexample: extracting values from receipts, \n433-435 <br><br>problem, 426-428 \nreferences, 436 \nsolution, 428-432 \nself-hosting LLMs, 6, 12 <br><br>distillation, 357-360 \nKL divergence, 359 \nquantization, 360-363 \nspeculative decoding, 364-367 \nsoftmax function for logit probabilities, 15  (see also Pattern 24: Small Language Model \n(SLM); Pattern 26: Inference Optimiza\u2010 \ntion) \nself-RAG, 185  temperature, 16 \nalternatives, 192 \nsparse-quantized representation (SPQR), 362 \nspeculative decoding \nsemantic caching, 374-376 \nsemantic chunking, 127 <br><br>about, 354, 356, 381 \nexplained, 364-366 \nPattern 24: Small Language Model, 364-367 \nhierarchical chunking, 133 \nRAPTOR, 133 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9d96350-2974-4dbf-b331-0363d0290cc7": {"__data__": {"id_": "a9d96350-2974-4dbf-b331-0363d0290cc7", "embedding": null, "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 225.04, "y1": 173.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f1e84331-fb39-4b25-99b3-fa29d70d90dc", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 175.62, "x1": 428.12, "y1": 422.22}]}, "hash": "36149bc4f3f7f1e96d95e366a2aee3e7895038e5e0f5e975f20dd726ffad6ed6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "022597c3-8657-4751-a0e6-0883abe39e55", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 429.97, "y1": 173.82}]}, "hash": "74213639ea97c5512ec78c9d4b7a65a0fbfdec533b5dd08d9d52a18bf5ad74de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tables, 130 \nSemantic Indexing (Pattern 7), 122-140 <br><br>about, 122, 206 \nconsiderations, 137-139 \nalternatives, 137 \nlimitations, 138 \nexample: indexing and querying tables, <br><br>135-137 \nproblem, 122 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "022597c3-8657-4751-a0e6-0883abe39e55": {"__data__": {"id_": "022597c3-8657-4751-a0e6-0883abe39e55", "embedding": null, "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 429.97, "y1": 173.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a9d96350-2974-4dbf-b331-0363d0290cc7", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 78.42, "x1": 225.04, "y1": 173.82}]}, "hash": "bbf745591f725413717443a3b570e173e35d9f6f580f8fc50cab71f9eef24239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f8b7b8e-0329-4352-b032-3f7d1df2d5f1", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.82, "x1": 387.48, "y1": 605.82}]}, "hash": "4303c4c6c397d75d99168e0c48bf103a5e6eff392d6e818cbf3fb43075c60178", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "method, 366 <br><br>Pattern 26: Inference Optimization, 384 \nSPQR (sparse-quantized representation), 362 \nstochastic responses, 31, 275, 284 \n(see also controlling content style; reliability \nimproved) <br><br>stop string, 39 \nstress analysis, 398 \nstructured outputs, 52, 62 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f8b7b8e-0329-4352-b032-3f7d1df2d5f1": {"__data__": {"id_": "6f8b7b8e-0329-4352-b032-3f7d1df2d5f1", "embedding": null, "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.82, "x1": 387.48, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "022597c3-8657-4751-a0e6-0883abe39e55", "node_type": "1", "metadata": {"bbox": [{"page": 505, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 429.97, "y1": 173.82}]}, "hash": "74213639ea97c5512ec78c9d4b7a65a0fbfdec533b5dd08d9d52a18bf5ad74de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8166057-ccf0-4b20-b999-57c2dba20a86", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 418.28, "y1": 325.02}]}, "hash": "fac4109903b4b2a8cac4d0f78a6e86f8cb20e00b1d03d0a32f4488e8c2e289c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(see also Grammar (Pattern 2)) \nLangGraph postprocessing, 62 \nStyle Transfer (Pattern 3), 63-74 <br><br>about, 63, 102 \nconsiderations, 73 \ncontext engineering, 74 \nexamples <br><br>images, 71-73 \nimages documentation online, 72 \ntext, 70 \nproblem, 63-66 \nreferences, 74 \nsolution, 66-73 <br><br>few-shot learning, 66 \nmodel fine-tuning, 68-73 \nstylebook adherence via Pattern 1: Logits Mask\u2010 \ning, 32 <br><br>summarizing a document via LLM, 133 \nsupervised fine-tuning (SFT), 9, 10 <br><br>hierarchical chunking, 133 \nRAPTOR, 133 \ntables, 130 \nTF (term frequency), 115 \nTF-IDF, 116 <br><br>post-training method, 23 \nTransformers library, 260 \nsupply chain example of Tree of Thoughts (Pat\u2010 \ntern 14), 233-238 \nsynonyms  BM25 modifying, 116 <br><br>lexicon, 131 \nsynonym expansion in prompt, 131  time to first token (TTFT), 391-393 \ntokenizers, 25 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8166057-ccf0-4b20-b999-57c2dba20a86": {"__data__": {"id_": "a8166057-ccf0-4b20-b999-57c2dba20a86", "embedding": null, "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 418.28, "y1": 325.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "6f8b7b8e-0329-4352-b032-3f7d1df2d5f1", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 326.82, "x1": 387.48, "y1": 605.82}]}, "hash": "4303c4c6c397d75d99168e0c48bf103a5e6eff392d6e818cbf3fb43075c60178", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3673543a-488a-4f89-8b6a-ce0e32bb2e66", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.29, "y1": 325.02}]}, "hash": "ea45867a10dccc940e67774ddded76c026717c28d9da2e29b585d80826d75e25", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "decoding token IDs, 38 \ntokens, 9 <br><br>grammar, 49 \nLLMs as next-token predictors, 9 \nlogits, 15-16 <br><br>beam search, 19 \nnucleus sampling, 18 \nsoftmax function for probabilities, 15 \ntemperature, 16 \ntop-K sampling, 18 \ntraining foundational models, 8 <br><br>tokens per second (TPS), 395 \nTool Calling (Pattern 21), 313-329 <br><br>about, 313, 351 \nabout the name, 316 \nconsiderations, 326-329 \nMCP limitations, 327 \nprompt injection, 328 \nreliability improved, 326 \nexample in OpenAI: booking a flight, \n316-321 <br><br>example: weather tool, 324-326 \ngeocoding tool called, 325 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3673543a-488a-4f89-8b6a-ce0e32bb2e66": {"__data__": {"id_": "3673543a-488a-4f89-8b6a-ce0e32bb2e66", "embedding": null, "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.29, "y1": 325.02}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a8166057-ccf0-4b20-b999-57c2dba20a86", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 259.5, "y0": 78.42, "x1": 418.28, "y1": 325.02}]}, "hash": "fac4109903b4b2a8cac4d0f78a6e86f8cb20e00b1d03d0a32f4488e8c2e289c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08bab5d7-5ba9-4329-a8e4-b69e1df08e93", "node_type": "1", "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.02, "x1": 243.13, "y1": 605.82}]}, "hash": "3f51d56dd57676bb545b69cb09d13c8dd602b0142c097ee6b6fb03585f2cf971", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "system prompt, 6 \nuser personalizing, 407 <br><br>**T**\nT (see temperature (T)) \ntables \nPattern 7: Semantic Indexing, 130 \nexample: indexing and querying tables, \n135-137 <br><br>PDF documents into RAG systems, 118 \nRAG systems, 130 \ntagging <br><br>metadata (see metadata) \ntemporal tagging, 144 <br><br>tail latency, 34 \ntax return into prompt for queries, 120 \ntechnical jargon (see jargon) \ntemperature (T), 16-18 <br><br>changing to generate different versions, 93 \ncode for varying, 17 \nTemplate Generation (Pattern 29), 417-422 <br><br>about, 443 \nconsiderations, 422 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08bab5d7-5ba9-4329-a8e4-b69e1df08e93": {"__data__": {"id_": "08bab5d7-5ba9-4329-a8e4-b69e1df08e93", "embedding": null, "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.02, "x1": 243.13, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "3673543a-488a-4f89-8b6a-ce0e32bb2e66", "node_type": "1", "metadata": {"bbox": [{"page": 506, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 74.72, "x1": 237.29, "y1": 325.02}]}, "hash": "ea45867a10dccc940e67774ddded76c026717c28d9da2e29b585d80826d75e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7596ca2-24d0-4d33-aef8-f5e3fc60d4a7", "node_type": "1", "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 425.28, "y1": 292.62}]}, "hash": "3afa40f25c1aed2872ab81d14d8c8a86e9ddde326f40b4a4221e31a957f72926", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MCP client, 325 \nMCP server, 325 <br><br>problem, 313 \nreferences, 329 \nsolution, 314 <br><br>function calling in OpenAI, 316 \nhow tool calling works, 314-316 \nLangGraph and MCP, 321-324 \nsupport for tool calling in LLMs, 319 <br><br>top-K sampling, 18 \ntop-P sampling, 18 \nTPS (tokens per second), 395 \ntraining <br><br>costs of model training, 105 \ncreating a foundational model, 7-10 \nchain-of-thought reasoning, 10 \ncold start, 9 \nnext-token predictors, 9 \npretraining stage, 9 \npure reinforcement learning, 9 \nreinforcement learning stage, 9 \nrejection sampling, 9 \nsupervised fine-tuning, 9 \ntrained on tokens, 8 \ndatasets for training <br><br>foundational model training, 108 \ninstruction-tuning dataset, 257, 262-265, \n266-270 <br><br>instruction-tuning dataset size, 272 \nPattern 15: Adapter Tuning, 243, 251 \nPattern 16: Evol-Instruct, 257, 262-265, \n266-270, 272 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7596ca2-24d0-4d33-aef8-f5e3fc60d4a7": {"__data__": {"id_": "a7596ca2-24d0-4d33-aef8-f5e3fc60d4a7", "embedding": null, "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 425.28, "y1": 292.62}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "08bab5d7-5ba9-4329-a8e4-b69e1df08e93", "node_type": "1", "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 262.02, "x1": 243.13, "y1": 605.82}]}, "hash": "3f51d56dd57676bb545b69cb09d13c8dd602b0142c097ee6b6fb03585f2cf971", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaa16abc-5911-4267-b86b-6e70cc18a254", "node_type": "1", "metadata": {"bbox": [{"page": 508, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 243.82, "x1": 236.83, "y1": 605.82}]}, "hash": "962ce3da3fcc82e6e4117f071c607c5567ad33f412fbcbe6a4b68225e78cd750", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "example: supply chain optimization, <br><br>233-238 \nexample online, 236 \nPattern 24: Small Language Model script  problem, 227 \nto generate, 368  random sentences example online, 227 \nPattern 24: Small Language Model distil\u2010 \nreferences, 240 \nsolution, 228-233  lation, 368 <br><br>personal-style emails, 75, 79 \npreference tuning dataset, 86-92, 95 <br><br>beam search, 229, 231 \npath evaluation, 229, 230 \nsummary generation, 232 \nthought generation, 229-230 <br><br>direct preference optimization, 92 \nfew-shot learning, 21 \nconsiderations, 73 \nPattern 3: Style Transfer, 63, 66 \nPattern 15: Adapter Tuning, 247-249 \npost-training with foundational models, <br><br>Trustworthy Generation (Pattern 11), 173-193 <br><br>about, 173, 206 \nconsiderations, 192 \nexample: adding citations, 188 \nproblem, 173 \nreferences, 193 \nsolution, 165-188 <br><br>22-27 \ndataset structure, 24 \ndetermining if post-training possible, 24 \nmethods, 22 \nPattern 15: Adapter Tuning, 241-249 \ncitations, 175-178 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eaa16abc-5911-4267-b86b-6e70cc18a254": {"__data__": {"id_": "eaa16abc-5911-4267-b86b-6e70cc18a254", "embedding": null, "metadata": {"bbox": [{"page": 508, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 243.82, "x1": 236.83, "y1": 605.82}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a7596ca2-24d0-4d33-aef8-f5e3fc60d4a7", "node_type": "1", "metadata": {"bbox": [{"page": 507, "page_height": 661.5, "page_width": 504.0, "x0": 84.24, "y0": 78.42, "x1": 425.28, "y1": 292.62}]}, "hash": "3afa40f25c1aed2872ab81d14d8c8a86e9ddde326f40b4a4221e31a957f72926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ebe486c-e742-4675-9c48-88bb489cea4c", "node_type": "1", "metadata": {"bbox": [{"page": 509, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.01, "x1": 432.01, "y1": 597.89}]}, "hash": "eeccdbb804440f1dc3080b6e0a46a153408f4d9b8488ac8081da7b0769ce042d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "corrective retrieval-augmented genera\u2010 \ntion, 182-184 <br><br>guardrails, 179 \nhuman feedback, 181 \nobservability, 181 \nout-of-domain detection, 174 \nself-RAG, 185 \nuser interface design, 186-188 <br><br>try-and-try-again antipattern, 34 \nTTFT (time to first token), 391-393 <br><br>**U**\nUnsloth.ai for open-weight LLM fine-tuning \nand training, 25 \nuser interface design for trustworthiness, \n186-188 \nuser profile updated based on user messages, \n408 \nuser prompt, 6 <br><br>**V**\nvector databases <br><br>auto-CoT, 218 \nfiltering, 144 \nvector stores, 123, 137 \nhybrid searches, 157 <br><br>vectors in semantic embedding, 123 \nviewpoint differences handled by HyDE, 155 \nVinyals, Oriol, 357 \nvirtual environments for Python, 447 \nVlissides, John, 1 \nvLLM \nprefix caching, 377 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ebe486c-e742-4675-9c48-88bb489cea4c": {"__data__": {"id_": "5ebe486c-e742-4675-9c48-88bb489cea4c", "embedding": null, "metadata": {"bbox": [{"page": 509, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.01, "x1": 432.01, "y1": 597.89}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "eaa16abc-5911-4267-b86b-6e70cc18a254", "node_type": "1", "metadata": {"bbox": [{"page": 508, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 243.82, "x1": 236.83, "y1": 605.82}]}, "hash": "962ce3da3fcc82e6e4117f071c607c5567ad33f412fbcbe6a4b68225e78cd750", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf93cb03-b934-4cab-a5b3-49ebba78f6c3", "node_type": "1", "metadata": {"bbox": [{"page": 509, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 220.81, "x1": 432.01, "y1": 312.91}]}, "hash": "4fed9c90fcd949be3b84c179d4a8279a93208e33c1eac8da1ab543b13b52a429", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**About the Authors**<br><br>**Valliappa (Lak) Lakshmanan**works closely with management teams across a range \nof enterprises to help them employ data and AI-driven innovation to grow their \nbusinesses. Previously, he was the Director for Data Analytics and AI Solutions on \nGoogle Cloud and a Research Scientist at NOAA. He co-founded Google\u2019s Advanced \nSolutions Lab and is the author of several O\u2019Reilly books and Coursera courses. \nHe was elected a Fellow of the American Meteorological Society (the highest honor \noffered by the AMS) for pioneering machine learning algorithms in severe weather \nprediction. <br><br>**Hannes Hapke**is a Senior Machine Learning Engineer at Digits, and has co-authored \nmultiple machine learning publications, including the book Building Machine Learn\u2010 \ning Pipelines and Machine Learning Production Systems by O\u2019Reilly Media. He has \nalso presented state-of-the-art ML work at conferences like ODSC or O\u00e2Reilly\u00e2s \nTensorFlow World and is an active contributor to TensorFlow\u2019s TFX Addons project. \nHannes is passionate about machine learning engineering and production machine \nlearning use cases using the latest machine learning developments. <br><br>**Colophon**<br><br>The animal on the cover of Generative AI Design Patterns is the blue pitta (Hydrornis \ncyaneus). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf93cb03-b934-4cab-a5b3-49ebba78f6c3": {"__data__": {"id_": "cf93cb03-b934-4cab-a5b3-49ebba78f6c3", "embedding": null, "metadata": {"bbox": [{"page": 509, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 220.81, "x1": 432.01, "y1": 312.91}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5ebe486c-e742-4675-9c48-88bb489cea4c", "node_type": "1", "metadata": {"bbox": [{"page": 509, "page_height": 661.5, "page_width": 504.0, "x0": 72.0, "y0": 321.01, "x1": 432.01, "y1": 597.89}]}, "hash": "eeccdbb804440f1dc3080b6e0a46a153408f4d9b8488ac8081da7b0769ce042d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c99bc1bc-8f00-484e-8cbc-9a17c1b2486f", "node_type": "4", "metadata": {"file_name": "book.pdf", "file_size": 3465913, "creation_date": "2025-09-13", "last_modified_date": "2025-09-13"}, "hash": "76971c87e2638fc53cf88b067ca9e696a3b1256254a0bad5832b40a26baa3291", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Many of the animals on O\u2019Reilly covers are endangered; all of them are important to \nthe world. <br><br>The cover illustration is by Monica Kamsvaag, based on an antique line engraving \nfrom Lydekker\u2019s Royal Natural History. The series design is by Edie Freedman, Ellie \nVolckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and \nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad \nCondensed; and the code font is Dalton Maag\u2019s Ubuntu Mono. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"51134260-f449-4ed2-97de-69e94473ecd8": {"doc_hash": "c01cfa9cbbeae3a030aa5280332601c69ba9233b67f6f03ae85aa5dffb439635"}, "4684e8ad-e933-467c-a7c4-241d7aeb6107": {"doc_hash": "d675ae7d0005ca977ba18f50d2685aa12e6c04d2bfddb5cba217c0b5f6dabc2b"}, "adb2b9a7-3a78-4cea-9b52-e04c0ac56deb": {"doc_hash": "3884f651012da37637139ae74e1a03f5430a40f004bedbdafc95ec27d8f7fa66"}, "84811740-609e-4e67-a8cf-f9a0b8cf222f": {"doc_hash": "aa9e07ac5d24d4d519233c8287d7ad9a33a08c61900ad91ad0c71d2dedcda6d1"}, "d00f936d-0787-452b-97a8-033ca4b18aac": {"doc_hash": "5a524eb00500a5d6eecf0d6ec180b2eb0d60e02ace2f3c2df6cd6cae279af5c0"}, "74d24e36-9bff-4138-a41c-6c152df3b6c7": {"doc_hash": "99da2dc473ad1e4111e588796511cba8941ded8674e89a0f3fa09bf1cb48b599"}, "e016ee1f-2079-49df-95c3-fcc27459bf64": {"doc_hash": "c083c193aa3bcf3437fee2d29dfa16c5ef2810ef95707f018133ebb7b2dedc5c"}, "585f4c7f-7b87-4b8a-96a5-fc66c3faa5df": {"doc_hash": "e8c812c6a8fc4fa51911b1e6a80eedc808b71d65ac56c2594e4579b7319c7b40"}, "2e24c3fa-1b1b-41b1-9180-cbe6daacfda1": {"doc_hash": "172f51753c3b9ab8a9b6856a2d0812ff0a3d83294e1d93161d41f0be4e90b9bd"}, "7ce67e5e-7055-4ce5-ad9c-62c60ac6c8db": {"doc_hash": "a0510f2d2952edb2f42e0fcb73f691764a4af0c7cc07c9fbb61e090eac0b9ab4"}, "e29c259c-42c7-41e7-a3d9-2920996831c0": {"doc_hash": "9d214566da0c8a2f82ea30a0807133d19276c247a8fa2610c10e164015e33a67"}, "048107fe-3278-42dd-b341-46c1f5898f80": {"doc_hash": "26b732cf6b063bff64cb18b0724926bc0bfd7a9a2dca5aeed8876ac5bdc799a8"}, "dce5e87f-9ea9-42a0-8ad9-1ca57b733ac1": {"doc_hash": "47c7a11b5e96db6f433e72e2c016fdd17f3f36a3f2b386d7a10199ab45be9aeb"}, "b4fd7240-62f0-476f-9ef8-bc862c402793": {"doc_hash": "df6a7f2e8b4fdb5bf7d9bc7c8100f673c827e9c7e769b6d832c75af0f943bd0c"}, "3813dab6-431b-4d37-b880-8f3fa0e2e3d4": {"doc_hash": "e4cb32901b8d8b378dc7481063357c384b582d87e1feee7ace81ccebd9c8a701"}, "44eaabf6-eb25-4eea-b477-c0ea9c1d2ad3": {"doc_hash": "1e9b9c1fa913f4b632218d3c88f69ac9408a50c7c4451b9a65be40009756d1bc"}, "0ac0cfa6-fe14-4c5f-a1fa-790e87292704": {"doc_hash": "393b4691224c91dab38d54a0f4d1f6f9f0335dd76ed0c11d4cf396e4ccee6fcc"}, "33ccd756-67eb-436c-a486-c08b1b2bc91c": {"doc_hash": "6a3489e715e4728bad97f12d246da7143eb189a1da3f3cfc2a4dcf0f29893af5"}, "7236fbbf-1e02-478e-8d6c-9adced49a440": {"doc_hash": "70a6c68711f3c4ab068dfe9b53b293f3c9fdcb84b9a89356c7d263cfb19ac8f5"}, "72617f1b-140a-4fbd-8212-c0e203e0b435": {"doc_hash": "474848feb5aa7b77d4988753a77275c2f6b68c0628a53631a318039d9eb6a4fd"}, "4ee49f38-4964-4da1-a730-d68a265ff03c": {"doc_hash": "54105454adc25a38e69f6ab3ec79cd17e369208920f192a54d9ca362fbea79ef"}, "f481bfa8-75d7-43f0-b415-67765d675983": {"doc_hash": "e9b3589a913bf0b2732c199381150f69a4b25b8c31b559743d21a90980278848"}, "234330d9-b6f1-478a-8352-e91c23b2ea02": {"doc_hash": "df36170e776ba46b6003a3793948af046a7eac88cf9e953e81196d1efee98e23"}, "86444a78-bbd3-46f8-a11b-e134d6a991aa": {"doc_hash": "41a1bad4938bb415dcc949f01b7ced060acfd7d6926b714d2192bd0310fe7e6e"}, "3be108ad-c1c6-4d76-be06-63e7d08d4a38": {"doc_hash": "a0a821eb0360a795b2d82581a0686cd4155a474b9a5d56a214784a699dd78846"}, "9c80b39b-d1f7-4605-a25f-4696b8db75b4": {"doc_hash": "65bf1b25f48f968dcb6f42c60a39d1a3aea09f0b65cc1363b661db586442fadb"}, "3aa28892-7477-467f-8930-e60f8010b834": {"doc_hash": "3c05553cb09728524636aab39c79046302afdc20c665f0098e108043e0f5efc7"}, "2f2e94b8-23e9-4719-bf82-ef47b9f432f9": {"doc_hash": "cca1b28836beccc8b03557ae57c9f206151fc0587e3ef02a2e1db8c69a1f1b87"}, "978b6792-24fd-4711-9b4a-44522485f0a0": {"doc_hash": "d3de4c7d88aadb6a46abdac97d0d51dc666816293910a545e078ee167ef42e86"}, "3d91315d-f452-41c1-b1c6-005c26c2cfea": {"doc_hash": "5005e6cb245c3dab09c45ea4e983b46115e8609941eda901d509b982ed556c21"}, "311831c2-bb1b-4753-8106-55eab744b1d7": {"doc_hash": "f4b78ad94ec7fb604d408cccaa535a53fdfdedd4deac293458f541c6fc2fe86a"}, "add545b1-01b7-4bc6-9536-a56a69a454ba": {"doc_hash": "d1c8ed9dfc294bd7b7bc18d7d8f8013bc0f4fa6fc8b22c8a7dca5fc9240e608f"}, "6c7adbd5-f8e9-44e4-8088-a5327b3beb42": {"doc_hash": "2212255af97d9e252d5fc9a9393635f314305446b782aa15661b4d1d60c477fe"}, "e8fe4d35-5ad2-4091-bfe3-c02e50a93ae4": {"doc_hash": "30220c220a2ff5f347f8062642cce2f8200c98e9463a2bf65cee4dae2709a662"}, "c6de5837-0630-40af-94f4-36e3dbf36a92": {"doc_hash": "26e59c6f072c67550c270475840df24d642095fe46e16966f5ef7411631e20a8"}, "8a9c9a0d-4d95-4521-b061-31f090f6928b": {"doc_hash": "9972fd7cbdad35b08b8956ae88f27b08a0612012728c7badce9273b0487c9d6c"}, "0f48875e-c3ce-4479-9994-9b31e199b07b": {"doc_hash": "035852dc3ed2637c04d81bb1794e3ed2d72460facb86f7be002be5bff2fa3fc6"}, "3be20e68-b0c0-4d6d-aa23-eeeff38d022d": {"doc_hash": "304bcb5a377bae95ee37a287cd672c5fe4ba4192a175e1994e6b6f3bdfdc48cd"}, "b7b89987-82bd-46b3-9969-d6834f351ede": {"doc_hash": "831189668a7f4e08a60000e809bd14f8f166dfb307ca4e9c5e802a712754135a"}, "44b8cf19-76a2-45b1-90cb-c1c166e744fd": {"doc_hash": "2a82ba68416190e1234101aa457ce545a84d0204c39040c86d445e8a8ed1c8a3"}, "c143b398-b016-446d-99f6-79257491e67d": {"doc_hash": "ef373df9516fe717baf53402193b0896842f0bdeecf2234897080b83d2678858"}, "afafda04-73d3-479a-9eb0-fe492c698ecb": {"doc_hash": "1b7ca3d92867e95a59bf63d8b5822e06fc4fe806be348c18803df826ebf2d2f6"}, "aaa31981-0425-4c35-8808-e7c78fb7e156": {"doc_hash": "ee96bd4bba9da56447b207a3ae664f217f88fc6335144d5a3809c8d07b22fe98"}, "4a19b8fd-0cd0-4704-bba0-f33ff9aa03d2": {"doc_hash": "fea8df80b084d2826f946a06fda870f338486bc4fb1e403913f9db70d16617e7"}, "133e24ca-61da-4334-acf9-2563bbcbc4f8": {"doc_hash": "96ca8a1ed351c46885850d839f99556b60f463835941b95dec6bb5b9d8faf69a"}, "514e4efb-b681-4a00-95fb-db3fe73e3280": {"doc_hash": "58143293526db25a235320d6bf47e3b4bc0e94a693882e11c611854ac6099d31"}, "a96e250b-11ab-46e3-9504-25433a7b2078": {"doc_hash": "c3b6073787489bb40502ad3d01670aef44b163ce7f234a3d477de66b951ad244"}, "739d6db3-2418-440c-9bcb-292c80dd6b8c": {"doc_hash": "2ccf80a9ba88f272bb22e26607ed49b20116a9b648f88481121c891746e29086"}, "f4c8b5c8-3a56-459f-b797-e5999a6101e6": {"doc_hash": "9d9c3512b4da5f426d9904157b1ec92263bc6609d3f9ec2b60d5af6bea9787dd"}, "3e948882-d6f9-4b11-b39b-5a7783b720ad": {"doc_hash": "47938c87601151ee2b78f1eee7d877bf6ed44bc80b1723c9b868062ece94a76f"}, "52bdd0ee-8bad-4ffc-8280-2c3189fdf2e1": {"doc_hash": "a4e9f2b41a0de2e792edc3954b38f1843a0a5c154d26f7f77a31284a1981e2a9"}, "3d159a17-b922-4b01-bd3c-02a4bc759508": {"doc_hash": "d6773fec007587e79bdc02c7cb33ba51934973584107c7d03baab7520ac660b5"}, "80c9ae6a-1e57-43bc-a3de-6243aeb0d70b": {"doc_hash": "af1f2caa48bb6f01f111788bfbc40de220d2c02f406a232c83ead6b525a35739"}, "d367847e-0d01-46c8-bd7d-954a58449ab1": {"doc_hash": "9e30e41a6cadfbeb30248cdca89255c32a8d31f65ad96d3b67551c66d227ee6a"}, "12b62324-cb46-4bf8-9114-93ac641e9516": {"doc_hash": "2332ff3e2cbf5a2b0446b8abaa0bab055dd6c6df0d85f9140e1b4edf8ffc1e44"}, "41b9054a-a4a1-4801-93a3-7141934393be": {"doc_hash": "a6650b60839a760806788897fc6a60baf0ea0c385d3aadbcce37b9a4fd8e1df8"}, "23f6f294-2aaa-4a36-81a8-242e492a10fd": {"doc_hash": "08b43c7c460b61bac5adc93e7902f0b60de58a62d5f91526b88ede5b8c5ea338"}, "0a70cf9c-66b4-4ed5-b2e9-33c6d38eee1c": {"doc_hash": "cd16d54eeb303114dca82824c34f1debb718b6e4416c150d33d72338d1cb70f8"}, "c3ab1db0-d66c-40d8-850c-b532bfddfb24": {"doc_hash": "4cc00aa882b65f58e569f3fe70ff9b0d6e5fca6194ec40d51e4e3679bfa97090"}, "cc461b7f-401a-4503-9046-075f1b13c800": {"doc_hash": "016ad03d31d1963a1bd5f4920e030fb26c0dc497c4054d64ee48753205d1bfa9"}, "f03c0911-4af6-4302-8e10-415c9c08717e": {"doc_hash": "fa341b71ec359351d6d3165ffe0908b051e95cfb4f50c5821782a4b561fff554"}, "c6a64285-62be-463e-92ab-006a16a5415f": {"doc_hash": "46999ed822f4fa8cc86f299cd9d7e7e919edabeb8c37e164f07e1e5505d6cd9e"}, "dec98454-2c5f-4b53-9ea8-e43cc7451dfc": {"doc_hash": "065bb45ed064067d4c017a22713118e3028a81929751c3da45e8bf6c727c74ff"}, "1fff960e-e4e0-4df8-b333-6dc80d74dcbb": {"doc_hash": "2a39e08a94a11a14f7e08dfb50698dc6d98d851ac6695d7ac683bd4dae318615"}, "07bee01c-df9b-45c2-bd79-367b56b9b590": {"doc_hash": "79d1a0b7b17332092272f34d27a19845f3c98db26743fb5d60cb35460c170d1b"}, "d9637787-b857-4f52-95d5-1e2b43ab3a2b": {"doc_hash": "c1051f1f0917c9e72b5e0492b84305ce7553727fc1d4dc6788ff8a8bc41b0bae"}, "a347c8f4-7e61-4efc-86b5-80bdbf67cc83": {"doc_hash": "44a662a45cd9e4b025e762867acb251f7e8be522feaab04487b1d1edeeb1a8fa"}, "b45106d1-5c8c-4ead-b551-c2d5c4497e3a": {"doc_hash": "9b3917af9aae3776911028e8fe289238e2b624ddb70afc3bcdabb9124828c5fc"}, "edf2803a-29e2-4306-bf4d-d35035267def": {"doc_hash": "77a458fe145be676d5b4c826c9b6ccb795ded8e6cd00d4ea9e5cb867b99da9af"}, "b5930cb2-2006-4071-85a7-8ec95f710822": {"doc_hash": "f30b578c674b8643ecef111371121bc04d3e431d7f872224ae0434d8d02f14a4"}, "a0690aad-fa54-48a4-9203-2e63d25f3f56": {"doc_hash": "62aea580be49a72056c785b434468d3d74454d803ed3bfe19161d6630f994fed"}, "bf195232-2791-4107-8aaa-eb909e0029f1": {"doc_hash": "1e20bf5932e11d007a6e0fcaa8d9f169d21781aeee7ed1649c5d42678b77bf7a"}, "58efc4c5-6782-4760-966f-83eb59a7549c": {"doc_hash": "ce77e06545cc765bfe38c97aa8205d9c8394f78366a9db7b64a639a0f51f051f"}, "c4aa330a-1566-4f33-bca6-0c2fc4492327": {"doc_hash": "68e6caa92e2b11e9cfcda5549655edce2621e9ee433e34bed5ed24b12cdd531b"}, "2e1acde8-e0d2-4522-b122-f6a11f2bb184": {"doc_hash": "df7b838f798a77dad81f10384929ff956f82bd272006f47bb82244c30670359c"}, "71d29be8-0163-4119-8a93-ad750f09a4b2": {"doc_hash": "5007d7a9ddfed18f41106fc8145dd98bbeb4ee6a8f3ddc689a9d191823d43595"}, "3cb96e89-332a-4220-8a31-0b99178d1a2c": {"doc_hash": "92fbe924fdf0bdfb903f4c719776c670cb06ba798fd256998feed2f8ab6d2c65"}, "bb6eaaf1-a5cb-4a2b-9399-112ff28937e1": {"doc_hash": "5b70acdeef40eb5acda84b6c61505ea51c6d44dc85ac0960915f44ad250ce08b"}, "62478426-8e12-410a-b9b9-ca79d990eb3e": {"doc_hash": "ab18993e1340733c092ce1dcb4bbd4e969e083b0696789a1d89885262ea0a26b"}, "048f3745-8ae8-41b1-aa61-9b928c9b7717": {"doc_hash": "5562bdead6b83ef98f2c0b4b925b0b3a58a1c04b3193a1bd0715b1738960b3d6"}, "83f043ff-5932-448c-bfe1-dde1dab855c1": {"doc_hash": "43c7cdff5e13a03fa89cd2b6e6a290889988825a97acebb681d0a7f6fdad031b"}, "41c6c631-5c97-4609-8f65-0673ce5126d1": {"doc_hash": "70713c535d93312c6975f7c5ee7804b31dc6fda7bd6f64aac8ce1330384650c0"}, "da448aae-1a67-47d8-bc10-4e53abac7bdf": {"doc_hash": "c9c32e4aa866bacd179d79fd7c07fe35cd40f23c75624dfdc2a971781a879a35"}, "8fa0d55d-b2ec-4d4f-a4ab-002df04e51f6": {"doc_hash": "768a774aa75fa43e69b965ccb8d7b95fcb28742ace6508db0764fdc900076283"}, "6aa18a0f-0a11-431f-868c-0d2ddaaeffae": {"doc_hash": "46d63d3eddbeade4a6a8ff1b96bd209e7746b020a00c6ca558b72d465ba7101d"}, "f3ba60f0-8a70-4493-8753-6074700af988": {"doc_hash": "733765abed964c7cca29e7122be3a94c561e2b6a8c3082b32c94e1688ed1f796"}, "15cc0e04-f9af-4734-8ab8-3a9410c25f59": {"doc_hash": "9bab594344cf9b31a261de4bc0dc6abb2f3f25678af6f84bb305189a06609b93"}, "f7ad00ac-f2af-4d32-ab9a-9c10484babed": {"doc_hash": "2478f58fc75ed73b0019ef2718a0c2ae9e37779597bc374879c45dec4741b26a"}, "0867d141-9d3f-4c1d-a593-6539b7776f8d": {"doc_hash": "2d8deedd4edd01a9730a46d7d46b76851c0c0446c730e072c5970c6a732f2be4"}, "dc2c83c2-eb1d-4288-846d-c36e466830e8": {"doc_hash": "0f3e8572d5dbacd45b09ef9acd7c0ace4639920e01eca6405ad9e208c96ba814"}, "3008201e-df95-4390-8b75-c5f47500a460": {"doc_hash": "58c17d03ea5958f20650c11e960bde5eb26952d9fe5cc31f207967f3b488070e"}, "faec23ef-4523-436e-88d4-ab359d43721b": {"doc_hash": "3de92a7112ee428eb85702351f00ea3dce0b8781883222eb8cdaf76f00729fc5"}, "93cee7f8-9a47-4f46-8220-64eaf1a16c9d": {"doc_hash": "496230b1061d28e77e5feae98867b3053b2587534d3569e6acb9a81ccff917b9"}, "9569bafb-0029-408d-beb1-6942471d4387": {"doc_hash": "3486d2c61d8bbecf99b6323c21b5db9dfdf0b848bfc1bd8eb3fb9e5585cdd2bd"}, "19a0da94-ad15-4e9e-953b-ed0b016f47fc": {"doc_hash": "47e4f4217f5efc3909a5b4af0d07d1f5e7c6554d76eca961db828fcb64b4ad9a"}, "3e5a6d24-5167-47b9-92fe-673daafda8df": {"doc_hash": "29635c0a630ed7f2e0930c911a5816a09081e85fe33a1852c94068fbcbfb0af8"}, "4df85add-8ee4-4596-a602-a850bd5dcdb7": {"doc_hash": "c7a90a094d363d4b0ab4a523d81e966544fe9f3819d3416e6f0c7ab4eeecd12b"}, "cda2accd-db5d-495b-8dd5-994a90561bee": {"doc_hash": "9403f72e59252854cec5577afe7d755e43eecff6258e100492fc59b93402a937"}, "c7ef32ee-70fd-440a-b41c-da9209aa76fd": {"doc_hash": "ebb062808cad0425f7c6105df012a3273c27043c7614a30f384e733ae6aba86f"}, "6a53db9c-2191-498a-81a1-1dc8ac06d6bb": {"doc_hash": "d69546483403be1966a77d1030d38d637d5a347ee58073b0ddd0faf3a75d4701"}, "868ccb0e-b94b-4803-ab4b-deae42d35cdf": {"doc_hash": "70ad1f121cd23be03ac33b73f2675903849b006587affab1159cb63921671242"}, "3b7d00da-f76b-49d6-98c5-ca0fe7f2dd4b": {"doc_hash": "723b63ec65813f9db7c5887e6e71bf275d57ec89951a1fcc616fd3f9f7cb8445"}, "788d0fcc-c201-4a1e-bb3f-4d0f3d3757de": {"doc_hash": "a686f41af715ee03cece54ccba85365cc9d635a0ab938193a7bf32ed68e83156"}, "67d3617b-b79c-41c7-978e-ed896d62ccd2": {"doc_hash": "d6eafb4959390a58687b6e0279ff147500de504190eb1ac082395cfc9a548014"}, "dd4bdda6-4072-43a9-95c0-845d5898a7b6": {"doc_hash": "7b2ef8126da98d122292c99860622837f8f08e2d433d2bb59e2f766f171bffb9"}, "00506daa-8deb-4d03-9aa5-78e3d043e0f3": {"doc_hash": "1a2b1bb7652ca35f5b077903822245179a998fe803d60f600aa7febba849429c"}, "ee5dd382-a261-48db-8ebe-557b84e7345c": {"doc_hash": "06bbf57bbc931105d078c0d8b906c814d5d7814b105333cdc80c251a0f7142fa"}, "3edaad65-0949-49c5-a4cc-c39b19aecaa4": {"doc_hash": "0db7aae0b8b74a34adeee9fa278bb6125bfe502b221cf79b8c8f00cfa32e34f0"}, "917b5be9-4e8e-4990-8159-c20ccec54095": {"doc_hash": "d8fc7ba0fd74e3eae8926a55debcd9d8f988f221b7b2bd47a2bdfc63048df3c8"}, "79fcd7f9-3c4f-4987-8844-23a46b6e76fe": {"doc_hash": "23acea2f613a023c9fd20f2056f4880811944c32b03f42c989462b112643753d"}, "c09a772c-12ac-4ffe-847a-52f0ae58c353": {"doc_hash": "a5363378155bda7a3d07d82bd0fd2ee617ab2b052b5bbee7e9ef34d15a52b756"}, "10b3370d-544d-42ee-8290-bdb711cad978": {"doc_hash": "c68da43c42000d23e1b68e94c5a28d500abb49fce69f28838ab04f89af133a1f"}, "4963c118-1a02-455b-89f0-7eb9d077910f": {"doc_hash": "44100f0af90ee211dfb780b9067892bf72c2f97d93f3f5ee4595411588aca8aa"}, "b3d35803-f1c2-4dcb-adf0-b0affb69ecb4": {"doc_hash": "cb70ff3a622dfda3a171d3d0394bf6a5ee0a81a83a773ee59ab97e05f8aa15ef"}, "b963ba37-3438-448f-89cf-87093022a2fb": {"doc_hash": "59afeb4ce63d3b5d4ae2b852387731baffb56ffec50d614a633abf949cc54b1a"}, "3e66644d-f3d1-4b42-9fc2-a005bfab1663": {"doc_hash": "ac57cc4fca4d885aee757c793f32365cfe88e3b5f7124b89c622561d3d271ad3"}, "d35fac06-d906-4172-bb35-465861a433a1": {"doc_hash": "ba392a9532a927fe496264512ea26f3ffad6da58e1020739f5fccdd5307b8241"}, "5bba0ea9-ca07-4019-aa8a-25b5e7d5cf1a": {"doc_hash": "d4f356dc6e04ec7eea90d557f878aa290174de3287a1f5efd59435bee153dc94"}, "a2a789ac-b0e2-47e9-ac6c-da03ce62bd8b": {"doc_hash": "386f511829c9a6a2784f8b554322fe4cc1bf4450fd0f7b91995e3f8622eeaa46"}, "22bf4fca-3ef4-444a-a281-2c7d2825c1a0": {"doc_hash": "197c637653dfe39446d3a7b7809ecdb7d5d98e288576d24ae4e80bf93c10c775"}, "e873403b-3c8c-4084-a195-006e5315488b": {"doc_hash": "bfa36fd5be9b478c653b4a0c0d7eedd6ceb789e60578affb791a50c5afeaec6a"}, "2ba30efe-c50a-42b9-8f62-df14b8d36a5c": {"doc_hash": "421af6baf7d1ef895390bab81b3ffc96c3f8c175c5e3b669bb49ff2155801737"}, "e1bf1178-3ca0-494b-8f65-d94ff40ee14d": {"doc_hash": "d6427711197d9908a7341e14580891c0c03f8a33bbfc86e01e9253f807d595a2"}, "f6a8ef04-62d3-48f3-ae7d-6d0013de8220": {"doc_hash": "c002473aad5081641aa5f00e78077f3233b696ea6aaef88c4e0b893dc202332c"}, "d61712a8-e22e-454d-8ccc-67216043231e": {"doc_hash": "725cd14a7f1faa2d727dadf033582586dd1eaf04a3c2993c044e8096ff9b20c6"}, "b7f2d70e-40db-4f7a-b9fd-4996fdb0a846": {"doc_hash": "c02a9fce34a479b3e470b0d20465aa3b0e82f9e19ee3cd6deba497e5e77f642f"}, "88ee2470-b313-4a4e-8d10-985343910a21": {"doc_hash": "c623ea0399845c5f565abcc1423e76e08849a6208fbe7f08fc3d086a96c3dc51"}, "728d203b-8dfb-4e0c-a8c6-09cfa9762a19": {"doc_hash": "54452cd853adf5df81a651905e2df78b5504b30e7705f4626fefd7240a4cf22b"}, "ab9da156-7610-421e-b09e-f3c189968f56": {"doc_hash": "31e477519ad8213816a4ac6596779c963dc37d692dbbfcae237eb915da56d45e"}, "736993d7-9622-4a4a-81b6-ba09b1f574a8": {"doc_hash": "e7487c4cf21c7072cb3785b0ff64b917f0f985870ed2e7476af6b9a5820eb7fb"}, "b908fe54-350f-4c72-8308-d01cada0e168": {"doc_hash": "f5bc4cd44145ca197ee887da588431deb7f54d10c86c39bdd40eccdba257f0ac"}, "0be24095-b356-4576-af53-6b06b415ba96": {"doc_hash": "f5313f987220f90e7626f5f96d75acc59fb1cb9a5951969b615fa741743ac094"}, "e057da30-afe5-4db1-97d3-b006bf386052": {"doc_hash": "c340cfa5f2ce63a43588b4a50f05c25bd47782039ac94c653894ff7bbaaba5f0"}, "1275ecb4-a93f-4658-b474-451738bac950": {"doc_hash": "12fd404dbddc3337c796ea07bca06a76ec1aed49c17461d3e8e38fcd69625f5a"}, "8a505cd9-3b07-4027-b23e-99fa4e622d4c": {"doc_hash": "40da1111151d3f72f1a624d4b5f12ad275c2852f4b260a437e54d1687a5b154a"}, "56c071bf-e148-4e30-adec-0c99e42f65b7": {"doc_hash": "1732b31344fd4a5bb017f50a24d863ab333f124785cdf6bfb7a7715f9760ec85"}, "7bfb3d03-59a4-4171-8e95-fec483887533": {"doc_hash": "62e99c341f6fd49a7a5f6ea552c8f73d31a87f57d37770b36e33bca90173c4e4"}, "781f495a-d752-4383-9987-2d165972c5a8": {"doc_hash": "3a4a54ed8050b415f589165ee308edab349940d79e53e660def1e2001ca0b6a9"}, "eb64e334-d48a-42cc-97a7-8f31f56cac94": {"doc_hash": "55838d1a5551631e552d5619c2ed3fa99192130a62f053e2783ac0dd8eda5898"}, "df5244bc-a822-40aa-ad54-b3d66cb17415": {"doc_hash": "99d3f6985d09c8d97dc678604bd7c158283cdf653118497257f0cccdbbaad0a3"}, "46581f03-2256-49ce-b7c2-44004525add4": {"doc_hash": "e681da3106cd55a1eb92919126d356af2e28c52a0a152553eab61bb41394589b"}, "e2973c2d-b33a-4a58-ab4b-7d9373157e5e": {"doc_hash": "8cb1760f5d211b3400a0c67a53211f212fd86932320cdcfed6e94b36c0dd1586"}, "ef50a8d9-0cee-43a2-b0a3-f8dcd4e661a0": {"doc_hash": "ac709a452d4c5caef4ce2b0a5ea3112af05a1318ae0db4132bcc202612474d9a"}, "e6760a32-cec3-4fb9-829e-cb8960675992": {"doc_hash": "6b2a6b89bf573eb8c1f7376fd736025ef0e0b49290667df2e58062a0024e97ee"}, "9d4fe9f2-a942-4d1a-9e2d-406e6e1904f9": {"doc_hash": "67be640fd0920e2825defd9c3712307d4d65dba0f98f3a8596f0d5ff51718ac4"}, "46537374-ffaa-4477-8c10-ad766a1947c5": {"doc_hash": "da6afdf92f0efda6c5802003f498536e5813ebdcf3842050d62d30a536ca3d02"}, "b36dfaaa-0868-4393-86ff-264b8978d56c": {"doc_hash": "1075f5b20297ebd5a5deaf37d5a451986b6e4803efd02c039d686c51e001599c"}, "410d2fa3-4993-420c-9790-ebd2b90f7597": {"doc_hash": "a6fc472aeda7ef2158544de48648a39829b835477a030e2c38a2aae333e78526"}, "972f5b4b-dfa5-42d0-98b0-ae805faa68b0": {"doc_hash": "ae901f9baa6aa342bf00eef22d7612a96f0cd1e7b0d3fe3299f0ca84c94f24ea"}, "91f9cc84-8370-431b-8907-05ffe0352aec": {"doc_hash": "c11020923f62725665dc34744a261bac9dbae7c53d565f7607a918857babd7fa"}, "c527133b-9f43-4579-97fd-4e2755394148": {"doc_hash": "a23a48b5acada6c00701efef801fcac6384cc8c699688018f48515cf60558090"}, "c7629348-8104-40a1-b1c2-66786e45f4b2": {"doc_hash": "cbbd01db46d7435911b77ca0645b332bd303d14fa453dbdca576862ddd17a1aa"}, "4965ae00-7b58-40ab-a04c-a2623ea21675": {"doc_hash": "48aa8136f34434149897314b986f9ff78dd67a6278e234e887e9ce9b16a4156a"}, "3447e428-cf3e-4fae-a5ea-2217c5141c4c": {"doc_hash": "76a83c21fb523789850830f014842f1b69ebc2b5edd52e29abd2dbaa5f207825"}, "a3136f44-1227-4cca-8dcf-dec55eb6347e": {"doc_hash": "26fb548d07fd633a5e0376d129ab8f8874ed760f4a61bcd017f886c16f17feed"}, "c6e87c30-9065-4c1d-a37f-26a7e024c55b": {"doc_hash": "4b72d0b3e4ea6fd8aba9f1d5dddde187f30ea4a36c6a8f7ca1aab1eae20baee0"}, "94a243f9-0d3b-4ada-99ed-df3ef518ddce": {"doc_hash": "84dd3e7ba380769f88079f63a1afafdf80919c98f20f4dc2322800772b35a791"}, "fd238d85-0900-4fe3-90d1-895aa7480eaa": {"doc_hash": "25351bdba51af76b946d4e83e25b203b6ffccc371a9d90ac3a5fe3c880345b58"}, "dd9c54f6-ac14-4d5a-82b1-06857bdc3c08": {"doc_hash": "0f2bb0ec7b60059a95c83d62328614c576cb5459ca473a5c6f77bdee9b6471e4"}, "35477b1d-1560-4dd5-b094-48c9242debe5": {"doc_hash": "9e77573d716743e99809a29038fd418ec5c1eb8d68f30fd3b958252175323f62"}, "852d8996-98c6-465f-ac51-c89f5b3e6afa": {"doc_hash": "88a7936c87318dc24b40d6d76c0a4642e1f2e18dba714ea0c20dc639a0543189"}, "90c57d9e-16f2-499c-819b-686482af5725": {"doc_hash": "9ceb97c7915b8453e334797b6b56e5baee65590eddf5a03da09b2c76f70d82c3"}, "9a74d0b8-1826-43f0-a5e0-b0f87f4efee3": {"doc_hash": "140ab6d6282384747639bdd935b7f9ea499574332f53a9fa3824decca5cbe5e3"}, "945cdc6b-7980-4270-ab1d-201822740b26": {"doc_hash": "0abeeafa26fe655d5385077c7cd566e4dfd8d99143fbe5af060739689055b30b"}, "1a32575f-0055-4934-baa6-7b25a6f50b5e": {"doc_hash": "89210f345c2a9f66754d228690b7d47793c2effddca45869fcbdfcb552029e0a"}, "57f7f572-852a-4d4d-88ef-a7f74d854a28": {"doc_hash": "faba66154d94cac6a484ecf318d584382301aad7be0b3acada90fc3114f6db8c"}, "2a161451-7a2b-4465-b5ae-8b9a15bef626": {"doc_hash": "33c5693befc20304dfa06d82aa84b7ed0940ea5953f31866e7f606e3ca7efce2"}, "94d1d4b2-949d-4459-85c3-d1c4c21329ed": {"doc_hash": "598b680275ec10154084441938e45ed9c0a46ee82779808b7ee5f22d74591d85"}, "5ed3308c-8ca8-4014-b467-b291fe445981": {"doc_hash": "726266caa40153d70851197c4d62e9c4d24ba5f0445030261845b6b9edac5813"}, "17089537-2dd1-437c-b2ec-bf606a344ace": {"doc_hash": "68109e9788f10d9b5886bf870f56ca6258e9a267873031c97a2dc5c4e57bafe2"}, "fc6da2ac-f693-4dd7-8093-2feba2d5ba00": {"doc_hash": "2f153eaab81db9df2e86b99fa59e24cbd49f699a95949f17d7b7b515a4623dae"}, "0138b286-6ae8-4638-b3be-8f80897904dc": {"doc_hash": "a307f8c355cf2219d48c3ecc89ee39b65a978821dcb18bbafc13e0d726972d22"}, "9db786c1-7f44-4d38-adb2-5025c3408a5f": {"doc_hash": "6838abe8000da233b355cccd8ad469fdd8723a14973f5f3f7daf57c527117382"}, "220d9f8c-358c-414e-b5e7-5c90be6a4978": {"doc_hash": "38f722d6f21d0daa45f588bffb04e68f1777974a9f198d228733ecfa48e00148"}, "bde8ec2d-732d-44fb-bb2b-69a4ab8bef65": {"doc_hash": "e0d510a4bf5f2c5d4b9eeaf27baa5714323eb75f2f8e7364396d2195baf01189"}, "762f678c-2396-409c-b812-a0133f647080": {"doc_hash": "a66386534ede6fca69a0e769b28c3e9029c6a924ece199f59dc708223952d00a"}, "1ddcd4f9-1413-44d5-b2a0-25a476896701": {"doc_hash": "4ff02161c8c4da04e1b03b1a5105c91bf5194fb98615c2213ffbedbe545fc7b4"}, "0bd1ff87-d0cd-4c1a-894a-649b787d8d0b": {"doc_hash": "3fad124b51789a62f182630d39d0fd6600d0eabaeb71e923a5be1966d071e102"}, "50896a58-ea65-4fb7-b26b-f7fa3ffb0801": {"doc_hash": "045de7993bcc0ce339c7c0b34f1297457aedb8a2f1be233f6f974f87aaa45575"}, "13c8e459-b0f9-4c66-bcfa-7a76a5090bf1": {"doc_hash": "1507dd35b2427d237166cbb40c2e707ccccaa28bf1cb23a27de2ef9ecc774d94"}, "854b0a35-d101-4c22-b6aa-44a9feb47bcc": {"doc_hash": "5e5db5b5b7d3e519994a736d5f03531e3dce0062afe16b4dc69053e670383faa"}, "2a865682-26ff-45b3-ade3-c6c1ff08a8c2": {"doc_hash": "0695be9e1912e2a0d91aa791ecd4a8b2f30993beaa7b9da6ffb4d0515a0d91c9"}, "5cf603d0-03b6-4735-9528-a1930bf6d404": {"doc_hash": "4f626ff9264ea316228643040f7e854f13b09fe886ad6cc56555848f378db354"}, "40b09d16-0067-4608-a0e8-4b7afe9b8432": {"doc_hash": "e86319ea0778ef9c3b21cd067d138b89ae1e9094d6aedee44448132dc38be928"}, "a747d8ec-47ef-42fe-b828-8fce713bd15c": {"doc_hash": "e8fa9c28ea752cff1c736dea4a1c93390731321a5aac37d8b23ac4545c92d761"}, "adfddc27-2db8-48e8-89d7-51399d47bbe2": {"doc_hash": "9104b3e6be871f48825321540881c664c9ca07a6400bb71e2da2d1beceb39710"}, "4289e806-c93a-4d61-91a6-aad69899c868": {"doc_hash": "a48b1f16eb52bca8e42612c05d7595ecf1f825469a307cb445b50457025491cc"}, "5c7d7afd-5a7b-48c5-ab3c-c925372cfede": {"doc_hash": "da4c906f4725123b87ce39a16407b491c163aa6a292a007aa9327a64d698dc9c"}, "8d09a041-3686-4cbb-a4f8-d54cbaad88ad": {"doc_hash": "4cb11c1b1c05246b03aeec3cecd728e00567c292cfff73e425a9756797500de4"}, "c63ff9e2-3d0c-49d3-928a-305929bcca17": {"doc_hash": "198eb36f463c345c9586814453fdf871973ac41ba300cf89ab4520e5c0baaea9"}, "fd3aaca5-e2ab-440a-9b43-07097cac6d17": {"doc_hash": "418965d001d0df562e41de2c4d47c732a51fc8b0acd2643eea89809af36f5d09"}, "94c1a00f-56f5-4c6f-836a-8ff080fc0517": {"doc_hash": "a84b8814cd720306e8cb6f9254843091619d83d281c5c7dc2395056a62d7f61a"}, "172524da-5ec4-474e-9f47-bbb4e856e49a": {"doc_hash": "9bd0ae64516219804940419518cb63620ed5702e0a0dafd3d9b5f30418e4a8e8"}, "ae310ad9-2bfe-4392-863b-81f307b9605d": {"doc_hash": "db280127de5a80b8318686a37140d243cef1c88ac2a2cc7ef003b26cb62ee8bc"}, "d3a96d31-d0bc-47bb-a47a-b9cf93cbaa75": {"doc_hash": "5dfe498817124b35c9e1cf16436e3b868c02d4878e6aa515341434ee655728bf"}, "8685acb9-9433-4fac-b3e5-ad8edfaf158d": {"doc_hash": "1d8106ab53c5556fabefbf3270caf9a32e89e96bcbe617300e677663679bd1ee"}, "077d3865-4b3a-40a1-96f3-f712713327fa": {"doc_hash": "008c9d2b577136fef9713d7c4670a8b71fd1facc9476610b0e68687d2043bce5"}, "27459fde-b1b2-447d-b819-b2b23a9306c1": {"doc_hash": "877fcb717840a596348e0742ddd0d61fc25c9d57d20430d54452337779de3e75"}, "0d857a3b-1a70-4694-8d3d-6ad70d63caf3": {"doc_hash": "e05dafcf5caee4c4718e9d27b73db87912613ac660ae4cb8691c0ff2088d2423"}, "5e8f9156-ae2e-41d0-9480-fdb02230c6f9": {"doc_hash": "e9b0cffc52f4e7c7a381943beec74c0ecad5720efd7f24937e434185e4ea26db"}, "86a1ef50-bbb2-4176-874f-1b929cca61e3": {"doc_hash": "1fb89b4b7512a7a7996cb6720e4cc419aa4b38b281eb772c5594a7da4c5020e5"}, "04d95066-0641-4d32-b0a5-8feed4471b2d": {"doc_hash": "16e49eee10fb26a8201551d01a02cbc1d1c1ede7a430e5e8e95028a8cc8b576c"}, "355e8c96-8f05-4083-b62c-2f5dc03418c1": {"doc_hash": "bcb21b0d3e38767605b9dfbef22f9594b0385c19a594dfc1bdcfb2e1b2d3463e"}, "4cf9ddfc-db42-4711-be78-08d746d8cab9": {"doc_hash": "364edd6b54b28388953389012abf97700ffc29bb18fc025c28c8239ce5fcf284"}, "9933f1ce-e945-4647-87f9-ab8e290a0a6c": {"doc_hash": "82112e48f558e38635743ff7b1ab528dc4e59065e0515ca44804b8b125f01e60"}, "b4202199-f228-492a-b333-659bb23290cd": {"doc_hash": "9741465a51b4043861c1ffba3d71f73db1dbf8444b85c92bfecfeac9b1a9e055"}, "9a2eab7b-dcc8-4192-9905-71a4fad24cc2": {"doc_hash": "84783c0756f4df04d28b363ddeaf2fcf2f6ba2b6208087e93ad78e943c447ee6"}, "e2b61d9b-68f3-44b3-a0ee-031dbc2de038": {"doc_hash": "78c5587013a065565989cec48bc550ae0434c593c48edf595e00ae174fd00126"}, "e619d21e-904b-407b-96ec-c682035aa7bd": {"doc_hash": "3645cb0a52e80c8198c8a6fd71ed220b787417ec63f2c0a4285c4e1eea316b70"}, "b7475945-cf92-48fb-af99-b8869ad225d7": {"doc_hash": "a1fce6b1927c4f110faa21003b7d8974bbaaf59dd77548809b0c394bf2f111a7"}, "9f015568-5076-46d4-bf6c-dcdd4fe34083": {"doc_hash": "73153977c5c7cf9e69b3a8372ef307f36d4a65dfd553b07472137ad5536f64d7"}, "0038430a-2909-42db-bdb2-91e57ae0a8e0": {"doc_hash": "cf2e13dabe015d226347464951dc273ea1d0032db0558aad27ff31d3ec3b0664"}, "14070fd8-f8b9-49ef-9123-3cdc3fe6f18a": {"doc_hash": "1de0c877df9216b586e7af9e75ac4152fef1eb8b4977e264b36995b59f9043f1"}, "a63262c4-e2dc-4cd1-b54d-44786baaf38d": {"doc_hash": "877fa451a0dc67be2f25c756e98a73be942336628349d143a2ded102b5395d2e"}, "ac0d2846-201f-4b2e-b97e-124493b5f2f4": {"doc_hash": "c936a7500f64a0506016c219208495256f9946dd82fa027a52659427de5b1c73"}, "7841c3f1-44b0-4c32-994f-d84c6b161c5d": {"doc_hash": "387defb5557b650e3cd7cf081eeb67a1a3f09c3d41628905aecf75a3d6f442a1"}, "14e6a16b-dd1d-45d1-8fda-e0e0c955cdad": {"doc_hash": "e19d0d0cd07b81f72f405ff80cfcdad46fdb8b91c39da8f90a1fee69844db60f"}, "ce8f04ed-2802-455c-b20a-bba93a1a8ca1": {"doc_hash": "da76efade6e0c401c91d25ce553d3a7e10577c2a690006dbb705bc1b36804f07"}, "c4c58635-b805-4951-b8e5-d0880c4765c1": {"doc_hash": "ed6f9bdab67490f9251e6637c150a70430c27da667159bd8e47d3f3cf2e41e9f"}, "cff4e886-1745-4fbd-9709-a36f809f02bb": {"doc_hash": "ad1ce5fe99e65f739e7befe1065c441c5d2c94d16c10aa760d0847a41506b2de"}, "57eb5f31-cec4-41f3-80a8-c0dd2685e16f": {"doc_hash": "7e4b3e091d1829788ef61a2deef02f3caf18c4f1a5a26ff4e87f150bdd23c4b4"}, "5dd5ec7d-a5fd-4dbd-8f00-b325316eab5c": {"doc_hash": "0f41376f9df184bb0efe067b1c2c97f55034e918c9e1b738ac242339f42f1fc9"}, "1641e7d7-feb3-4e9e-8bf5-0d2d544282a5": {"doc_hash": "0e243f638d71096f3a89d2f67fb3ccb2d15044eb9af57afe5d9219cedf8d73b7"}, "7db68fbf-92f1-469d-abd8-876090769e98": {"doc_hash": "c454aa9ee0d41bd241f08ad19e8f3219d0ddb287573f627888ffd1547d1b4053"}, "e2f49a73-a41b-4cc0-94bc-43ca5e191c0e": {"doc_hash": "a824cf6e8a017284e08d861855cbff90b27e672b92755a30a561fbfffa20de3e"}, "a85bb2a1-01d2-46f9-ad3c-dc30b602f1f0": {"doc_hash": "655b7edec90368bee0f861f4fd68ea31f59802d7ed78e6851b426d166f1310e6"}, "7e7629e4-8afe-451a-89c7-393f3c2af179": {"doc_hash": "a13cfcbbccc64538fd0db65f577d41649231c9f146da5880649e03210ea48a05"}, "02992b45-84ef-4cfc-a7b4-d2a8a6a939ce": {"doc_hash": "aa2eccf4f1974095a3e647665f5ee5087131a55f56720fa6fc057192cba516c1"}, "60786c79-d6bb-482e-8805-5d3a2b2ce9b3": {"doc_hash": "977a76995a4990b6aff692a2c6310d5848d4e2f153436de1acaf02c780dfa0e0"}, "065e47e4-d960-4c68-8efa-5d5451f0f813": {"doc_hash": "c8f629b014da78aadd378e4c81db5e3c0250ea3164d21926b96b569543f99527"}, "70c661ea-4616-46df-97fe-87fc1dca73a3": {"doc_hash": "22d22f61b09e292157583de6042fc38f2c1e3a986f3e360df8a8fc3fc3c9d7de"}, "57406f0a-2af1-4f36-b03a-4c25e5f7435b": {"doc_hash": "4f83f7f1e6cc0b87086915a17368c0c78055fa0029c438c4eb705fac4df9919d"}, "2406dbb1-a8f1-4980-84a2-d94c24978c60": {"doc_hash": "6353e16ca364f1e19958604b9c208195c922a12f8ff010c74990952120ef134a"}, "1d1a7a89-887a-4225-9b8c-88871ca54ea0": {"doc_hash": "585e1e7bb32420850e8bd623f5e9f2457e70021c1e496b565c0bc3ae3a04b480"}, "75ec9b66-e6b8-417e-9d18-f40724f79acc": {"doc_hash": "fc5bed0d08de69c7ccb1d4acd9cb25847fa5f45480d14fd38e7fa1452d54dd0b"}, "df7b52d8-25a3-4f8f-9fdf-37a75a844c26": {"doc_hash": "57082d06d204bae652fb1a6aa63b23df6a4d5f13355eddba113b9416dc21b040"}, "67bc9c67-3466-4f5e-afb8-b7157eea5813": {"doc_hash": "160e789a6fdd8efe8123ea5877b83531312dfdad9afbd231355d7322abc0a880"}, "7a0d545f-5a0c-4ad9-a5e6-f7dc58b7b3c0": {"doc_hash": "9605fbc23becfeea00634741aa3000050dfd08c94db8afb1d230e8287d16ee82"}, "97ffb17a-eb78-4d15-9db5-982a8c34dd71": {"doc_hash": "05acd3c80a5d8f096de10133d12026d3c1f9239ae2bd65e27a8c9dc4d9c0a419"}, "22a098e8-2ed7-44ee-98a7-4e8f1b9befec": {"doc_hash": "8a354b45361d424ac6e4ad420f28b4162a02bd310e070da03f6e1d80335f55b2"}, "c3b8af68-5d26-450a-85e5-19d1b5281914": {"doc_hash": "e8b4e06712b901f987cd042d8184c3c983c5ce5fd0977a8b4b96a1efbacf8e42"}, "ed5b3b98-d131-4aa6-a3e9-9732e5faf531": {"doc_hash": "464c28cb3159c99df86fcb05c6c4e24971665ce8422617b059249c055e71e0c3"}, "5f22c7fc-99a3-475b-8b95-da4db7e46a60": {"doc_hash": "c626eee5f068c9ff3df08c7ac275ec8d6c3a5a5e9afbd7d37d6686790f9d53f9"}, "ff4df780-3288-41a6-b0dd-cd3aca9006b6": {"doc_hash": "3ad1da684375abd450c4d7b76c0a92dcfe3158f9c1855778f7593898aafe0665"}, "452e0739-b055-4c62-8616-a5b5522bfeff": {"doc_hash": "ddfe4da7d9016e2fa888038b1d7eac4785630a19bffe8c105accf2d713d800a0"}, "79d620e3-2d04-4185-b8e2-bf67f83e43c0": {"doc_hash": "96b9584ccf14a363e3705336905c99f731232b54b55da566f2b57e2156001174"}, "c4a7d7bb-1028-45c9-9138-161a50bf929a": {"doc_hash": "5c834e6b19371cadff0c7f4d1f9efff2c52efd06631640de3a48b83e6fc6ff9a"}, "d056a8af-24f3-4620-9035-dafb9cef11b7": {"doc_hash": "0a8b17b9ac122069350565a8e20d81cc681a0b97c2ea12d4e8455e5a31b7a1fb"}, "2d23736d-94dd-4c2b-ba5f-7f2bba878494": {"doc_hash": "fc48f05d1feb0b4dc2384943714d9bed16009517abfccd3c9f69cf0644086205"}, "79789cd0-501a-45cb-a33a-951d943c2dda": {"doc_hash": "07a1b88ad11aa903c1e62352925d76a534cd941d0fb6594c3b3d1fee2fa06e35"}, "309a46fc-8551-4566-9d88-4e4692b5db37": {"doc_hash": "7b00074c2c6e566f788513b1cc3643811f1fe64c731e7e2e4395262c1c0768bd"}, "cfe2bce9-f5bf-4c3c-9658-3f64e3fed016": {"doc_hash": "64f3210a8d03455a477b6fe11cd04877127e32ccdaaae523ab85af76e68ab458"}, "28e75883-e6ce-47da-afdd-74c3505b7b86": {"doc_hash": "cbbd668fc463f7aae95c4dd78b6a24beeb5a84346d8ea05af2a1de56c2ca099e"}, "af10fc44-b6ac-4fa7-8046-55c382575c1b": {"doc_hash": "4453df0ee2387a1f9be4463946fbfb5776672d9bfaee0cdbcf3ee52fd7838973"}, "51b830db-6831-4867-a45a-5084e8ee1892": {"doc_hash": "9db8f4d4262c8b09a2300a8da4edd3f930d433d2b4d97e7420acf84b7414d29e"}, "da7d7b5c-545e-47f0-8409-f70075169a27": {"doc_hash": "d31e6de41d50d350132909fda92aef7ad5173da0175c4e8cf41870c75964672d"}, "5dc6344c-f7bd-4c7e-a2ba-ad698c813263": {"doc_hash": "245943b4df46416684783556fc972e03f95963af98f97477868aed84c0c99c34"}, "aac59e3b-3be8-4fcd-ac8b-0c6de5f424a3": {"doc_hash": "727a302dd8162152475b04f8fd5e3fab1fd3a55a94fecdf5ec4782074bd6e70f"}, "e099e39b-7a86-4642-83ea-5ca3dc4833b5": {"doc_hash": "268c89e84db893cdf4cf67df740caf9564fc598368dec5beecbe25849ffd14b3"}, "68f74068-b909-4195-9449-28ef14636f5c": {"doc_hash": "e05295e3b742e4219f40b8302b37b70b54aeecce2901c0686852514c84e7236a"}, "f2046c89-be1f-473b-b7d8-00cc75a02b6e": {"doc_hash": "0938cf9ab85349ce48d21c00978af37fe0d9bb78ae3f8f4e8f4229ec0f3cca90"}, "61fbcc4a-2ff3-4434-a2b1-def4e50448c6": {"doc_hash": "afbcb184505230837f342a49bede87dda61b56cb2e4718b207b60c016b2cd286"}, "3d7ea0ea-a670-46dd-bace-1f24075e9cf5": {"doc_hash": "5fa6a899b1d7fc571945f0511b26dbd45382227c6e0a85bd7c2afa21d49712fb"}, "0f4fa1e0-3c12-4608-a88c-32dde4d9f89a": {"doc_hash": "fcc81a5e3c6796034cf41573f96e5e21f6e2d2fe6b52493370f1084d34d035ea"}, "21b52473-5a0b-4639-9f60-744512e66eea": {"doc_hash": "2bccdebcfa9a6213d458e8cea7469a8ce21760d2e4d48381639bf87848669f09"}, "3abc1999-a0e9-4f88-83d1-065b75b6c629": {"doc_hash": "22b95fafb012b5be667263810a4fb2cd3a403b8290b6514dd8563bab015c1e20"}, "405af04b-ca6e-48fc-b55b-8af18842885e": {"doc_hash": "b0baafffc7366ef7daf427614024815b9c5bef804e0cc3094d5826a64816c754"}, "5abe54a2-1121-488f-8432-d5e8a4639a11": {"doc_hash": "8e31f2cac266ccbe431b4cb6ab20556ec92e3fbdb64e26c5a3b5c8d60a16deec"}, "79f4d535-870d-4f3e-b884-4d1e73519918": {"doc_hash": "e14020cfbccf524f305c0544b42bcf6a2758827d19a335a30e5417b4ee04378e"}, "98a84edb-f88d-40d2-8713-9282aed6c882": {"doc_hash": "005af5274afd27eca15c91d0ec9e65749abc5a4122123c691c5cbe14281fd066"}, "c2e2869e-0646-4451-83cd-af5d240fe323": {"doc_hash": "0ad14f633fb25fd49951a3f680d3ec30ac337381fd4d77aa24e59ddcee1747d3"}, "6e5f51c0-2503-47a5-9fa2-7c990f83ecaf": {"doc_hash": "7ed08006d59ddbe5fdc0f680604830e5ae8c60369bd9d14f685cc64c88e9edc9"}, "d1c08777-5fad-4bf8-8452-00565091df05": {"doc_hash": "c626f97e5487d3b3d6b6b6f8c63be02e0fb49b704e50d2926c2f927ead3f2eaa"}, "4a119085-03e4-45e2-90e4-7f29b1f32c15": {"doc_hash": "6fe8ea33ae13fc827d95e23e81a667d5d6b38ffd371b256de011c03d2208035c"}, "fed134e9-f268-433e-8ba2-c9c06ff67c01": {"doc_hash": "dfd7d8e6db1b3f1e396e695ef6435f0ee727ec180f3451547c704554d164e403"}, "31d4f3b6-d791-49cb-aff1-2bc4e875c24a": {"doc_hash": "4dc268e6547e9005dc44c40b4da72bb6d868dc8ec4b72d644125c4e6962f4a55"}, "b85b82f2-6e83-436a-b41e-7e745f1ba34f": {"doc_hash": "cd0828a02c02ad051717ffc24c323a9141e90f2279d70922a1ac6e4a72ff02d5"}, "b29defad-b5ef-4013-9dbe-22fa54f5723a": {"doc_hash": "349d8e57153840168a91fcd56e4c40f26bbf9a0f85eb34b801cd3ff84958ddd8"}, "f5e742da-cb52-431b-902a-3c9499254c6f": {"doc_hash": "b3732b690e1f1415d13f715b04579a9aaca34c2604b57b8e42766f10b9bc49a0"}, "79228d4f-13fa-4555-896c-ce4206dea5d5": {"doc_hash": "d0583378e7d754d64c2e8585b3ef8d77450c71b67e336c1a87eb33537c6ca526"}, "103a4c2c-dbd8-43e9-8317-81b986b82e4d": {"doc_hash": "710a3617589b173ebeb60c6d66625018b830ca99d2d5cdfb4679c921e37c41c1"}, "6498597a-d236-48d0-aea3-492dd2d4c43b": {"doc_hash": "a6a9d841d82ad2b6c272f48b8235c0d35daab8ebd09a7c9beb0e75eda903daf8"}, "2216e220-f0e1-4903-a543-765bc6d8b176": {"doc_hash": "4856a9ab4e5dd1c88466b48d420caa0fa948d5a6d9dbd6f458adb335c92ae857"}, "28e1be47-aceb-49e2-aee4-c7f364d027bb": {"doc_hash": "ce8fa6ea167a583105567d18eef2b7f17b64f09e0bcee2f055035c99d1918c59"}, "f6d9231a-63d9-441d-8e4a-ddb1de142b76": {"doc_hash": "7983dd2ce256fb370d8393c97a536db549a1d78f10705b0ad9303bc9b3846c89"}, "767123c4-0bc0-405f-84e6-2931d7010b50": {"doc_hash": "b1982933162473b06ff29467c4f6885e8751b820cf9eadb86ef5b3dd3b64afa0"}, "81849ef4-779c-4f45-b0fe-7f64e619641c": {"doc_hash": "0c6074e2c04c9d0ea2cb1f43829f9e8d0e1e656bcf4af1a59a85ad061c90a263"}, "c31e3bac-ac87-48b2-856e-21b9b62bdf99": {"doc_hash": "d9f9e92c7ca5028cead6d7e37457933d7f791fda5449919c5b0ea7d006c5d36c"}, "8368835a-1c90-482c-a0d4-1f8b08e89abf": {"doc_hash": "84f7988c64d796aa015976f60469f81bea86f260cd8f0f37eef81cd26aea48a4"}, "dda6a4a3-6c99-4d6d-b3d5-0eb53e959339": {"doc_hash": "eb933a2cca6baf4f339d0a7131c7a9f600a34d8d4ea4f6b3e82bbf2c850ffe22"}, "1e6d2ab7-9aa2-48a1-b92e-d710d6081e22": {"doc_hash": "251dc41ea9568ff7ca5464a946d9caf3b8e6de41ea06ba76cb1afd683068a1df"}, "a7069000-2263-4ef5-b4bd-2670f7b244af": {"doc_hash": "3aabde4d2c0c25292ef93382fd5b64b6f04cbfd09a8e8e823808530bdda4d254"}, "cb00854a-cfa3-4808-b312-34a371fa9bb7": {"doc_hash": "cd2de8831fc164565c514c6229b6b8f37b526c34dd75728ad75499b56b1eb0ac"}, "439299cc-4c63-48ad-ba1d-9f0f23568c37": {"doc_hash": "afbd606f2fd8b7635e111362a9193fab81703b9b96069b52369983aa0f6a7360"}, "d08a7801-73cf-4689-8f08-b89c88b4e895": {"doc_hash": "0b98462e5f9261fd370a367242a31c686e0845f88ebb0eb1339bc88edc5fba0c"}, "c1eaa4aa-464b-496f-9d6b-fb2a6a25e673": {"doc_hash": "8a9ddb7965e1f06b36933fc997972d383510fd939f37c8ef4b12ba27100413ef"}, "e2a99473-860e-49a5-bbd2-f9d5a1b7f481": {"doc_hash": "7bf0af86a3cf781206d876d446dbfe053eef1788674b42984dc8831032861d2f"}, "2a3baf32-1e8d-4b37-8af1-e69df72875c9": {"doc_hash": "6a0b96bdb294169309c81671f7723c1627876d5092739c71840d8d872ff7799a"}, "24af4f21-8a06-418e-a896-3ce61908ad42": {"doc_hash": "ecb0daf14e62e206ed9df94e6e13fff6015de0ddb84adcb16487ff81662679c6"}, "6df3df07-e780-41db-afc2-e1ec0a185488": {"doc_hash": "71e6aa60111e5a9cd3e60d47c6734b4dacc384d0bcc314788b046db35cb43fee"}, "3d4c44e6-ebc3-4d81-ada8-519e511e2996": {"doc_hash": "c97a33af3b220a1a1fada4a80ed61307a9205cf57b47b700bbfb8afbecaa226e"}, "b72172e4-3057-4829-a1cc-47da2571f4e9": {"doc_hash": "8a52e6fe2bf54fed05e3dcd2301eae0130ef3c7a6366164ed8769b872d527bdb"}, "d55d06a1-bce2-4b5a-828d-391ee5b842cf": {"doc_hash": "78874a8440bccd08aea2cb06c85b95f73335f1759e460e4f5d5e2d53f49554c2"}, "0c2ce629-591d-4169-8808-e1c2070ef19b": {"doc_hash": "493dd1ea4616e212d9be27e9608bc942a6c52e022dca80b91d9ff4aacdff77f7"}, "4efbee76-ded5-4622-8bde-d1602fe79a68": {"doc_hash": "67374e9704065d20bba0607ff0e21df84e9793d725b6b37b4445da2098a54466"}, "b4181ad8-31bd-4eca-8280-0af32859d00e": {"doc_hash": "ff3262bcbcd4eca935447d99e08be6a627dbc8c4c9de28da6118aedfeadb018b"}, "2f5033c6-1b63-4bc4-b41a-31ba2ea7df74": {"doc_hash": "9e0cf619690ec3660d242bcacd8dd1c3269e45017c83266aa5e93917eacea523"}, "9afffe00-31b1-4e7b-a559-e681a5544934": {"doc_hash": "1dd2fde29a40b00244b7e1dac33b72d89a47a08d2fb2d0d51d44ecfc06a37bd0"}, "8f99bbe3-d88b-4cc4-b894-32271d58c31a": {"doc_hash": "3ef6d4ded08c59cd8d1b030fe2632ca89bacf390acc3e4c159fbac7d1f01dd9e"}, "b30f9d62-c284-49a0-9a85-d9a33e43e125": {"doc_hash": "7554d680939b77b832bf61602a2a11cfb218e00116e1d42d8f6d0e7e53ab740c"}, "7c4436ae-990e-478e-9fb5-04b2b5776af8": {"doc_hash": "1ef6ae55ab574f06debdb58662c7ed423160c4308923d1c563c135052702587a"}, "217f2408-b54a-4a96-9534-dc24027aaa70": {"doc_hash": "14b8d75aa565b0fa15ce6e1400324e24f88e507bf8a35be7c3bb546ac45bb9b6"}, "49c6cf70-4a15-4376-9ca1-0deb425a7c8f": {"doc_hash": "859e2bfce964cb285716842bb5b5ff09b8a0a4d9eabc5e9406218c615d1795ea"}, "8403f396-80df-4204-9e23-b599ac6e60d7": {"doc_hash": "de4541ce04bc3a163cd59138bca109a99d401ad3c5ebea601d2505a7a02c6b36"}, "c5af9b16-e7be-4705-8a2a-8a4272c3fb01": {"doc_hash": "3d670df5286b1a64095e984b920f5f8748d51a52c4d20f3b1b1e55e0f9bcf3e0"}, "4af557dc-3447-439e-b446-ec7c2d1e99fe": {"doc_hash": "c6440b2a061d7fff1a4129a14a1adb6682a1da735a58b6001298009753988570"}, "793b3124-9da0-44d8-b861-924a69cbaf2a": {"doc_hash": "37c66561dfd857cbb76b08831ce5bfd8804ed5b675d726029f169c48ad749032"}, "f52231ef-d2f1-4916-97b5-f218dca62527": {"doc_hash": "256da524e7a6dc88fc15a8770d2c5494a2278b6ac0097d49add840bf06bd0d93"}, "95ea57dc-154d-4031-a8b5-32f9af749b9e": {"doc_hash": "19f5bcccc4ef8d5fb256806f9deb2c6db3ae1c828bdb82c528a892afc06e27de"}, "2ced4b6c-bc30-4ba2-bb20-a0c3ef7a2fba": {"doc_hash": "e1a493f955f59ccddf47e4a5919675a747f9f420dd30dbb7907dbb57ff9d626a"}, "ae043c54-47a6-422e-9c38-4b71dad7f186": {"doc_hash": "537d2785eab2f4b7cd7e446bcf79ba860f2a7f9110ab7f157535bfeb71a4c6b4"}, "ff46bd3c-3963-40c7-9730-a7366ec681d6": {"doc_hash": "abc214234c6ff5edce1826583b14c11a4192ca13f281e88c9260f241101e5c07"}, "fec5ecb8-90a0-4bd2-ba3e-973d1815d4ed": {"doc_hash": "cf30fa68e7ac61241fcb272c682099271f77444734a80460089ecfb22286851a"}, "f2b79dd8-25b2-4f5b-8750-08ab0cb13915": {"doc_hash": "8b2cf21124f026ae7fbe8a455a8c66fa7c6ad6be121e05d0071f30491f08e171"}, "7e379785-7af4-4d44-a5fa-8e69ef2b58a7": {"doc_hash": "0ef52f44c8c7c92cf000580e3c751754fa099523b63e372453d16975f20f7bdf"}, "72166321-1dca-4273-9ec2-700b2d669006": {"doc_hash": "6604be04c35a4cd626182c6225c3a90a82759019c61785bb4cb3739e9d7c6d3d"}, "c512290b-5a14-4689-a471-61a3ff057da4": {"doc_hash": "5f6acf836730d2004e31a80f1637dff0113ac60a62e6b092bef25489f936ea30"}, "576e8886-ad7a-46b5-b76b-c248e2aa0bec": {"doc_hash": "0c254cb2acfd36ac6399feadd59466f14a7de75d3456e3a1cfd8c530f337bad7"}, "7e07be2e-7da4-4e32-940e-c63ddc36c42e": {"doc_hash": "c793962c7fe80992826fab29b2cc7354aeb4bb28e5c8546535d8ed72ed34c92c"}, "601fb2c4-3d1f-4866-9a48-783cefe84dc7": {"doc_hash": "4f219b2467bfd6583e1965544b59d0b33f47d9fb80b5a731ff6918f01c7f1970"}, "5d3e47b7-4f4d-4dcf-8d72-b76c7a0f6961": {"doc_hash": "bb31171f64b8212a3083d679c29194bbc0f6913bb068681ad938f72173ae2e76"}, "1ac69615-14df-4f64-a476-de9376bdae35": {"doc_hash": "3088e03d9fde1940c5513adcc53c62f4d79a6bd462ddd0a6f93f7dc93445ca70"}, "1754b9f0-98b1-4e44-8f44-d38d180936a3": {"doc_hash": "382edab58625e529ab2301f97103db4609b0cbc1d28e1c37f28e3321da57e0ad"}, "bd9cd194-efde-4912-b656-d828341bbd76": {"doc_hash": "8331c20e5f0893915ffbc44affa6cc0c6efe5cce2f6e4bb540ad739bff705b8b"}, "50c3b015-0ec2-4551-bfe6-6afff9e75a28": {"doc_hash": "3c041c1d6d17a3ff49890b724308ee0df70c82530aa372763f2c5e62ff8a4a71"}, "0a115760-469b-43c2-9a94-8a26e81e5d6e": {"doc_hash": "517fdc8c4f96d8c6d7751e11e95ff71a266aba268c6921a644cc97406614b004"}, "3e75d103-8f42-4922-84bb-5b2dd2655e16": {"doc_hash": "19bac118563834c1bf178a2abe69c586caf63a112e19a42345df564f8eeee09c"}, "1fd0026a-812e-4a5e-820a-2a1f5c0e7368": {"doc_hash": "9f522c020eb2c843830c808af1c47996e8c512db0d4750004285d4c98d314f0f"}, "544b5863-8b81-4e92-85cb-51cd3b8730db": {"doc_hash": "00d76ee4afd5d8acbea13de49325531db2d20117bf9fc56f4666bb897989f349"}, "19fdd741-111e-4fa9-9249-c8c70ac9ccb2": {"doc_hash": "fbd428a90525ab357ee0dea1c84407190ffc988e76bd473f6715994b35b7160d"}, "9c95fa39-13bd-46f7-b7c7-c1050b3ef02a": {"doc_hash": "07b9deefc8ca95f3047b8623586e69de0045f1500bfc8310ecdc3d0d5e48425a"}, "76e01180-cd69-4c8e-9e36-55d224684a59": {"doc_hash": "c29bd8d15d07a88ed8c19bc97116f89f0e4c13f6e7b1422adf448fc381e586ff"}, "83e66da3-8e67-49bb-bada-5da0edf25532": {"doc_hash": "4655c48aa240d8184213afa7b95dbd7678271c2359ac2dc1c7e2adc827929ffa"}, "e1aa94f3-5b60-4654-8af4-aa483b434a49": {"doc_hash": "9d5a10d5801507deb85f63c94e3fb2f072b515e2b4927a8f761348fb26e1e29e"}, "186050ab-6594-46c2-a7b5-68653138ad2a": {"doc_hash": "cc915c16b7a7576d63f9078371d8ae55bd1dc7be8bdf094d18096e9b7522ae8b"}, "152b64b6-2de6-481a-b735-9e252d568a6a": {"doc_hash": "57360aa7a8bd87ef4437a6f843b6202f918f14f3c867c64b095872ef2d13f123"}, "29d93411-773a-481e-8772-dcfe093691ef": {"doc_hash": "eba152eec5cd5982d99a780ce4f3059329527450b9de4cac80870b22e630408a"}, "5572ad3c-c278-4fe4-a883-6f9dbc9d38ba": {"doc_hash": "581081dba6cce6cdfeb7e1d0d5148ff9b6acf679df8cb27347934fd059668119"}, "778289c5-46ff-4e3c-82fb-de2bd109c098": {"doc_hash": "88253931095efee37382ef891230bace749210ddcf3e3db508183b6500885bbf"}, "a0a4f0a5-39fe-4e83-a779-269db96baff6": {"doc_hash": "3195e403687ff2df71625b54d4d29413d106bcd7713e23827d9679ad4d01c4b5"}, "b3d19d2e-b0b7-4a47-9e78-7a773358262a": {"doc_hash": "dded578ad93345ec814c3d320525d4710f0d32e053c57ca71fc0fff57aa5312a"}, "90cb922f-bcab-450d-ba72-bda575bb213a": {"doc_hash": "19c8e77e7013176432026784cb97e6df393965ba7a38c9702c89865077d4c74e"}, "d87ceba1-1e4d-44fe-8c4a-406d00706124": {"doc_hash": "f90e327395216b5d86b5ec733dfb7fb5ddc8763b7b7d3d2da381a59e33b95e58"}, "00ef839a-3ab4-460e-b8e5-d9f455a0a3e9": {"doc_hash": "be78a1f77c02c763eca055df4d442824000103d7b231461185e9a2169c5dba63"}, "4c68f481-dba2-4b86-9843-cda6bd230dff": {"doc_hash": "790bd29bcc1bf55e469ad770e8b8018c9bd048842d1201dd5b9a4eadabfa8e66"}, "7919dc62-f635-4486-845f-26fbb3e54d54": {"doc_hash": "369773d65bc3fa38216cdcbcdb3c2d334efeae9ccdd9888bec6f41f3836892b1"}, "37605183-27c4-4691-99bf-2bac4417d33b": {"doc_hash": "52d8ddf079c8c6cf511d68ddc8e590da4278b1f9d78bf95902cc3a6c9c62eb32"}, "99e03d29-d56f-4462-a8cb-c63d24c63901": {"doc_hash": "ddf555f8c835d8668747229d57a3aaee2a39a3172ae79167ca4812b7f950c8c8"}, "409b493f-7693-4cf5-a8b3-734125930ee1": {"doc_hash": "2e045da88072cf42a2a933eed833691a880f28bc29eccd270dedfb9e7fbd0946"}, "8111fa1e-282a-4aeb-ad76-325b5957aa0b": {"doc_hash": "90f8fdbcc99ddb33fed69085768c3cc1d0236befe632a893d251783949d2bc62"}, "017a8080-0874-485c-8c38-3963e7facec5": {"doc_hash": "db8304c3fe3d52f8f8cc2ebdec571db4c0eebd7fb32d7d505f16cfb85a7fb332"}, "9da0fe77-879c-42b5-87e9-3c8a9369b93e": {"doc_hash": "b353321e4600538b966a7f46fd2de8b0bc9ff3a5a26a4545217f5e999859cf30"}, "c71edb0b-fb2a-4534-90de-9ae86f9431b4": {"doc_hash": "6606cf2eb8816baa62bde967ed0f35e1d781158570f3a3a57c9e98734c83d638"}, "a5d13e85-bf66-4ccb-846b-bc7092de000e": {"doc_hash": "b7b45686dd820f8252767387c4af39df22f1bde3dc34c3b4277d036d015fb97b"}, "411b3211-e640-40f2-93c9-93ddd2a96699": {"doc_hash": "4c71c7a4a43a03f8b7a6a693f0967f04db53167529d51a84e7c7df6b75802438"}, "aa01f046-2894-414b-8934-c00584b6c791": {"doc_hash": "364b44e5418b7c8566b121b9b3bc6a025355ece1ac807a1646e974e7d4df21bb"}, "05d539ea-44d1-46c1-a2e7-b021b3ae14ca": {"doc_hash": "397e823abb100ee88fd8b8d928217c6776cb445fc9dbb221ae3b3e5e0abd228e"}, "daab348c-23b6-498b-a7c6-2a873bd96d63": {"doc_hash": "d18baefba5e261b482bad3ae509039ac724c1656ffce7b50665c6c4fafbea74d"}, "1568921c-912f-4f8c-884a-e38c8d51b90a": {"doc_hash": "0f9c32dae4a43f0851902e1855d28bceb4a0ff433a76a81c5ddd1aa16ce0d930"}, "bb5e56c2-b605-427f-9cd7-608d82422d44": {"doc_hash": "a96767811af060544023015e10a59f975ef8a145949c47bb28ba95c0c63dc2ff"}, "87760bb4-e9dc-4161-a20a-f5437f573091": {"doc_hash": "9fccb5be64fb5a4b163323ec0a23b403130f51e46402da82a7bce4cadfd0e8f5"}, "209a3e40-b0e1-4ea9-abdd-e4e86f6b3763": {"doc_hash": "d7879fae9b2f753f6dbc42e6a57cd31ece3a2be98fe8de641930d3e5c79b968a"}, "09aa26d9-a6fd-4eb5-b87c-b233257b9689": {"doc_hash": "2eb7a510e1bdc8743dce923ece5aebb43c48f4dda21a8d206f3a5f17f3b0ecb9"}, "7d9d9298-2ef5-4516-9af6-c36d844d24db": {"doc_hash": "a689cea7a537c5e34b58b8f3e654f31eeb7b16ef6a72d7e70b818a3c843abeb0"}, "fe5b7378-fead-44f5-959c-3a4e195cc181": {"doc_hash": "dcbafd4ab9ee58e046e479a07f25fe65feed5a5b2650dae0aac173b0dcea94f1"}, "e937e334-b291-4e0f-b7a4-a71e9671b2c6": {"doc_hash": "45fa0b3de8c02571f88c172878458eea0d2d4d8f5d505e828ed8166233af1a5a"}, "98f2ebf6-bb0b-46cd-a1d8-8511d0564c56": {"doc_hash": "dde95f354a7335bce0c3da1595fefcb37fccf387653348a1770ae76b5519f348"}, "e5acdcca-6a13-4cd4-9166-d7ac1a9c8f28": {"doc_hash": "491a4581fee55aae7784a5e497cd8bf25911c5bb4829a1de51d784cdb845a7bf"}, "58026760-9a14-444a-85aa-45a9aae1c390": {"doc_hash": "9635ca81f9ad093d5170a80b6dcdb1cd64396c7eff1080ba8c4d485fa8008c2d"}, "10980c55-07b3-4091-ba5a-859dd76552ae": {"doc_hash": "95f8042e9eca94323d454992bce97eb0198d91b2842e02f1a5d634455811ec15"}, "b000c5c3-cbbd-4f9d-9465-c93e4e15cae9": {"doc_hash": "c9b54abd6202d0ac6db299c67a1a4632e407563956723d9ec5900305975c94ba"}, "842a92dc-b7c6-4730-bdf6-06a781779d33": {"doc_hash": "86a2218dbe700b205fc2762bb6f298ba07f3123eb2b887941481d0bf4ee99d01"}, "e6a6b86b-fca0-4bb8-b1f2-b7881005466c": {"doc_hash": "e027110ef2e1878e4dcef3f1cbd117c517f0e17a3a4f32b4931d44afe40ab10a"}, "3efc200d-28ed-44e9-8cc1-b2c4fb61c336": {"doc_hash": "fe5ae48738579c19b042c7a554d8a680a2c0a8a9ee3f5d2bd305fe9fa2f753b5"}, "503e7bce-80da-4b28-854f-f7653d96c676": {"doc_hash": "7ea015225abadf7fb9beb859cdcea13fbdbc62bea056cb0d9d1990b5ac9112f1"}, "8b5dc7d0-c856-4cd3-86f7-27e1b22982ec": {"doc_hash": "21a2457a58ab06777945119bb78af76895eeb9c73fe214fa2b33506f707b4584"}, "10937da5-037e-4fa2-9538-13bfccf6cb6c": {"doc_hash": "077545a71b8ff7f532aac1a9c525e700b6c1dae091b744955d3584e4bc8237d1"}, "7a6eecea-77c2-4266-99cf-0b19177ebeda": {"doc_hash": "62bdaceea6109c2a07d1978eb952f938b945ac623cba4bfb697fef07d3cced5e"}, "50e08c05-fb0c-4ffa-a649-f78d06af18e6": {"doc_hash": "a12feb3345d392c738b10f40f650d20ad61f0c355de96663db002a9398bd85e4"}, "c9749ae7-92b2-4f41-a0cd-c23014d604a0": {"doc_hash": "ca28037d0861696c3c9737c246bd388b461bea1b2d71ea9a59139dcdfe41a67a"}, "10160f54-a6bf-4b33-ae5b-39e7fe563069": {"doc_hash": "7abf1e52d8be5881be2f1436d315e5a49d2292c4179667ef37495d53033a48df"}, "d4056a26-f4de-4473-9a6d-8cada883c9b2": {"doc_hash": "cb7b4f7dd186a6c191124a94e270a1dcb167169a14cafa3a2c98a0197e25bbad"}, "7e9dae0c-d347-402d-b6d6-0a1905c0765d": {"doc_hash": "37756f5d6030eee1be4cd26c382f62831a4c8ff7e3bbf0f004155beb50d92280"}, "a54ebd65-7fcb-4243-a969-ee6c042da829": {"doc_hash": "9c2af8e09b841e7bce5e26fc2321d7542f2db9304e79025fb9fb78ede0d1fc57"}, "fec35965-c571-4a85-a05c-2563e4ffa32d": {"doc_hash": "97f59f2dd09b60cc98d5e6a824f53f2cae03da544a43819cf422383fd0a0aa9a"}, "a8742dc2-740d-4b53-b8fc-5f5cc281abf0": {"doc_hash": "c6f71b05c58af4e991b2a8517e3b6b952fc3edc20ee56e215b270e23e6d379a9"}, "8ce4934b-ebd6-4361-afbf-c894fd1e4f95": {"doc_hash": "fe21e588d65fd64a625546959d2a339ebd07a42c92a2091eda005fb2f5fb6cc6"}, "e0edede4-fd98-4250-b737-0d02c30a4095": {"doc_hash": "dee725b9b1452e9119593f681c565780dac7613bd78f76dfdc0906131789ef8f"}, "00734b50-c197-451e-967e-e0fed853a91d": {"doc_hash": "41e862ba6f4a824b31a1609f616fbe1dd19e1a3c98e785f53a436bdf36228691"}, "98d33290-6274-47c3-b17a-409fc9b0fe15": {"doc_hash": "3adf910341d8ced5685a5f7227079322affa24fa97f1adfa5be3720325a58609"}, "fe962d19-fd26-49b5-9f39-5e4ad131e701": {"doc_hash": "bc33138a8203d6a5989ae12436993ed052def3b557b72643d83b5177622e9e32"}, "be5332f3-7a96-44d3-aa44-1989e7c3aff9": {"doc_hash": "71d3a3ecf2d1f7d08bb3bee9496574920a99ac758db8968edae2af3b6bf0c990"}, "bb4ba5d3-41c8-4e4b-a08a-041a1cb1ba34": {"doc_hash": "b1d19a8cd776e630f2a0054f41c7f9b7a3dfb5ebc90c302a35719a210d74220a"}, "4fcd4b1c-d310-41e9-96c2-6738fdf8e127": {"doc_hash": "0b3c2fe80e8746bed95d79c01d7b62fe1d16294f4e978ddf419b247c6708fedd"}, "651448dc-7033-4afe-be24-ec734717b567": {"doc_hash": "1ca2e0c82d3c2c8aa3a1b8405d1d10663f243690418ee9e1250d970c611dcf18"}, "96fe2b0c-e2d5-4e99-a061-b6b441d8a076": {"doc_hash": "a953b8a4dc6c6769fce98d9222d4d32b297619c6804a53227d37e8866bcbf2e6"}, "23c54c6f-5ab6-49b3-bd1f-a573d6d954e4": {"doc_hash": "534cbdaac23649388a1f2a6f2d79b5661551451e7876f1dfe27f709114614dd0"}, "df1a903e-1c80-42fa-b362-c3d0947931fd": {"doc_hash": "f9a220df44a7c16da7974d61782e1e8cf37c263fb793c6102660d9f89505bda5"}, "690389aa-3813-47d7-a3b6-8dd1625bd368": {"doc_hash": "956be51e0a5a969defdcaaf9f8f5e47206a8a516124ca6c2a96ba9dfa12a9616"}, "93ff9b69-514a-45f7-9ba8-5daa806b6007": {"doc_hash": "80902d716083876605f9924922984d4950b70aed98df6580a3acafd16534c558"}, "0ff563cd-e753-428d-9ef3-3664d88309cb": {"doc_hash": "fabfafff237b2376ce0366ad6a45e1fd9995ad6b457f5bfc9c713faf3bc58d4a"}, "db1dbd59-68bd-4d34-b6e2-5bfaf15366b2": {"doc_hash": "97bab4e317e821ed5707fcd9a2e7c3b80a51741c85c7a792e5a9e2936f12dc4e"}, "46f076dd-dffd-4397-aaae-74bce5b5bea8": {"doc_hash": "7de0a805f2290589755875290f441040ebf42fd706b1b61d435fcbc7e9ef1903"}, "9f0cf87f-35fb-4ebf-99e7-130b29df2193": {"doc_hash": "bf57df5a3eea32f7d7db70d6139f7fdba54e4e31d92778c97dc2ee2959ec3d3d"}, "0c33b3bd-0066-4d00-916e-e32401fe9650": {"doc_hash": "587c9984fc63e39eb26e1ea48caddf6efa16f8a95dfd318a6075738f4bf8d5ba"}, "0cbc6b0e-d6c8-4117-9419-5bedeb3936eb": {"doc_hash": "d4da9590440c371a88c3ef8176ce81b562726edffd19904f694dd5c0b81a8d69"}, "bdf420b2-2366-4251-a5cf-741686d1578a": {"doc_hash": "5a0b1274fd0410b98978e38c2542a9eb80606a6031297672f7fa8aed59d74c2a"}, "ea4a1bbb-846d-4f7e-8bba-c006efce7256": {"doc_hash": "0f8c1dfe090c9df31afb090b68a18b2de8b16b5a1a67bb66e619bb1f54fc97bf"}, "5d0f0250-d790-4f6e-bb02-65617a283d31": {"doc_hash": "0dc45ddecd5d836f0da4baf0290be26b2e2951cb8fa2a84d4fee162b8df877d6"}, "3d32703a-be8c-4c0b-9395-14e294fa723e": {"doc_hash": "30c84cf221104e20cee6b6fd82e922f6a4c4404b37cf728c77b41d16d0836993"}, "369a3b8a-a71e-4be0-870c-51a739d1c073": {"doc_hash": "73392f19dfc0ad53b033b28ee4f2319392e947ae37a4e1e23df683d404cfc5fb"}, "3aa2b078-6037-47fa-930c-37af2349e0cb": {"doc_hash": "d3ff9960a8dc777dd1e0881caccd266e114ea535a0d28848ef539687c9d69035"}, "ef20d96b-784d-4d7d-9060-405a11c3e355": {"doc_hash": "9645421262d576eff64e05497f9ac6f4077772825c8be584541c4b93df1e5e82"}, "09d1b8a0-094b-4def-9a8b-38e6bfee07ca": {"doc_hash": "9e19fb1bef06584bc70c53585d7ab0c52fa2db869cc7af007382252bd3810a15"}, "cc41c678-f51c-4920-ac6a-8b0f05e47c2d": {"doc_hash": "1294ab5fd46512b63e57732fcba38cbf9dbbd2df9243e2438bab411093e13440"}, "0f26a159-0b16-4f5f-8520-db00ebf39179": {"doc_hash": "476f9746bc3c6e19619a8cb84b022cd514583d5cfd958aea1d829a2788fc8f34"}, "b24950f1-5349-4740-972e-308187b05fba": {"doc_hash": "7e3669cc1ea80bf7ae9bcc52a1bcbfc36ba234938a83f7dde9f5e2d7e74756dc"}, "49d86a18-3e70-4626-879a-f252cd44527d": {"doc_hash": "193df77ace2bf4a70a1b209f37acd8fec3464dcac21034e115eebbb5a759e1ad"}, "c0007d41-3ddd-49fa-bfdf-cecc4612cf0c": {"doc_hash": "b00025c9c6e0791f229944dec816d4f15e3eb19a66e3d585bab97ecdc2a490de"}, "758e4d1a-e9e5-4722-8a57-359e0b30038d": {"doc_hash": "ceb9cca9a6843ccc1b0f8435e150e6801a8ef52ddbf62229cc4fab75151ae2eb"}, "f104c9fa-404b-4f46-bfd8-92f2c5f2f2d2": {"doc_hash": "cfd05c7af492ea812aff089ad1cb09df14ce1b1a02f2cfeace7a08596992c880"}, "143e14f1-5368-4eba-8bd1-934e50b749db": {"doc_hash": "7ba1f8130ecf6456ff10732aa929af271049d3fef933794b308200189082c09b"}, "008def5d-53f9-47ef-a967-b103713b7417": {"doc_hash": "312ca17abc20d58e9cd8d6f94ada8e95823e8a147b14477d05534b81437b3cb7"}, "b5856735-225f-44fa-bd38-13238b5a1600": {"doc_hash": "913fbfda451e425e1f5e91e9d2099fa9d25847233db04a5d1a76408876894e42"}, "fa1a463d-73cc-44e2-b387-a214ec70d339": {"doc_hash": "540aabc10e06e1c7d700c9e20d2e3508273b329592a2b5bb7a7e85dce1914fa9"}, "98cd11e6-2a19-4f4b-9a1d-241f673ea579": {"doc_hash": "f8d3953a3850d898526bc1d318f018dca6af93e7ddbb4f61e58159a87c595a21"}, "5ecf0323-cb7e-4855-a4d5-73fd3325119d": {"doc_hash": "6d77a33ba5ba18c1384fc381a936d971b78f8b88a2bfe7e25dc4f39ee79b9ed8"}, "a62e534c-dc0a-4f23-8ff5-c02fd7227d2a": {"doc_hash": "d916ddd44583cbb3ca6a4a2cd9c3bc640ba470676b845777f7c2ca6f36f7cd51"}, "8b8156ca-1b27-4cf4-a97d-1f7db75c1c48": {"doc_hash": "2702af43bc2f210ff1491972c55769e1ff95fd0141e9f1c5b40f15b7e69b1f23"}, "977e71d7-d3d9-4a86-9cbd-9b0d74875be5": {"doc_hash": "7848330e2c051ab5a2163e10666f3889bd1815599d2a46b72ade51735be73308"}, "2a49ea3f-f97e-48bb-8e60-522669e5a6ce": {"doc_hash": "021a4716a05950532c03e79f6f0bf6ae063786c95715342b15ded101b9419f9c"}, "647c441a-317b-44ae-ad0a-d3b676414870": {"doc_hash": "e268def1c1563fe91cb93441504807482bbe7c7503862b11b2eb90fe50038416"}, "6a6ac874-6cee-4cb3-b708-393e7f9c2b4c": {"doc_hash": "5f3774428689fc14e9a891ee2cb54dd517e41bbd23e30da2167062bb04db4356"}, "67ba3954-8bcc-498d-af0f-622c8f06da03": {"doc_hash": "f9359d4ff6e68cb9a958fabfdb19ef6f58c14e7898ef1e84609159f34280fa0a"}, "f23b029c-2575-46b2-bddc-0e113bde4e22": {"doc_hash": "a81a269ea6becbc9261c57465ecfc6d854616fc976e6c1f232037e7725b022fa"}, "f25bf879-6e54-40b0-9093-b72b50ee1594": {"doc_hash": "79bf397c39e6ca6f6170969e8ef9a57e25969a6c4ebda3d165dcee19a8e981c8"}, "d92fb4cd-5b10-4116-8bb1-f8050aa6a773": {"doc_hash": "161afa0a45d728749a50c76144819511bb5aa5b413204ed2372fbc3a936f0d7a"}, "7edaee02-5ed8-4618-8055-0e64f95ccd43": {"doc_hash": "a157d3d6e82c5e039530709f26f6f12dae38a628b3b25f7bff8c2d59733a6671"}, "2fe6c804-8fd5-4168-bd4e-4a95a469d7bf": {"doc_hash": "9df5a071052a4bed53c0751bc4f4fccf70f5652ed12fc2b6c7f9fdcb7761dc02"}, "a420fffc-cdbe-4a75-b515-55452e7d50ab": {"doc_hash": "d481e301cfaf89ef43768c75bfdc08ad8fedf7f68e7399999d6483d19e0675f5"}, "21ba9a70-50d9-45c6-8c11-2c6e37237db5": {"doc_hash": "482f2a040abfc147d485d5338da7fbb57d476b25798d73045d3b511e6eae372b"}, "d5730284-ad9a-4bd4-b93f-8fea33c716d2": {"doc_hash": "52eebedf14d2fde6a367c675f002a49289833de8ab88d1c51d1b6109c69cc9ca"}, "e481ac7a-d91a-46ac-bb45-ae6fc8209ba9": {"doc_hash": "62bdd67e93702a0e69f97521c63d08a81b2ad937ce3c0e09f7a5785326c799ae"}, "109031cb-3ece-435c-b7bb-92916a291764": {"doc_hash": "6046f66ba246151d220f4b04697b7773ae53dacfc6b839a96e455a0b9fa8d63a"}, "7035445f-bc91-45cd-b6e1-cb5dbe990b24": {"doc_hash": "23e48766633870656d7f122c6d357031a92bc51cff6b5274abd7aae10ed5d6eb"}, "524c5041-4330-48e9-8291-afb62bca493e": {"doc_hash": "d578e9102300cfe6fd42508dd054dea5b351cc3188fde8cce92c6e7a096356b9"}, "62f07fc0-a4b1-4c2a-9265-322d606af6c0": {"doc_hash": "fe44f800741e7e259a034a4dd4376848f9f073582637d7b2a25a843a08722887"}, "ccd841cc-111d-4d65-86f0-92bcf98d657a": {"doc_hash": "9a962eb392a76896d4fe1f5ee9d1d9a364949e7ab348ad2bc7645b27911d9e50"}, "d3d6b1b3-5cfd-49de-9f74-1b6ca1bbbfd1": {"doc_hash": "039c4d2477ac94c428402f8fb0f012617886a8048252ef0332b9545eb1c60dc0"}, "da88303b-6d89-4c91-a3b0-94d45156a116": {"doc_hash": "763c4a0f1aa245dfcde4c8f9291acee9967f1e67946f9ae440aad929840ad6b0"}, "5f96fe2d-77d6-4f19-917a-992b886f5596": {"doc_hash": "63e37cfaca602b3e204a3766dae5a741b999441074bc450e2a162a205ceec065"}, "2c27fa68-8aba-478a-ab1d-e20d13509793": {"doc_hash": "760181659fe5bdf05ee6e23a2708bf190d5c16a80d39b12c3e71bc44d690a1a7"}, "e1278f72-b643-4d56-8ebd-c87c44d07812": {"doc_hash": "1a85fea852356c595598bd409c90c93f841ea2ad0208a7926f0c8337c5bd9e1a"}, "de80ca07-9d74-43c0-949f-63d0cd41d499": {"doc_hash": "0c48b217f9ffdb211ef84e188eb99fc2fcec65842ece539e99ba356f33a13d21"}, "2026a230-eb08-4765-abd0-2547f6414398": {"doc_hash": "458ec7ce2bd1122f40c221de74ab2496c89747870622a734b607b2700e80514c"}, "c175af1f-26d5-4f67-964c-f8ead1fb0da3": {"doc_hash": "f3ada342d3fef59b208a839e46f3f5c1d2aaf5225ec29d76326f4f32096151b3"}, "74b10294-e51d-4de2-bac6-10e8f9de2922": {"doc_hash": "80395312320317d60434bb2e18f1a9cc70d4ffcd1014e0c83e032364a7f23ec4"}, "2bbdb5f1-333e-4f38-b4e8-97566a6b5fc1": {"doc_hash": "cb0a3e67657d45507d710b27be3bef2e27f18a5f1eb288988488e9a01531975e"}, "65f279e3-1db4-486b-98ec-48a5931b7f14": {"doc_hash": "884599b7a3a66886b072721508e18148ec4e73533e27a4b7f70f01b1d63a4ef3"}, "5d8173a7-f6f9-4224-89d8-6f9b8eced1f8": {"doc_hash": "f12631723b242af8b0619cbf90e014435e3e333834e7c378aeef015598fdc7e1"}, "3298f657-361e-4144-a50c-2419a38baab2": {"doc_hash": "158eccee38d52ef8e7e62e3b324a48ee7392d4edd94003dcafb7c741ad498815"}, "a8189b9a-1db2-48c8-aa2f-35088739c43b": {"doc_hash": "c6e05348d35cddbb9d30baa3b0893d3e1446671133a23cedec026fed6b6afcf5"}, "ddeb8c32-f31b-4f21-959e-6b9d899586c4": {"doc_hash": "534a3c8139d3acf1d7157ea29719e5eaf915544c97a98afe4fe9698610b9724f"}, "8685baea-bfdf-40f1-8c75-d1aefac285b8": {"doc_hash": "419616973179891b273623b3f4eaff0c0682b77c25d011c1f48dbda128ea3bda"}, "91ba294d-4b11-4091-917f-e6fbf27ba745": {"doc_hash": "624e530005110e4c782cda1678f6b472660d9a004cdfde43f98081de97c30b48"}, "ad31daba-7fe3-48f1-ba8b-818e3f1ba463": {"doc_hash": "455e3164bb13ff3362bb1b9b9c60d660b633ea68ca14e8f7746a8009b9aefd23"}, "63dade9e-0bdc-4ef9-a2f5-f7f607c21064": {"doc_hash": "18bb27a60b76a0abb3331cccbe4457c4be42da8ba79e1738a73afb1cc0eda330"}, "c3793458-fa6f-4e13-8af6-b9a6063536a6": {"doc_hash": "19fafb6d9b5cd0dc5e5abf0745ef6c47c99c64df381686ff4dc0ab2bef776040"}, "ba272cfa-53b5-4ff9-bd17-8525f9732c0a": {"doc_hash": "439df7d26dc4f0ac3d1dbc8310cc181cb4a0048f0b1d61540702ff87a3f23d28"}, "27478eb3-a3f1-41be-88ab-7b8d9ec9cabe": {"doc_hash": "79299d752d19515e4fca07dddc35ad725951e2ac31ae3c851320e1492be2bcd8"}, "3ae42d6a-ee99-4f04-b0df-971eb61f4b94": {"doc_hash": "949dfd23e8c416cca0811830ec58bdc0474a2fe08d0651dd6234d6c9a4055e67"}, "aae88323-c36d-42c7-b865-1a206b7cdb31": {"doc_hash": "dc45956958648150fcdfb1869006381184046de63613739630743533672749e4"}, "8fc33c58-d086-47a8-b0e2-d920f745c2bd": {"doc_hash": "444fa2687408ca16bc97889090d7ad462afd4fba01794a306685b184a1e0e674"}, "5e8f0e54-9088-4315-8aa5-efc0551297ec": {"doc_hash": "d263790396a76f2061309e0f390e40d0b250db8c6b35e5fa60226e029e64f92d"}, "7fc36dfb-8014-43a0-8fa6-348dd333052a": {"doc_hash": "20baa5ad9708053bebbd55f07957ebc2923c81ba6a316d591b7335dd697c57b1"}, "98684ed0-f1be-470d-b6b8-d3dae59984bf": {"doc_hash": "e2eb6a4eded649cdaa6ca5fa0911e60f4779eaf9dba0258c3fcfdd59ff54cca3"}, "721fbe0c-9385-429c-b33c-c6c7766426c1": {"doc_hash": "3711cce1f8dc15bc542c1cde827b80868752cf8f2457842ac13425a970eb743c"}, "acd5824d-4f50-4ae0-b31e-546f7bddf8c2": {"doc_hash": "411d86180fed49a0020be8bdc43ada7a6445e1896d5a27ab13b1e83670894213"}, "91253c81-ed39-4ff6-887d-ce286d3c574e": {"doc_hash": "694c42774245b4b67b4c61f8eae7a79b064c9fa226e56e056ce2a9657260df32"}, "7577febb-6ed3-4406-a535-ff96a6ceee1f": {"doc_hash": "d985bef5bda9282a8a3c026ae07e4f993ca67a4f1bb226ecc416fb8c09107a71"}, "62b5ef8b-eb43-4c8e-8cd2-cd89b95fef09": {"doc_hash": "80101eb6f8a70b15c33debec47107fc7e63a0603322056a2359e4b9b16070ac6"}, "d0d8967e-4a10-499e-82cb-5437a44be844": {"doc_hash": "05e75eeac2fdda0c5633ff71c5e17d88b627b4a8718cba65bfeb312fd5b2cd0d"}, "ef5679dc-0a2c-412d-bb8e-e6fb99666db2": {"doc_hash": "e54eb001dcc88256bf5dbf5f8fa7d42e954dd7a5bb531434d868d55865db1b4c"}, "af4fbd4c-ec03-4b75-8b65-5a4fe8a9aad8": {"doc_hash": "b53ecf43906a13baf5bec72010cf3db693e663bc93abb8fd967a0f7ae1ccc8c2"}, "6c309842-50db-430d-86ea-38209cf35792": {"doc_hash": "af35267863d4b703041ff883e2699d20a0220d176da63a5858eed8946b817342"}, "5b4d770f-b378-4ce2-b2ee-b3ea92812a92": {"doc_hash": "abc9155d745a2bbd951b3d81960ff593ae3a2bf8c0f8dfd802792854d3668790"}, "57677e66-7d2b-4032-8a3d-8be75bb8e301": {"doc_hash": "e8c84037d72b028761056734ecffe942ef182c815f886eeecc236521b3657a1f"}, "2e5ef6cc-51e6-4f59-b6c9-a64fd1ebcc3f": {"doc_hash": "8f2871a0acc886de3a550645926042ad24fe76b98c91d99c9340ced317e9a61f"}, "97e40446-216f-464a-8358-8156fed480f5": {"doc_hash": "1fb7c03a2c8e7859030f4c36d311506419c19a6e5060533aae7d18eb7973f91b"}, "d00e4faa-b245-4e01-9ae7-0f4b2be949cc": {"doc_hash": "1530ca1c45c2a8395c58c626f8e92adae006a4a16b39f4b6be8b5de04f637605"}, "3714b9c2-be2d-4b20-97e4-9dd58c54be8a": {"doc_hash": "b7440ffc4f50601dac91fe49fe43e5bad2473fae342c6d8e84a3e9aa0bcd8da9"}, "a4bfa2ac-76b3-43fb-86d5-636405609c51": {"doc_hash": "bd756e048eb69acf7a4e47d82612acdc7b895eb90e5a0d76dfa93f4585cbf7e7"}, "432c5169-c87c-4f74-944f-fc6c960ab55e": {"doc_hash": "3374b8c7437e7c9c55a877b7f3cc0f6876bb753c9be52255ce54c86c7afa07ad"}, "98a27ef8-7451-47f0-bee8-b77c334f4816": {"doc_hash": "d09e52b06f248852607e5b86ab3f62f89d19269c330f5c2e16bf128aba7672bc"}, "7a2cd343-24c8-43d0-a33c-e2df37a962d3": {"doc_hash": "33059b2a21b89782fb86e069011b08f072bba981de1a949ed5f0bc6511905e11"}, "73ddac6e-58f0-44ae-84d3-99ae3ceb1eba": {"doc_hash": "4f8bed6a6aa92714bd1d08e7162bfcce6e6889d286534a9258a2168d2799e333"}, "e4f76778-c4af-40da-9ac2-4081f66156ee": {"doc_hash": "ce533b231e634227017d17e33271cbdd52202c28dcec2a46cbb8b4691f0c38bb"}, "8292afc1-2ddc-4e38-9efd-eafa2f412f43": {"doc_hash": "ca7fd336beb0362a1ca4ca254675443ee825f34236a5f7b32f39a651ccfbc49e"}, "03bcc048-789d-4740-89ca-6bf50df0dee1": {"doc_hash": "aa27a7394483f0b2137faadf0d2e96c6e686344ed58aeb5f38641d772a6f9087"}, "9e8ba278-5de2-480e-9064-7ce9f28f7ce9": {"doc_hash": "835efe50f806f08c3f0d9a453a66605bdebe5a6ad82b5215f25d1cd8d714b483"}, "2906fa52-7759-4973-9504-97fa5e666089": {"doc_hash": "be7e918220aabe39a7d302781c5945feb82cc6b9990f67a29effd1e235e9764a"}, "fcd7947e-0db0-441a-ba90-3149669ac621": {"doc_hash": "e74f41461d411912c37e7c68512e89517256c498700da85025a507a0641e0128"}, "913c4ee2-feb8-49ea-aca5-c0da107caf08": {"doc_hash": "53254df6b03e6433bf6b64fd85af2661d3567541ba4b243356b5d5e0ec810fd8"}, "77163a6e-71ad-44b8-b689-ee9ac6a5bffb": {"doc_hash": "877507920f457ec50a6ca17fac864a08de1ab0a0a575cf72828cca06d501c13f"}, "f5dec663-5ec6-4e7d-9ed9-0e4bb0f3cd68": {"doc_hash": "067359b0af68999c91e12ce41578f7adc3dd6ba09b814797b8ae38ac4ad93e8f"}, "172b09fc-1d8a-45b3-9bf0-ed395f2702d3": {"doc_hash": "a9b5d7b0815621d9ddb45b8e889a1e309ad383edf08dd7b20ced1b614d71dd06"}, "62cc3979-57bf-4b57-9f5f-c6daceb89f7a": {"doc_hash": "78294bbefa772a6675d80d6c30ea592ddc7c4f0f7dfd9a28d29c5c40bf796a17"}, "090b47d2-87f4-4339-bf9b-46c8d2ccdb0a": {"doc_hash": "fa30405a612c0a167f69ad8d78f96904206c05e1a0cc96c27599950b032db2e3"}, "deb06cd8-6c6a-417e-b762-d75830610d62": {"doc_hash": "f1b53543835565c84742fcac77a0100de820686cb6a7e350cff01b41db4dec99"}, "ea7fe245-ba09-4c02-98c7-65ba0608c36c": {"doc_hash": "1097b0fd9b95c423da31e7e37f1e9ecb09be1abb9bc0938ba4454f9e3a9fdaa1"}, "d63201b5-981a-4847-aac8-15880e8f515f": {"doc_hash": "db786c089c0456866f496f06d0a32c066a47e9dcb8eef8afb7ddc34f38a3592b"}, "9cfedb9e-f339-4f9a-b7c5-4b9c6508be9b": {"doc_hash": "fccee6a478a0d1fcd2f6c94fd4642b55baefbc30e032b02cd8cdd633aa345ad0"}, "aaa1a8e4-9dd8-4806-996e-15e3e7a74433": {"doc_hash": "97430688c9d5565e8bb7f15efd35be428af74fa91e336fe74659d6c483ecda94"}, "b99c67bc-90b7-4233-897b-01f633332e0b": {"doc_hash": "f7fdf6034c732982c4ab1a28ed3d72d14bd0224d0edb206422919df83b115c4c"}, "e1ef7693-d4e6-4eb7-a09f-806e2e6b911b": {"doc_hash": "e9687753359630980ef6fa94c8161218a07e11b961d15eb436567b9f588cc67d"}, "f8238174-bfa6-442d-b3e0-8b705513624e": {"doc_hash": "529f27ae457e0b3323efebf8175e6d881449e13adff5ed635cb982d2cd7146d2"}, "1343be09-055e-488b-8950-c0c23143f667": {"doc_hash": "17d99bdb24ba75d63a035bc48231bb8f48cebce659b5ea1c11790c080119b6dd"}, "1bcf9694-e60d-4d3b-a7bc-ab9853e49ea3": {"doc_hash": "b336ce8ae27678c54c9b05fd55a436e48f236870ed4a34dfeba59560df9a6914"}, "e2204a59-1851-49fa-8f33-3e8cdf0a2dea": {"doc_hash": "e902cfcceb56393ee5df2f6ecfbd3a84737bb34e5d286f48fe50a19155a84b72"}, "f1951d7f-32e5-4f21-bbbe-d1a1b6b9a08e": {"doc_hash": "99df3fc550add97e4ca5fcdd7569376769335482ab604a4170d8643bd1477843"}, "2cab1e53-df29-4146-af83-8afbf51d4178": {"doc_hash": "bc80d6ca835c21b53b5dbb24cb10dc3af817620d65d27e485bf724f70fae7f2b"}, "a0e20e99-7055-4d38-9dc4-1079b2ee96b7": {"doc_hash": "fd998bc1c4e208eb1ec59b021eac9c3b14f1f50f1e55cc328e65e2beb80b5a73"}, "06c86b96-25ec-4e5f-b3af-ce0537336583": {"doc_hash": "68dc3b9bf00eeb8bd7cc2e97442edb22f0f4911d150e23ae1dbd0f5908766836"}, "41d5831e-182f-4895-9834-6d4915fb5ea5": {"doc_hash": "a4ea97970a75c328ccbd498ad83d0ea8ce670f4626698a52f465f1da8f9ecd24"}, "bad559f3-157c-43d2-ae2c-e6c040993564": {"doc_hash": "1a49eec8417b644728b627ed87224bba15d3a8c859217bffab66aa65d6be9883"}, "8d0d5206-9b98-4fe5-8095-3dd61c69c5b2": {"doc_hash": "68167e61dc9155e61940b0e68f89a34bdf6997e2392f94548c3bd6ec80f3726e"}, "6f7269fd-5a60-4cb1-9b31-aeafd377e0a8": {"doc_hash": "7393cf2c16fda84383032761b5384b556513e1705ac928f792022cf228968036"}, "3403c672-38c4-4407-a758-c535271cb3f4": {"doc_hash": "fe8f1c6b01cca651db7903b95aeb36f79bab6a50810a13a7abb82864b719d376"}, "00bf0a76-7f33-4cfe-85ec-c31b7820ef04": {"doc_hash": "383ef0fe93351d8205601130918eaae85392439701e1ad628ed3a7ad1d162d8c"}, "f1e84331-fb39-4b25-99b3-fa29d70d90dc": {"doc_hash": "36149bc4f3f7f1e96d95e366a2aee3e7895038e5e0f5e975f20dd726ffad6ed6"}, "a9d96350-2974-4dbf-b331-0363d0290cc7": {"doc_hash": "bbf745591f725413717443a3b570e173e35d9f6f580f8fc50cab71f9eef24239"}, "022597c3-8657-4751-a0e6-0883abe39e55": {"doc_hash": "74213639ea97c5512ec78c9d4b7a65a0fbfdec533b5dd08d9d52a18bf5ad74de"}, "6f8b7b8e-0329-4352-b032-3f7d1df2d5f1": {"doc_hash": "4303c4c6c397d75d99168e0c48bf103a5e6eff392d6e818cbf3fb43075c60178"}, "a8166057-ccf0-4b20-b999-57c2dba20a86": {"doc_hash": "fac4109903b4b2a8cac4d0f78a6e86f8cb20e00b1d03d0a32f4488e8c2e289c3"}, "3673543a-488a-4f89-8b6a-ce0e32bb2e66": {"doc_hash": "ea45867a10dccc940e67774ddded76c026717c28d9da2e29b585d80826d75e25"}, "08bab5d7-5ba9-4329-a8e4-b69e1df08e93": {"doc_hash": "3f51d56dd57676bb545b69cb09d13c8dd602b0142c097ee6b6fb03585f2cf971"}, "a7596ca2-24d0-4d33-aef8-f5e3fc60d4a7": {"doc_hash": "3afa40f25c1aed2872ab81d14d8c8a86e9ddde326f40b4a4221e31a957f72926"}, "eaa16abc-5911-4267-b86b-6e70cc18a254": {"doc_hash": "962ce3da3fcc82e6e4117f071c607c5567ad33f412fbcbe6a4b68225e78cd750"}, "5ebe486c-e742-4675-9c48-88bb489cea4c": {"doc_hash": "eeccdbb804440f1dc3080b6e0a46a153408f4d9b8488ac8081da7b0769ce042d"}, "cf93cb03-b934-4cab-a5b3-49ebba78f6c3": {"doc_hash": "4fed9c90fcd949be3b84c179d4a8279a93208e33c1eac8da1ab543b13b52a429"}}}