{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f4b00e-59c2-4496-a0e5-cd10dff0f0eb",
   "metadata": {},
   "source": [
    "## Autocomplete phrases based on logits (Sequence Selection)\n",
    "\n",
    "Typically, autocomplete is done using past search behavior. An interesting alternative is to use an LLM grounded on the document being searched -- this avoids cold start issues and reduces leakage of senstive data. You can  think of it as a variant of the Sequence Selection in Logits Masking, except that the human typing ends up doing the selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41664a72-3e67-4e57-9e6d-2a01ed7ed8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet transformers torch fbgemm-gpu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b078983c-cda3-46d0-8438-d284bc017b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHANGE this to the Llama model for which you have applied for access via Hugging Face\n",
    "# See: https://www.llama.com/docs/getting-the-models/hugging-face/\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"HF_TOKEN\"][:2] == \"hf\",\\\n",
    "       \"Please sign up for access to the specific Llama model via HuggingFace and provide access token in keys.env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7a518-e7fc-4f71-a492-e9c5d6d15798",
   "metadata": {},
   "source": [
    "## Load document\n",
    "\n",
    "Ideally, this is done only once (such as by using Context Caching or Prompt Caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e87858-16d8-4651-bc94-f997c1ce23ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to julius_caesar.txt\n"
     ]
    }
   ],
   "source": [
    "# Download text of play from Project Gutenberg\n",
    "TXT_URL=\"https://www.gutenberg.org/cache/epub/1522/pg1522.txt\"\n",
    "LOCAL_FILE=\"julius_caesar.txt\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_text_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully to {file_path}\")\n",
    "\n",
    "download_text_file(TXT_URL, LOCAL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048e606b-cc21-4cad-bc72-4972ca073437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = open(LOCAL_FILE).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2588ac22-450c-4cb9-b54a-bb368dd522ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = 0\n",
    "end_index = -1\n",
    "for idx, line in enumerate(lines):\n",
    "    if line.startswith(\"*** START OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "        start_index = idx\n",
    "    if line.startswith(\"*** END OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "        end_index = idx\n",
    "lines = lines[start_index+1:end_index]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d262c-edd8-40e7-8441-9cfc8f8dd1f7",
   "metadata": {},
   "source": [
    "## Use logits processing to select the next word\n",
    "\n",
    "Display options to user, take what they provide back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddad926f-2822-47ea-88c5-db42c8316e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessor,\n",
    "    LOGITS_PROCESSOR_INPUTS_DOCSTRING,\n",
    ")\n",
    "from transformers.utils import add_start_docstrings\n",
    "\n",
    "class AutocompleteLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, selection_func):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.selection_func = selection_func\n",
    "      \n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, input_logits: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        output_logits = input_logits.clone()\n",
    "        \n",
    "        decoded = [self.tokenizer.decode(seq) for seq in input_ids]\n",
    "        selected = selector(decoded) \n",
    "        \n",
    "        # logits goes from -inf to zero.  Mask out everything other than the selected index torch doesn't like it to be -np.inf\n",
    "        for idx in range(len(input_ids)):\n",
    "            if idx != selected:\n",
    "                output_logits[idx] = -10000\n",
    "                  \n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a8745-2a97-43f1-b819-2fc074d404c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee30e0c840844d21baa2bd0596c9d76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=MODEL_ID,\n",
    "    use_fast=True,\n",
    "    kwargs={\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "    model_kwargs={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef0ee7d-e7f9-4354-b511-958d53a7ecf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m     results \u001b[38;5;241m=\u001b[39m pipe(input_message, \n\u001b[1;32m     32\u001b[0m                    max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     33\u001b[0m                    do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     35\u001b[0m                    num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     36\u001b[0m                    logits_processor\u001b[38;5;241m=\u001b[39m[autocomplete])\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 40\u001b[0m choices \u001b[38;5;241m=\u001b[39m \u001b[43mget_autocomplete_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(poem)\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mget_autocomplete_phrases\u001b[0;34m(document, typed_so_far)\u001b[0m\n\u001b[1;32m     23\u001b[0m input_message \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     25\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt}   \n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# alliterate on the first letter of the animal. So, donkey would be D\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m autocomplete \u001b[38;5;241m=\u001b[39m AutocompleteLogitsProcessor(\u001b[43mpipe\u001b[49m\u001b[38;5;241m.\u001b[39mtokenizer, simulate_human_selection)\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m pipe(input_message, \n\u001b[1;32m     32\u001b[0m                max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     33\u001b[0m                do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m                temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     35\u001b[0m                num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     36\u001b[0m                logits_processor\u001b[38;5;241m=\u001b[39m[autocomplete])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "def simulate_human_selection(choices: [str]) -> int:\n",
    "    import random\n",
    "    selected = random.randrange(len(my_list))\n",
    "    for idx, choice in enumerate(choices):\n",
    "        print(idx, \": \", choice, \" (selected)\" if idx == selected else \"\")\n",
    "    return selected\n",
    "\n",
    "def get_autocomplete_phrases(document: str, typed_so_far: str) -> str:\n",
    "    system_prompt = f\"\"\"\n",
    "        Use the following document to identify a potential continuation\n",
    "        for the given phrase. Provide just the continuation without any preamble.\n",
    "        \n",
    "        <document>\n",
    "        {document}\n",
    "        </document>\n",
    "        \n",
    "        **Phrase to complete**:\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "        {typed_so_far}\n",
    "    \"\"\"\n",
    "\n",
    "    input_message = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}   \n",
    "    ]\n",
    "    \n",
    "    # alliterate on the first letter of the animal. So, donkey would be D\n",
    "    autocomplete = AutocompleteLogitsProcessor(pipe.tokenizer, simulate_human_selection)\n",
    "    \n",
    "    results = pipe(input_message, \n",
    "                   max_new_tokens=16,\n",
    "                   do_sample=True,\n",
    "                   temperature=0.8,\n",
    "                   num_beams=10,\n",
    "                   logits_processor=[autocomplete])\n",
    "    \n",
    "    return results[0]['generated_text'][-1]['content'].strip()\n",
    "\n",
    "choices = get_autocomplete_phrases('\\n'.join(lines), \"Lend\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11412660-ac2e-4925-94d4-957f7bb715ea",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "Little donkey, ears so bright,\n",
    "Hee-hawing loud through day's delight,\n",
    "He trots along with gentle pace,\n",
    "A friendly friend in a sunny place.\n",
    "```\n",
    "Has 3 ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f8396-d5e7-4e2f-b896-c5d4a30b1439",
   "metadata": {},
   "source": [
    "## Combine prompting and sequence selection\n",
    "\n",
    "Enhance the prompt but make it clear we want it to be readable,\n",
    "but use logits processing to prefer words that start with the desired letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586183fd-5b2b-4d6c-a8f2-13641b0cdf37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down the dusty desert, donkey did stray\n",
      "Dreaming of delicious dates to devour each day\n",
      "Dainty donkey danced down the desert way\n"
     ]
    }
   ],
   "source": [
    "def generate_alliterative_poem_v3(animal: str) -> str:\n",
    "    start_letter = animal[0]\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "        You are writing nursery rhymes about animals for a children's book.\n",
    "        Each poem should be 3-5 lines long. The poem must be readable and suitable for children.\n",
    "        Return only the poem, without any introduction or preamble.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "        Write a poem about a {animal} that has a few alliterations involving {start_letter}.\n",
    "        Do not overdo alliteration, and emphasize readability.\n",
    "    \"\"\"\n",
    "\n",
    "    input_message = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}   \n",
    "    ]\n",
    "    \n",
    "    # alliterate on the first letter of the animal. So, donkey would be D\n",
    "    grammar_processor = AlliterativeLogitsProcessor(pipe.tokenizer, start_letter)\n",
    "    \n",
    "    results = pipe(input_message, \n",
    "                   max_new_tokens=256,\n",
    "                   do_sample=True,\n",
    "                   temperature=0.8,\n",
    "                   num_beams=10,\n",
    "                   use_cache=True, # default is True\n",
    "                   logits_processor=[grammar_processor])\n",
    "    return results[0]['generated_text'][-1]['content'].strip()\n",
    "\n",
    "poem = generate_alliterative_poem_v3(\"donkey\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ce7c1-bff2-4186-98d0-8b014f66c854",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "Down the dusty desert, donkey did stray\n",
    "Dreaming of delicious dates to devour each day\n",
    "Dainty donkey danced down the desert way\n",
    "```\n",
    "It's still a readable poem, but we have picked the one with the most ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58154506-ff85-41dc-a436-6364283b9a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
