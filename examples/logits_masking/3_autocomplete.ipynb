{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f4b00e-59c2-4496-a0e5-cd10dff0f0eb",
   "metadata": {},
   "source": [
    "## Autocomplete phrases based on logits (Sequence Selection)\n",
    "\n",
    "Typically, autocomplete is done using past search behavior. An interesting alternative is to use an LLM grounded on the document being searched -- this avoids cold start issues and reduces leakage of senstive data. You can  think of it as a variant of the Sequence Selection in Logits Masking, except that the human typing ends up doing the selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41664a72-3e67-4e57-9e6d-2a01ed7ed8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet transformers torch fbgemm-gpu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b078983c-cda3-46d0-8438-d284bc017b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHANGE this to the Llama model for which you have applied for access via Hugging Face\n",
    "# See: https://www.llama.com/docs/getting-the-models/hugging-face/\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"HF_TOKEN\"][:2] == \"hf\",\\\n",
    "       \"Please sign up for access to the specific Llama model via HuggingFace and provide access token in keys.env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7a518-e7fc-4f71-a492-e9c5d6d15798",
   "metadata": {},
   "source": [
    "## Load document\n",
    "\n",
    "Ideally, this is done only once (such as by using Context Caching or Prompt Caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e87858-16d8-4651-bc94-f997c1ce23ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to julius_caesar.txt\n"
     ]
    }
   ],
   "source": [
    "# Download text of play from Project Gutenberg\n",
    "TXT_URL=\"https://www.gutenberg.org/cache/epub/1522/pg1522.txt\"\n",
    "LOCAL_FILE=\"julius_caesar.txt\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_text_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully to {file_path}\")\n",
    "\n",
    "download_text_file(TXT_URL, LOCAL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048e606b-cc21-4cad-bc72-4972ca073437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = open(LOCAL_FILE).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2588ac22-450c-4cb9-b54a-bb368dd522ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = 0\n",
    "end_index = -1\n",
    "for idx, line in enumerate(lines):\n",
    "    if line.startswith(\"*** START OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "        start_index = idx\n",
    "    if line.startswith(\"*** END OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "        end_index = idx\n",
    "lines = lines[start_index+1:end_index]\n",
    "lines = [line for line in lines if len(line.strip()) > 0]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d262c-edd8-40e7-8441-9cfc8f8dd1f7",
   "metadata": {},
   "source": [
    "## Use logits processing to select the next word\n",
    "\n",
    "Display options to user, take what they provide back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddad926f-2822-47ea-88c5-db42c8316e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessor,\n",
    "    LOGITS_PROCESSOR_INPUTS_DOCSTRING,\n",
    ")\n",
    "from transformers.utils import add_start_docstrings\n",
    "\n",
    "class AutocompleteLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, selection_func):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.selection_func = selection_func\n",
    "      \n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, input_logits: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        output_logits = input_logits.clone()\n",
    "        \n",
    "        decoded = [self.tokenizer.decode(seq) for seq in input_ids]\n",
    "        selected = self.selection_func(decoded) \n",
    "        \n",
    "        # logits goes from -inf to zero.  Mask out everything other than the selected index torch doesn't like it to be -np.inf\n",
    "        for idx in range(len(input_ids)):\n",
    "            if idx != selected:\n",
    "                output_logits[idx] = -10000\n",
    "                  \n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89a8745-2a97-43f1-b819-2fc074d404c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05929d9afcb4492a2465a9d30856abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=MODEL_ID,\n",
    "    use_fast=True,\n",
    "    kwargs={\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "    model_kwargs={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef0ee7d-e7f9-4354-b511-958d53a7ecf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_human_selection(choices: [str]) -> int:\n",
    "    import random\n",
    "    # for simplicity, we'll assume they select something in the list\n",
    "    selected = random.randrange(len(choices))\n",
    "    return selected\n",
    "\n",
    "def get_autocomplete_choice(document_lines: [str], typed_so_far: str) -> str:\n",
    "    # Llama has a very limited context, so let's filter\n",
    "    document = []\n",
    "    for line in document_lines:\n",
    "        if typed_so_far.lower() in line.lower():\n",
    "            document.append(line.strip())\n",
    "    print(f\"Found {len(document)} lines containing {typed_so_far}: {document}\")\n",
    "    document = '\\n'.join(document)\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "        Complete this phrase in a style similar to the ones below. You are acting as auto-complete.\n",
    "        Simply complete the phrase without any introduction or preamble.\n",
    "        Make sure it is only one sentence\n",
    "        \n",
    "        ** Examples **:\n",
    "        {document}\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "        ** Phrase **:\n",
    "        {typed_so_far} ____\n",
    "    \"\"\"\n",
    "\n",
    "    input_message = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}   \n",
    "    ]\n",
    "    \n",
    "    # get autocompletion selection\n",
    "    autocomplete = AutocompleteLogitsProcessor(pipe.tokenizer, simulate_human_selection)\n",
    "    \n",
    "    results = pipe(input_message, \n",
    "                   max_new_tokens=256,\n",
    "                   do_sample=True,\n",
    "                   temperature=0.8,\n",
    "                   num_beams=5,\n",
    "                   logits_processor=[autocomplete])\n",
    "    \n",
    "    return results[0]['generated_text'][-1]['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdb111f-6625-45b7-b744-c19e93c51bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 lines containing Lend: ['Look in the calendar, and bring me word.', 'Lend me your hand.', 'Friends, Romans, countrymen, lend me your ears;']\n",
      "your hearts\n"
     ]
    }
   ],
   "source": [
    "typed_text = \"Lend\"\n",
    "choice = get_autocomplete_choice(lines, typed_text)\n",
    "print(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58154506-ff85-41dc-a436-6364283b9a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
