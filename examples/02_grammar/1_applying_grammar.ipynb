{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f4b00e-59c2-4496-a0e5-cd10dff0f0eb",
   "metadata": {},
   "source": [
    "## Filtering logprobs based on a Grammar specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c71c5a8-b449-4864-9325-b2f1643f1fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet transformers-cfg outlines datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b078983c-cda3-46d0-8438-d284bc017b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7717ded-01ba-4fa2-b1c4-44b342cc0ab6",
   "metadata": {},
   "source": [
    "## Use Grammar to ensure that only an arithmetic expression gets generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cad70f-b88b-4b35-98fd-d9be4b64f268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7864f9e008f426f95f09f69157f852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers_cfg.grammar_utils import IncrementalGrammarConstraint\n",
    "from transformers_cfg.generation.logits_process import GrammarConstrainedLogitsProcessor\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=MODEL_ID,\n",
    " #   tokenizer=tokenizer,\n",
    "    kwargs={\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "    model_kwargs={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d864a57-d022-4783-884e-dacd778be4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_expression_that_solves(math_problem: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You are a math instructor. I will ask you a math question.\n",
    "    Respond with the mathematical expression that can be used to solve the problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the grammar\n",
    "    grammar_str = \"\"\"\n",
    "root  ::= (expr \"=\" ws term \"\\n\")+\n",
    "expr  ::= term ([-+*/] term)*\n",
    "term  ::= ident | num | \"(\" ws expr \")\" ws\n",
    "ident ::= [a-z] [a-z0-9_]* ws\n",
    "num   ::= [0-9]+ ws\n",
    "ws    ::= [ \\t\\n]*\n",
    "    \"\"\"\n",
    "    grammar = IncrementalGrammarConstraint(grammar_str, \"root\", pipe.tokenizer)\n",
    "    grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n",
    "\n",
    "    input_message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": math_problem}   \n",
    "    ]\n",
    "\n",
    "    results = pipe(input_message, \n",
    "                   max_new_tokens=256, \n",
    "                   do_sample=False, \n",
    "                   logits_processor=[grammar_processor])\n",
    "    return results[0]['generated_text'][-1]['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f9785d-76c0-4034-a860-23bb32fa7b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill_apples +mae_apples = total_apples\n",
      "\n",
      "3 +2 = 5\n"
     ]
    }
   ],
   "source": [
    "result = get_expression_that_solves(\"\"\"\n",
    "Bill has 3 apples and 2 oranges.\n",
    "Mae has 2 apples and 4 oranges.\n",
    "How many total apples do Bill and Mae have?\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4c7a-468d-4bba-9e59-b11df223ac8b",
   "metadata": {},
   "source": [
    "Our result:\n",
    "```\n",
    "bill_apples +mae_apples = total_apples\n",
    "\n",
    "3 +2 = 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae69d720-3343-4be2-be66-8c0c9a80898d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 +2 = 3\n",
      "2 +4 = 6\n",
      "\n",
      "3 = 3\n",
      "6 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 = 6\n",
      "\n",
      "3 =\n"
     ]
    }
   ],
   "source": [
    "# here, the expression is (3+2) > (2+4), which is not allowed by our grammar which expects equality\n",
    "result = get_expression_that_solves(\"\"\"\n",
    "Bill has 3 apples and 2 oranges.\n",
    "Mae has 2 apples and 4 oranges.\n",
    "Do Bill and Mae have more apples than oranges?\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96249629-ffbd-4771-b87c-4568e0aaca84",
   "metadata": {},
   "source": [
    "Our result:\n",
    "```\n",
    "1 +2 = 3\n",
    "2 +4 = 6\n",
    "\n",
    "3 = 3\n",
    "...\n",
    "```\n",
    "Obviously, this is a problem if your grammar is too limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb27476-f093-450e-8d20-e841b397d002",
   "metadata": {},
   "source": [
    "## Use Grammar to specify a format + validation\n",
    "\n",
    "We want:\n",
    "author | title | year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2129434-3a07-486a-b098-3bf28918f749",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gabriel Garcia Marquez | Love in the Time of Cholera |1985'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_book_info(paragraph: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You will be given a short paragraph about a book.\n",
    "    Extract the author, title, and publication year of the book.\n",
    "    Return the result as author | title | year\n",
    "    If any piece of information is not found, fill the spot with NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the grammar\n",
    "    grammar_str = \"\"\"\n",
    "record ::= author separator title separator year\n",
    "author ::= [a-zA-Z ]* | unk\n",
    "title ::= [a-zA-Z ]* | unk\n",
    "year ::= [1-2][0-9][0-9][0-9] | unk\n",
    "unk ::= \"NULL\"\n",
    "separator ::= \"|\"\n",
    "    \"\"\"\n",
    "    grammar = IncrementalGrammarConstraint(grammar_str, \"record\", pipe.tokenizer)\n",
    "    grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n",
    "\n",
    "    input_message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": paragraph}   \n",
    "    ]\n",
    "\n",
    "    results = pipe(input_message, \n",
    "                   max_new_tokens=256, \n",
    "                   do_sample=False, \n",
    "                   logits_processor=[grammar_processor])\n",
    "    return results[0]['generated_text'][-1]['content'].strip()\n",
    "\n",
    "\n",
    "parse_book_info(\"\"\"\n",
    "Love in the Time of Cholera (Spanish: El amor en los tiempos del cólera) is a novel written in Spanish\n",
    "by Colombian Nobel Prize-winning author Gabriel García Márquez and published in 1985.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28e745-0379-4e9e-bf10-e850901ed8c1",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "Gabriel Garcia Marquez | Love in the Time of Cholera |1985\n",
    "```\n",
    "Note that accents have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16f8fca-de55-416d-a58f-9ced8d21ed39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valluvar | The Tirukkural |NULL'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_book_info(\"\"\"\n",
    "The Tirukkural (Tamil: திருக்குறள், lit. 'sacred verses')\n",
    "is a classic Tamil language text whose authorship is traditionally attributed to Valluvar,\n",
    "also known in full as Thiruvalluvar. The text has been dated variously from 300 BCE to 5th century CE. \n",
    "The traditional accounts describe it as the last work of the third Sangam, but linguistic analysis\n",
    "suggests a later date of 450 to 500 CE and that it was composed after the Sangam period.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd3f11-4372-4b0a-95a9-8e1ae568b6c7",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "Valluvar | The Tirukkural |NULL\n",
    "```\n",
    "Note the use of NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd00b3-f734-41cf-9644-137db8f82aca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Efficient Generation using Outlines\n",
    "\n",
    "See: https://github.com/dottxt-ai/outlines\n",
    "and\n",
    "https://arxiv.org/pdf/2307.09702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cbf83b-acc6-4789-8743-b10cca9ce19d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7044bd856371475cb9274818b21f3bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import outlines\n",
    "model = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e9f291-5d7a-460a-8575-f5e4b0f49faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You need to specify either `text` or `text_target`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     structured \u001b[38;5;241m=\u001b[39m generator(convert_message_to_prompt(input_message), max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m structured\n\u001b[0;32m---> 34\u001b[0m \u001b[43mparse_book_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43mLove in the Time of Cholera (Spanish: El amor en los tiempos del cólera) is a novel written in Spanish\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43mby Colombian Nobel Prize-winning author Gabriel García Márquez and published in 1985.\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mparse_book_info\u001b[0;34m(paragraph)\u001b[0m\n\u001b[1;32m     21\u001b[0m input_message \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     23\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: paragraph}   \n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     26\u001b[0m generator \u001b[38;5;241m=\u001b[39m outlines\u001b[38;5;241m.\u001b[39mgenerate\u001b[38;5;241m.\u001b[39mregex(\n\u001b[1;32m     27\u001b[0m     model,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([a-zA-Z ]+|NULL) \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m| ([a-zA-Z ]+|NULL) \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m| ([1-2][0-9][0-9][0-9]|NULL)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moutlines\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mgreedy(),\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m structured \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_message_to_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_message\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m structured\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/outlines/generate/api.py:490\u001b[0m, in \u001b[0;36mSequenceGeneratorAdapter.__call__\u001b[0;34m(self, prompts, max_tokens, stop_at, seed, **model_specific_params)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt of list of prompts.\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m generation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_generation_parameters(\n\u001b[1;32m    487\u001b[0m     max_tokens, stop_at, seed\n\u001b[1;32m    488\u001b[0m )\n\u001b[0;32m--> 490\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_specific_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format(completions)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/outlines/models/transformers.py:229\u001b[0m, in \u001b[0;36mTransformers.generate\u001b[0;34m(self, prompts, generation_parameters, logits_processor, sampling_parameters)\u001b[0m\n\u001b[1;32m    227\u001b[0m     input_ids, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode([prompts])\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     input_ids, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    234\u001b[0m }\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    238\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/outlines/models/transformers.py:87\u001b[0m, in \u001b[0;36mTransformerTokenizer.encode\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     86\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 87\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2862\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m all_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either `text` or `text_target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2864\u001b[0m     \u001b[38;5;66;03m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m     \u001b[38;5;66;03m# input mode in this case.\u001b[39;00m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n",
      "\u001b[0;31mValueError\u001b[0m: You need to specify either `text` or `text_target`."
     ]
    }
   ],
   "source": [
    "def convert_message_to_prompt(messages):\n",
    "    prompt=\"\"\n",
    "    for message in messages:\n",
    "        prompt += f\"\"\"\n",
    "<|im_start|>{message['role']}\n",
    "{message['content']}\n",
    "<|im_end|>\n",
    "        \"\"\"\n",
    "    # add the response\n",
    "    prompt += \"\"\"\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    \n",
    "def parse_book_info(paragraph: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You will be given a short paragraph about a book.\n",
    "    Extract the author, title, and publication year of the book.\n",
    "    Return the result as author | title | year\n",
    "    If any piece of information is not found, fill the spot with NULL\n",
    "    \"\"\"\n",
    "    input_message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": paragraph}   \n",
    "    ]\n",
    "\n",
    "    generator = outlines.generate.regex(\n",
    "        model,\n",
    "        r\"([a-zA-Z ]+|NULL) \\| ([a-zA-Z ]+|NULL) \\| ([1-2][0-9][0-9][0-9]|NULL)\",\n",
    "        sampler=outlines.samplers.greedy(),\n",
    "    )\n",
    "    structured = generator(convert_message_to_prompt(input_message), max_tokens=30)\n",
    "    return structured\n",
    "\n",
    "parse_book_info(\"\"\"\n",
    "Love in the Time of Cholera (Spanish: El amor en los tiempos del cólera) is a novel written in Spanish\n",
    "by Colombian Nobel Prize-winning author Gabriel García Márquez and published in 1985.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23298400-0ab9-485f-958c-43ebd12f89c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
