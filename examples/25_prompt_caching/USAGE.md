## Usage

Documented uses of prompt caching in the wild:
* [Notion](https://www.anthropic.com/news/prompt-caching) uses prompt caching with Claude to make Notion AI faster and cheaper while maintaining state-of-the-art quality, optimizing internal operations and creating a more responsive user experience.
* [Anthropic](https://www.anthropic.com/news/prompt-caching) provides comprehensive examples including conversational agents, coding assistants, large document processing, and agentic search - showing up to 90% cost reduction and 85% latency improvement for long prompts.
* [OpenAI](https://humanloop.com/blog/prompt-caching) offers automatic prompt caching for prompts over 1,024 tokens, providing up to 80% latency reduction and 50% cost savings for applications like coding assistants and customer support chatbots.

-------
To contribute a use case, submit a pull request. Make sure to link to a publicly accessible blog post or article that has the relevant technical details.
