{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62246b6b-17c0-4387-b27c-8a435e7b93d5",
   "metadata": {},
   "source": [
    "## Using Direct Preference Optimization on a preference dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c3366b-dd37-4526-aeb7-7cfc7a7b92ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --quiet transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e407f98-cc34-44f7-952c-96b1c0f83ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\"   # has to match generate_synthetic_data so that data is in-distribution\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"HF_TOKEN\"][:2] == \"hf\",\\\n",
    "       \"Please sign up for access to the specific Llama model via HuggingFace and provide access token in keys.env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c55dd0d-3df9-43ad-aa88-bd2776265b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d0d76332724db9bd271d9c6ba2b0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=\"ad_preference_dataset.jsonl\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f330a81-16c4-402b-babe-25db0b24ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ce0421-1cd1-4333-8340-b61682c5ba2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'llama' in MODEL_ID:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aab82da-ce2e-41a1-b405-8ccd2e0b5648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d754dabf4f4a749ee2f10f2959bb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ae1d8634db4c4b8583c0f2a6923ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871cfcf7a2e436f98b90f931659b201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.49373050103895366, metrics={'train_runtime': 127.5862, 'train_samples_per_second': 2.351, 'train_steps_per_second': 2.351, 'total_flos': 0.0, 'train_loss': 0.49373050103895366, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = DPOConfig(output_dir=\"ClassifiedAds-DPO\", logging_steps=10, \n",
    "                          per_device_train_batch_size=1)\n",
    "trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacba1dd-f9e4-4f89-a434-9b87812e3edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc110254-aa2f-46c5-a927-71866c91bd71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try out the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1261190f-a4d3-4106-8459-9d3445f4549f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ClassifiedAds-DPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dca6ef6-4eba-40fd-acba-50063ec05df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if 'llama' in MODEL_ID:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79e7bd4-d78a-4e8b-a97d-b09169a96fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb554a31-53ae-4c3c-bc01-d7e39be60c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are writing a one paragraph classified ad for used items in a neighborhood online forum. Write an ad to sell a used copy of the book Pachinko by Jin Min Lee for $5. The price includes shipping.\\n\"Find used copy of Pachinko by Jin Min Lee for $5! Shipping included. Available through our community forum. Need it by September 1st.\" \\n\\nIn this ad, you must have also provided details like title, author, genre, and any relevant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are writing a one paragraph classified ad for used items in a neighborhood online forum. Write an ad to sell a \"\n",
    "for_sale = \"used copy of the book Pachinko by Jin Min Lee for $5\"\n",
    "generator(system_prompt + for_sale, \n",
    "          max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ff8bf-c480-422e-ad35-64f2e5a0e284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
