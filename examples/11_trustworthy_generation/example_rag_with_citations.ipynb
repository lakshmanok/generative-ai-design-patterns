{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import our fixed embeddings class\n",
    "from fixed_embedding import FixedHuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "assert os.environ[\"OPENAI_API_KEY\"][:2] == \"sk\",\\\n",
    "       \"Please sign up for access to the OpenAI API and provide access token in keys.env file\"\n",
    "\n",
    "# Create a language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory_path):\n",
    "    \"\"\"Load documents from a directory.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory_path,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"Split documents into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks):\n",
    "    \"\"\"Create a vector store from document chunks using the fixed embeddings.\"\"\"\n",
    "    # Using fixed HuggingFace embeddings that can handle non-string inputs\n",
    "    embeddings = FixedHuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # Create and persist the vector store\n",
    "    persist_directory = \"chroma_db\"\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(f\"Created and persisted vector store to {persist_directory}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store():\n",
    "    \"\"\"Load an existing vector store using fixed embeddings.\"\"\"\n",
    "    embeddings = FixedHuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    persist_directory = \"chroma_db\"\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of sources with identifiers\n",
    "def create_source_references(docs):\n",
    "    sources = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # Create a source identifier\n",
    "        source_id = i + 1\n",
    "        source = {\n",
    "            \"id\": source_id,\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        }\n",
    "        sources.append(source)\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a prompt that includes properly formatted sources\n",
    "def format_sources_for_prompt(sources):\n",
    "    formatted_sources = \"\"\n",
    "    for source in sources:\n",
    "        source_id = source[\"id\"]\n",
    "        source_info = f\"[{source_id}] \"\n",
    "\n",
    "        # Add metadata if available\n",
    "        if \"metadata\" in source and \"source\" in source[\"metadata\"]:\n",
    "            source_info += f\"From: {source['metadata']['source']}\\n\"\n",
    "        else:\n",
    "            source_info += \"Source document\\n\"\n",
    "\n",
    "        # Add content preview\n",
    "        content_preview = source[\"content\"][:200] + \"...\" if len(source[\"content\"]) > 200 else source[\"content\"]\n",
    "        source_info += f\"Content: {content_preview}\\n\\n\"\n",
    "\n",
    "        formatted_sources += source_info\n",
    "\n",
    "    return formatted_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scientific_rag_pipeline(vectorstore):\n",
    "    \"\"\"Create a RAG pipeline with in-line scientific-style citations.\"\"\"\n",
    "    # Create a retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}  # Retrieve top 4 chunks\n",
    "    )\n",
    "\n",
    "    # Create a scientific citation prompt template\n",
    "    template = \"\"\"Answer the question based on the following sources, using in-line citations like in scientific papers.\n",
    "\n",
    "    SOURCES:\n",
    "    {sources}\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Use information only from the provided sources\n",
    "    2. Provide an answer with in-line citations using brackets, e.g., \"Einstein developed the theory of relativity [1]\"\n",
    "    3. Use the source ID in brackets [1], [2], etc., corresponding to the source number\n",
    "    4. Cite ALL facts with their source IDs\n",
    "    5. If multiple sources support a fact, you can include multiple citations [1][2]\n",
    "    6. If the information isn't in the sources, say \"I don't have enough information to answer this question\"\n",
    "    7. Include a \"References\" section at the end listing all the sources you cited\n",
    "\n",
    "    Your answer should be comprehensive, accurate, and include citations for all factual claims.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a retriever chain that returns document references\n",
    "    retriever_chain = retriever | create_source_references\n",
    "\n",
    "    # Create the RAG chain using RunnableParallel for the inputs\n",
    "    input_processor = RunnableParallel(\n",
    "        sources=retriever_chain,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "\n",
    "    # Create the full chain\n",
    "    rag_chain = (\n",
    "        input_processor\n",
    "        | (lambda x: {\"sources\": format_sources_for_prompt(x[\"sources\"]), \"question\": x[\"question\"]})\n",
    "        | PromptTemplate.from_template(template)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_session():\n",
    "    \"\"\"Run an interactive session with the scientific RAG system.\"\"\"\n",
    "    print(\"Loading scientific RAG system with in-line citations...\")\n",
    "\n",
    "    # Make sure we have an API key set\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        api_key = input(\"Please enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    try:\n",
    "        # Check if we should create a new vector store or load an existing one\n",
    "        if not os.path.exists(\"chroma_db\"):\n",
    "            directory_path = \"raw_texts/\"\n",
    "            if not os.path.exists(directory_path) or len(os.listdir(directory_path)) == 0:\n",
    "                print(f\"ERROR: {directory_path} directory doesn't exist or is empty\")\n",
    "                return\n",
    "\n",
    "            print(\"Creating new vector store from documents...\")\n",
    "            documents = load_documents(directory_path)\n",
    "            chunks = split_documents(documents)\n",
    "            vectorstore = create_vector_store(chunks)\n",
    "        else:\n",
    "            print(\"Loading existing vector store...\")\n",
    "            vectorstore = load_vector_store()\n",
    "\n",
    "        # Create the RAG pipeline\n",
    "        rag_chain = create_scientific_rag_pipeline(vectorstore)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nScientific RAG System loaded successfully!\")\n",
    "        print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            return\n",
    "\n",
    "        # Process the question\n",
    "        print(\"\\nProcessing your question...\")\n",
    "        try:\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(\"\\n\" + answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scientific RAG system with in-line citations...\n",
      "Loading existing vector store...\n",
      "\n",
      "Scientific RAG System loaded successfully!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Wolfgang Amadeus Mozart was a prolific composer known for his numerous theatrical works, including operas. He was known for composing pieces that were performed on stage by vocalists singing in character [3]. Despite alterations and mutilation of some of his operas in earlier performances, productions in the past century have tended to follow his original compositions more faithfully [1]. One example of his work is \"The Magic Flute,\" which was well-received but did not have reviews of its first performances [4]. Additionally, \"The Magic Flute\" has been noted to have references to the music of Antonio Salieri, such as the similarity between a Papageno-Papagena duet and a cavatina in one of Salieri's works [2].\n",
      "\n",
      "Therefore, Wolfgang Amadeus Mozart was a renowned composer of theatrical works, particularly operas, whose compositions have stood the test of time and continue to be performed today.\n",
      "\n",
      "References:\n",
      "[1]: raw_texts/mozart_the-magic-flute.txt\n",
      "[2]: raw_texts/mozart_the-magic-flute.txt\n",
      "[3]: raw_texts/mozart_list-of-operas-by-wolfgang-amadeus-mozart.txt\n",
      "[4]: raw_texts/mozart_the-magic-flute.txt\n",
      "\n",
      "Scientific RAG System loaded successfully!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "The difference between Mozart and Beethoven lies in their approach to composition, particularly in the genre of opera. Beethoven's opera \"Fidelio\" demonstrates his unique style, incorporating elements such as the singspiel form, domestic realism tinged with comedy, a heroic or patriotic storyline involving violence, and often a spectacular catastrophe, along with a happy ending [1]. Beethoven's opera \"Fidelio\" had a long and complicated history of composition, with Beethoven revising it three times and vowing never to compose another opera due to the vexation it caused him [3].\n",
      "\n",
      "In contrast, Mozart's operas are known for their distinct style and elegance, showcasing his mastery of the form. While both composers made significant contributions to operatic composition, Mozart's operas are often characterized by their lyrical beauty, emotional depth, and intricate character development, setting them apart from Beethoven's more dramatic and intense approach [4].\n",
      "\n",
      "Overall, the key difference between Mozart and Beethoven in terms of opera composition lies in their stylistic choices and the unique elements they each brought to the genre.\n",
      "\n",
      "References:\n",
      "[1] From: raw_texts/beethoven_fidelio.txt\n",
      "[3] From: raw_texts/beethoven_fidelio.txt\n",
      "[4] From: raw_texts/beethoven_fidelio.txt\n",
      "\n",
      "Scientific RAG System loaded successfully!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Based on the provided sources, Beethoven's Symphony No. 9 includes a choral section in German, \"Ihr stürzt nieder, Millionen? Ahnest du den Schöpfer, Welt? Such' ihn über'm Sternenzelt! Über Sternen muß er wohnen\" [2]. This choral section is a key element in the ninth symphony. Additionally, the symphony is known for including strings [1]. \n",
      "\n",
      "In Bach's St. Matthew Passion, Part One opens with the chorus \"Kommt, ihr Töchter, helft mir klagen\" [3]. This demonstrates the use of chorus in Bach's composition to convey a message at the beginning of the piece. \n",
      "\n",
      "In Mozart's The Magic Flute, there is a scene where Pamina is contemplating suicide because she believes Tamino has abandoned her. However, the three boys restrain her and reassure her of Tamino's love [4]. This scene highlights the emotional depth and complexity of the characters in Mozart's opera.\n",
      "\n",
      "Overall, these sources showcase the importance of choral elements, string instruments, and emotional storytelling in the works of Beethoven, Bach, and Mozart.\n",
      "\n",
      "References:\n",
      "[1] raw_texts/beethoven_symphony-no.-9-beethoven.txt\n",
      "[2] raw_texts/beethoven_symphony-no.-9-beethoven.txt\n",
      "[3] raw_texts/bach_st-matthew-passion.txt\n",
      "[4] raw_texts/mozart_the-magic-flute.txt\n",
      "\n",
      "Scientific RAG System loaded successfully!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_interactive_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
