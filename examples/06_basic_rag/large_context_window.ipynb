{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a56bb7-e753-4731-84ed-cdc1084f57e7",
   "metadata": {},
   "source": [
    "## Do you really need a RAG?\n",
    "\n",
    "If you have a document that is smaller than the context window of the model, you can avoid the chunking/retrieval step, and simply put the entire document into the context of the prompt.\n",
    "\n",
    "Illustrated here using Barack Obama's 2015 tax return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e939628-0665-4fff-b8af-d128c7575359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaacc11-e131-4170-ad1b-42bdf2e01b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GEMINI=\"gemini-2.0-flash-001\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"GEMINI_API_KEY\"][:2] == \"AI\",\\\n",
    "       \"Please specify the GEMINI_API_KEY access token in keys.env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecab8a-4bdc-4374-8c65-4fc773fc3780",
   "metadata": {},
   "source": [
    "## Cache President Obama's tax return\n",
    "\n",
    "It has long been a bipartisan tradition for US candidates for high office to release their tax returns.\n",
    "Here, we download the return and upload it into Gemini's cache. This way, we don't need to keep\n",
    "sending it the data (see the Prompt Caching pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4dda2-3a3d-4fac-9539-b4b9a9b9ea65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cache_pdf(pdf_path: str, \n",
    "              model_id: str = GEMINI,\n",
    "              system_instruction: str = \"You are a tax attorney\") -> str:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    import io, httpx\n",
    "\n",
    "    client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])\n",
    "    doc_io = io.BytesIO(httpx.get(pdf_path).content)\n",
    "    document = client.files.upload(\n",
    "      file=doc_io,\n",
    "      config=dict(mime_type='application/pdf')\n",
    "    )\n",
    "    # Create a cached content object\n",
    "    cache = client.caches.create(\n",
    "        model=model_id,\n",
    "        config=types.CreateCachedContentConfig(\n",
    "          system_instruction=system_instruction,\n",
    "          contents=[document],\n",
    "        )\n",
    "    )\n",
    "    # Display the cache details\n",
    "    print(f'{cache=}')\n",
    "    return cache.name\n",
    "\n",
    "cache_name = cache_pdf(pdf_path=\"https://s3.amazonaws.com/pdfs.taxnotes.com/2019/B_Obama_2014.pdf\")\n",
    "print(cache_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608a336-8086-442f-80fa-fa26e38c8def",
   "metadata": {},
   "source": [
    "## Find the document in the cache\n",
    "Here, we cached only one document, so I'll just use that one. Normally, you'd have some other way to track which one you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebc7441-d75e-4138-9929-babb1effdc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='cachedContents/wc0yofrcdjiv47at07bxfy9i0o9qyyimg6suw0pn' display_name='' model='models/gemini-2.0-flash-001' create_time=datetime.datetime(2025, 6, 1, 17, 2, 54, 807005, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 6, 1, 17, 2, 54, 807005, tzinfo=TzInfo(UTC)) expire_time=datetime.datetime(2025, 6, 1, 18, 2, 53, 965026, tzinfo=TzInfo(UTC)) usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=9811, video_duration_seconds=None)\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])\n",
    "for cache in client.caches.list():\n",
    "  print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751e903-4668-4ef1-9256-090cfb0cb43f",
   "metadata": {},
   "source": [
    "No chunking necessary since the whole document is only 9811 tokens, which is well within the context window supported by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235c1e2e-d327-43d8-8cc6-3059d6299acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cachedContents/wc0yofrcdjiv47at07bxfy9i0o9qyyimg6suw0pn\n"
     ]
    }
   ],
   "source": [
    "cache_name = client.caches.list()[0].name\n",
    "print(cache_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245eae48-bb1f-44b5-9b58-282718360c40",
   "metadata": {},
   "source": [
    "## Queries that use cached content\n",
    "\n",
    "Each of these prompts uses the full PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55751bec-6462-44af-94e2-aa5f004dd0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=7), ModalityTokenCount(modality=<MediaModality.DOCUMENT: 'DOCUMENT'>, token_count=9804)], cached_content_token_count=9811, candidates_token_count=25, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=25)], prompt_token_count=9820, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.DOCUMENT: 'DOCUMENT'>, token_count=9804), ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=16)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=9845, traffic_type=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the Schedule C form provided, Obama claimed $6,708 in business expenses (line 28).'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate content using the cached prompt and document\n",
    "def answer_question(prompt: str, cached_tax_return: str) -> str:\n",
    "    response = client.models.generate_content(\n",
    "      model=GEMINI,\n",
    "      contents=prompt,\n",
    "      config=types.GenerateContentConfig(\n",
    "        cached_content=cached_tax_return\n",
    "      ))\n",
    "    print(f'{response.usage_metadata=}')\n",
    "    return response.text\n",
    "\n",
    "answer_question(\"How much did Obama claim in business expenses?\", cache_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183041d4-5683-46f4-a388-57aea940422e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=7), ModalityTokenCount(modality=<MediaModality.DOCUMENT: 'DOCUMENT'>, token_count=9804)], cached_content_token_count=9811, candidates_token_count=43, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=43)], prompt_token_count=9819, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=15), ModalityTokenCount(modality=<MediaModality.DOCUMENT: 'DOCUMENT'>, token_count=9804)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=9862, traffic_type=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, according to line 28 on form 1040, Obama made self-employed SEP, SIMPLE, and qualified plans contributions. The amount of contribution was $17,400.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Did Obama make any retirement plan contributions?\", cache_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74485947-fa65-4a86-af4e-133626042b3a",
   "metadata": {},
   "source": [
    "Note that the latter query used 9862 total tokens of which 9811 came from the cache"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
