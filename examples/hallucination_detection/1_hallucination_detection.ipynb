{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Hallucinations in LLM Responses through Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "assert os.environ[\"OPENAI_API_KEY\"][:2] == \"sk\", \"Please sign up for access to the OpenAI API and provide access token in keys.env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logprobs(message):\n",
    "    logprobs = message.choices[0].logprobs\n",
    "\n",
    "    if not logprobs:\n",
    "        print(\"No logprobs available in the response\")\n",
    "        return\n",
    "\n",
    "    # Print each token and its probability\n",
    "    print(\"\\nToken-by-token analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    for token_info in logprobs.content:\n",
    "        token = token_info.token\n",
    "        logprob = token_info.logprob\n",
    "        probability = round(100 * (2.718281828459045 ** logprob), 2)\n",
    "\n",
    "        print(f\"Token: {token!r}\")\n",
    "        print(f\"Log Probability: {logprob:.4f}\")\n",
    "        print(f\"Probability: {probability}%\")\n",
    "\n",
    "        # If top logprobs are available, show alternatives\n",
    "        if token_info.top_logprobs:\n",
    "            print(\"Top alternatives:\")\n",
    "            for alt_token in token_info.top_logprobs:\n",
    "                if alt_token.token != token:\n",
    "                    alt_probability = round(100 * (2.718281828459045 ** alt_token.logprob), 2)\n",
    "                    print(f\"  {alt_token.token!r}: {alt_probability}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def analyze_token_confidence(logprobs):\n",
    "    \"\"\"Analyze the model's confidence in its predictions\"\"\"\n",
    "    if not logprobs or not hasattr(logprobs, 'content'):\n",
    "        print(\"Debug: logprobs structure:\", logprobs)  # Debug print\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        confidences = [2.718281828459045 ** lp.logprob for lp in logprobs.content]\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        min_confidence = min(confidences)\n",
    "        max_confidence = max(confidences)\n",
    "\n",
    "        print(\"\\nConfidence Analysis:\")\n",
    "        print(f\"Average confidence: {avg_confidence:.2%}\")\n",
    "        print(f\"Minimum confidence: {min_confidence:.2%}\")\n",
    "        print(f\"Maximum confidence: {max_confidence:.2%}\")\n",
    "\n",
    "        # Find tokens with unusually low confidence\n",
    "        threshold = avg_confidence * 0.5  # 50% of average confidence\n",
    "        low_confidence_tokens = [\n",
    "            (lp.token, 2.718281828459045 ** lp.logprob)\n",
    "            for lp in logprobs.content\n",
    "            if 2.718281828459045 ** lp.logprob < threshold\n",
    "        ]\n",
    "\n",
    "        if low_confidence_tokens:\n",
    "            print(\"\\nTokens with unusually low confidence:\")\n",
    "            for token, conf in low_confidence_tokens:\n",
    "                print(f\"Token: {token!r}, Confidence: {conf:.2%}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Debug: Error processing logprobs: {e}\")\n",
    "        print(f\"Debug: logprobs type: {type(logprobs)}\")\n",
    "        print(f\"Debug: logprobs content: {logprobs}\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_response_confidence(logprobs):\n",
    "    \"\"\"Calculate an overall confidence score for the response.\n",
    "    Returns a score between 0 and 1, where:\n",
    "    - 1 indicates very high confidence\n",
    "    - 0 indicates very low confidence\n",
    "    \"\"\"\n",
    "    if not logprobs:\n",
    "        return None\n",
    "\n",
    "    # Convert logprobs to probabilities\n",
    "    confidences = [2.718281828459045 ** lp.logprob for lp in logprobs.content]\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    min_confidence = min(confidences)\n",
    "\n",
    "    # Weight both average and minimum confidence in the final score\n",
    "    # This helps catch both overall low confidence and individual uncertain tokens\n",
    "    confidence_score = (0.7 * avg_confidence) + (0.3 * min_confidence)\n",
    "\n",
    "    return round(confidence_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token-by-token analysis:\n",
      "--------------------------------------------------\n",
      "Token: 'Cl'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' Clara': 0.0%\n",
      "  '\\n': 0.0%\n",
      "  '-': 0.0%\n",
      "  '**': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: 'ara'\n",
      "Log Probability: 0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  'are': 0.0%\n",
      "  'aras': 0.0%\n",
      "  'arah': 0.0%\n",
      "  'aris': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' Sch'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' Wie': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ' Sh': 0.0%\n",
      "  ' Joseph': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: 'umann'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  'um': 0.0%\n",
      "  'uman': 0.0%\n",
      "  'uh': 0.0%\n",
      "  'u': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' was'\n",
      "Log Probability: -0.0004\n",
      "Probability: 99.96%\n",
      "Top alternatives:\n",
      "  ',': 0.03%\n",
      "  ' (': 0.01%\n",
      "  ' is': 0.01%\n",
      "  ' ': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' a'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' an': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ' one': 0.0%\n",
      "  ' German': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' renowned'\n",
      "Log Probability: -0.4789\n",
      "Probability: 61.94%\n",
      "Top alternatives:\n",
      "  ' German': 16.7%\n",
      "  ' highly': 12.18%\n",
      "  ' prominent': 3.15%\n",
      "  ' celebrated': 2.11%\n",
      "--------------------------------------------------\n",
      "Token: ' German'\n",
      "Log Probability: -0.1835\n",
      "Probability: 83.24%\n",
      "Top alternatives:\n",
      "  ' ': 9.55%\n",
      "  ' pian': 6.72%\n",
      "  ' Romantic': 0.25%\n",
      "  ' composer': 0.16%\n",
      "--------------------------------------------------\n",
      "Token: ' pian'\n",
      "Log Probability: -0.0518\n",
      "Probability: 94.95%\n",
      "Top alternatives:\n",
      "  ' composer': 3.87%\n",
      "  ' musician': 1.16%\n",
      "  ' Romantic': 0.0%\n",
      "  ' classical': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: 'ist'\n",
      "Log Probability: 0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  'ists': 0.0%\n",
      "  'is': 0.0%\n",
      "  ' ist': 0.0%\n",
      "  'st': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' and'\n",
      "Log Probability: -0.0208\n",
      "Probability: 97.94%\n",
      "Top alternatives:\n",
      "  ',': 2.06%\n",
      "  ' of': 0.0%\n",
      "  ' who': 0.0%\n",
      "  ' considered': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' composer'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' composers': 0.0%\n",
      "  'composer': 0.0%\n",
      "  ' one': 0.0%\n",
      "  ' a': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' of'\n",
      "Log Probability: -0.2232\n",
      "Probability: 79.99%\n",
      "Top alternatives:\n",
      "  ' from': 11.96%\n",
      "  ' in': 7.27%\n",
      "  ' during': 0.42%\n",
      "  ',': 0.21%\n",
      "--------------------------------------------------\n",
      "Token: ' the'\n",
      "Log Probability: -0.0001\n",
      "Probability: 99.99%\n",
      "Top alternatives:\n",
      "  ' Romantic': 0.0%\n",
      "  ' romantic': 0.0%\n",
      "  ' classical': 0.0%\n",
      "  ' ': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' Romantic'\n",
      "Log Probability: -0.0023\n",
      "Probability: 99.77%\n",
      "Top alternatives:\n",
      "  ' ': 0.22%\n",
      "  ' romantic': 0.01%\n",
      "  ' early': 0.0%\n",
      "  ' nineteenth': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' era'\n",
      "Log Probability: -0.0015\n",
      "Probability: 99.85%\n",
      "Top alternatives:\n",
      "  ' period': 0.13%\n",
      "  ' Era': 0.01%\n",
      "  '-era': 0.0%\n",
      "  ' Period': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ','\n",
      "Log Probability: -0.2163\n",
      "Probability: 80.55%\n",
      "Top alternatives:\n",
      "  '.': 17.39%\n",
      "  ' who': 1.26%\n",
      "  ' and': 0.58%\n",
      "  ' known': 0.21%\n",
      "--------------------------------------------------\n",
      "Token: ' known'\n",
      "Log Probability: -0.0579\n",
      "Probability: 94.37%\n",
      "Top alternatives:\n",
      "  ' as': 1.81%\n",
      "  ' and': 1.54%\n",
      "  ' considered': 0.76%\n",
      "  ' best': 0.45%\n",
      "--------------------------------------------------\n",
      "Token: ' for'\n",
      "Log Probability: -0.0004\n",
      "Probability: 99.96%\n",
      "Top alternatives:\n",
      "  ' not': 0.01%\n",
      "  ' especially': 0.01%\n",
      "  ' particularly': 0.01%\n",
      "  ' as': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' her'\n",
      "Log Probability: -0.0002\n",
      "Probability: 99.98%\n",
      "Top alternatives:\n",
      "  ' being': 0.01%\n",
      "  ' both': 0.01%\n",
      "  ' overcoming': 0.0%\n",
      "  ' breaking': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' virt'\n",
      "Log Probability: -0.5502\n",
      "Probability: 57.68%\n",
      "Top alternatives:\n",
      "  ' exceptional': 37.95%\n",
      "  ' extraordinary': 1.64%\n",
      "  ' impressive': 0.75%\n",
      "  ' significant': 0.6%\n",
      "--------------------------------------------------\n",
      "Token: 'uos'\n",
      "Log Probability: -0.5301\n",
      "Probability: 58.85%\n",
      "Top alternatives:\n",
      "  'u': 41.15%\n",
      "  'uo': 0.0%\n",
      "  'uous': 0.0%\n",
      "  'uso': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: 'ic'\n",
      "Log Probability: 0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  'istic': 0.0%\n",
      "  'ism': 0.0%\n",
      "  'itic': 0.0%\n",
      "  'ity': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' piano'\n",
      "Log Probability: -0.1434\n",
      "Probability: 86.64%\n",
      "Top alternatives:\n",
      "  ' performances': 11.8%\n",
      "  ' playing': 0.8%\n",
      "  ' skills': 0.37%\n",
      "  ' skill': 0.2%\n",
      "--------------------------------------------------\n",
      "Token: ' performances'\n",
      "Log Probability: -0.1161\n",
      "Probability: 89.04%\n",
      "Top alternatives:\n",
      "  ' skills': 6.04%\n",
      "  ' playing': 4.79%\n",
      "  ' performance': 0.05%\n",
      "  ' technique': 0.03%\n",
      "--------------------------------------------------\n",
      "Token: ' and'\n",
      "Log Probability: -0.0027\n",
      "Probability: 99.73%\n",
      "Top alternatives:\n",
      "  '.': 0.25%\n",
      "  ' as': 0.02%\n",
      "  ',': 0.0%\n",
      "  ' alongside': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' compositions'\n",
      "Log Probability: -0.6589\n",
      "Probability: 51.74%\n",
      "Top alternatives:\n",
      "  ' her': 11.15%\n",
      "  ' contributions': 7.86%\n",
      "  ' influential': 7.56%\n",
      "  ' significant': 7.08%\n",
      "--------------------------------------------------\n",
      "Token: '.'\n",
      "Log Probability: -0.0007\n",
      "Probability: 99.93%\n",
      "Top alternatives:\n",
      "  ',': 0.03%\n",
      "  ' that': 0.02%\n",
      "  '.\\n': 0.01%\n",
      "  ' as': 0.0%\n",
      "--------------------------------------------------\n",
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 90.58%\n",
      "Minimum confidence: 51.74%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Question: Who is Clara Schumann? Answer in one sentence.\n",
      "Response: Clara Schumann was a renowned German pianist and composer of the Romantic era, known for her virtuosic piano performances and compositions.\n",
      "Confidence Score: 78.93%\n"
     ]
    }
   ],
   "source": [
    "def get_response_with_confidence(question: str, model: str = \"gpt-3.5-turbo\", show_logprobs: bool = False) -> dict:\n",
    "    \"\"\"Get a model response with confidence analysis for a given question.\"\"\"\n",
    "    # Make the API call\n",
    "    message = client.chat.completions.create(\n",
    "        model=model,\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "\n",
    "    response_text = message.choices[0].message.content\n",
    "    logprobs = message.choices[0].logprobs\n",
    "    confidence_score = calculate_response_confidence(logprobs)\n",
    "\n",
    "    if show_logprobs:\n",
    "        analyze_logprobs(message)\n",
    "\n",
    "    result = {\n",
    "        \"response\": response_text,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"detailed_analysis\": {\n",
    "            \"confidence_analysis\": analyze_token_confidence(logprobs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Try the function with a simple question\n",
    "question = \"Who is Clara Schumann? Answer in one sentence.\"\n",
    "result = get_response_with_confidence(question, show_logprobs=True)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 78.90%\n",
      "Minimum confidence: 29.73%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: 'The', Confidence: 38.52%\n",
      "Token: ' Office', Confidence: 29.73%\n",
      "Token: ' Ed', Confidence: 39.09%\n",
      "\n",
      "Question: Who is John Cole Howard? Answer in one sentence.\n",
      "Response: John Cole Howard is a fictional character from the TV show \"The Office,\" portrayed by actor Ed Helms.\n",
      "Confidence Score: 64.15%\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is John Cole Howard? Answer in one sentence.\"\n",
    "result = get_response_with_confidence(question, show_logprobs=False)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ed Helmes character is Andy Bernard, not John Cole Howard. The low confidence score on the show's name could be due to the fact that the model is not sure about the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 84.54%\n",
      "Minimum confidence: 36.26%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: ' don', Confidence: 37.59%\n",
      "Token: ' in', Confidence: 36.26%\n",
      "Token: ' Let', Confidence: 41.53%\n",
      "\n",
      "Question: What was the exact time and temperature when Marie Curie made her first radium discovery? Include the barometric pressure in the lab.\n",
      "Response: I'm sorry, but I don't have access to real-time data or historical records of specific events like Marie Curie's first radium discovery. However, I can provide you with general information about her discovery and the conditions in which she conducted her experiments. Let me know if you would like to know more about that.\n",
      "Confidence Score: 70.06%\n"
     ]
    }
   ],
   "source": [
    "# Try the function with a question where the model hallucinates\n",
    "question = \"What was the exact time and temperature when Marie Curie made her first radium discovery? Include the barometric pressure in the lab.\"\n",
    "result = get_response_with_confidence(question, show_logprobs=False)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to make the models hallucinate on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 87.40%\n",
      "Minimum confidence: 32.15%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: ',', Confidence: 34.12%\n",
      "Token: ' their', Confidence: 35.47%\n",
      "Token: 'Tesla', Confidence: 33.25%\n",
      "Token: ' developed', Confidence: 34.73%\n",
      "Token: ' this', Confidence: 32.15%\n",
      "\n",
      "Question: Describe the collaboration between Nikola Tesla and Thomas Edison on their joint patent for wireless energy transmission in 1891. What were the specific technical details?\n",
      "Response: I'm sorry, but there seems to be a misunderstanding. Nikola Tesla and Thomas Edison were actually rivals in the field of electrical engineering, and they did not collaborate on a joint patent for wireless energy transmission in 1891 or at any other time. In fact, their approaches to electrical technology were quite different, with Tesla focusing on alternating current (AC) systems and wireless transmission, while Edison was known for his work on direct current (DC) systems and the invention of the incandescent light bulb.\n",
      "\n",
      "Tesla did work on wireless energy transmission and developed the concept of the Tesla coil, which could transmit electricity wirelessly over short distances. However, this was not a collaboration with Edison.\n",
      "\n",
      "If you have any other questions or need more information on a different topic, feel free to ask!\n",
      "Confidence Score: 70.83%\n"
     ]
    }
   ],
   "source": [
    "# Try the function with a question where the model hallucinates\n",
    "question = \"Describe the collaboration between Nikola Tesla and Thomas Edison on their joint patent for wireless energy transmission in 1891. What were the specific technical details?\"\n",
    "result = get_response_with_confidence(question, show_logprobs=False)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
