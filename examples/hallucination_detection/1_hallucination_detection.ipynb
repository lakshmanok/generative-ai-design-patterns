{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "load_dotenv(\"../saved_keys.env\")\n",
    "\n",
    "assert os.environ[\"OPENAI_API_KEY\"][:2] == \"sk\",\\\n",
    "       \"Please sign up for access to the OpenAI API and provide access token in keys.env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logprobs(message):\n",
    "    logprobs = message.choices[0].logprobs\n",
    "\n",
    "    if not logprobs:\n",
    "        print(\"No logprobs available in the response\")\n",
    "        return\n",
    "\n",
    "    # Print each token and its probability\n",
    "    print(\"\\nToken-by-token analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    for token_info in logprobs.content:\n",
    "        token = token_info.token\n",
    "        logprob = token_info.logprob\n",
    "        probability = round(100 * (2.718281828459045 ** logprob), 2)\n",
    "\n",
    "        print(f\"Token: {token!r}\")\n",
    "        print(f\"Log Probability: {logprob:.4f}\")\n",
    "        print(f\"Probability: {probability}%\")\n",
    "\n",
    "        # If top logprobs are available, show alternatives\n",
    "        if token_info.top_logprobs:\n",
    "            print(\"Top alternatives:\")\n",
    "            for alt_token in token_info.top_logprobs:\n",
    "                if alt_token.token != token:\n",
    "                    alt_probability = round(100 * (2.718281828459045 ** alt_token.logprob), 2)\n",
    "                    print(f\"  {alt_token.token!r}: {alt_probability}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def analyze_token_confidence(logprobs):\n",
    "    \"\"\"Analyze the model's confidence in its predictions\"\"\"\n",
    "    if not logprobs or not hasattr(logprobs, 'content'):\n",
    "        print(\"Debug: logprobs structure:\", logprobs)  # Debug print\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        confidences = [2.718281828459045 ** lp.logprob for lp in logprobs.content]\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        min_confidence = min(confidences)\n",
    "        max_confidence = max(confidences)\n",
    "\n",
    "        print(\"\\nConfidence Analysis:\")\n",
    "        print(f\"Average confidence: {avg_confidence:.2%}\")\n",
    "        print(f\"Minimum confidence: {min_confidence:.2%}\")\n",
    "        print(f\"Maximum confidence: {max_confidence:.2%}\")\n",
    "\n",
    "        # Find tokens with unusually low confidence\n",
    "        threshold = avg_confidence * 0.5  # 50% of average confidence\n",
    "        low_confidence_tokens = [\n",
    "            (lp.token, 2.718281828459045 ** lp.logprob)\n",
    "            for lp in logprobs.content\n",
    "            if 2.718281828459045 ** lp.logprob < threshold\n",
    "        ]\n",
    "\n",
    "        if low_confidence_tokens:\n",
    "            print(\"\\nTokens with unusually low confidence:\")\n",
    "            for token, conf in low_confidence_tokens:\n",
    "                print(f\"Token: {token!r}, Confidence: {conf:.2%}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Debug: Error processing logprobs: {e}\")\n",
    "        print(f\"Debug: logprobs type: {type(logprobs)}\")\n",
    "        print(f\"Debug: logprobs content: {logprobs}\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_response_confidence(logprobs):\n",
    "    \"\"\"Calculate an overall confidence score for the response.\n",
    "    Returns a score between 0 and 1, where:\n",
    "    - 1 indicates very high confidence\n",
    "    - 0 indicates very low confidence\n",
    "    \"\"\"\n",
    "    if not logprobs:\n",
    "        return None\n",
    "\n",
    "    # Convert logprobs to probabilities\n",
    "    confidences = [2.718281828459045 ** lp.logprob for lp in logprobs.content]\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    min_confidence = min(confidences)\n",
    "\n",
    "    # Weight both average and minimum confidence in the final score\n",
    "    # This helps catch both overall low confidence and individual uncertain tokens\n",
    "    confidence_score = (0.7 * avg_confidence) + (0.3 * min_confidence)\n",
    "\n",
    "    return round(confidence_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token-by-token analysis:\n",
      "--------------------------------------------------\n",
      "Token: 'The'\n",
      "Log Probability: -0.9156\n",
      "Probability: 40.03%\n",
      "Top alternatives:\n",
      "  'Bar': 35.67%\n",
      "  'In': 24.28%\n",
      "  'During': 0.02%\n",
      "  'As': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' president'\n",
      "Log Probability: -0.3546\n",
      "Probability: 70.15%\n",
      "Top alternatives:\n",
      "  ' President': 29.85%\n",
      "  ' ': 0.01%\n",
      "  ' US': 0.0%\n",
      "  ' United': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' of'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' in': 0.0%\n",
      "  ' the': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ' was': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' the'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' United': 0.0%\n",
      "  ' The': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ' of': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' United'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' united': 0.0%\n",
      "  ' Un': 0.0%\n",
      "  ' U': 0.0%\n",
      "  ' US': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' States'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' State': 0.0%\n",
      "  ' states': 0.0%\n",
      "  ' S': 0.0%\n",
      "  ' St': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' in'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' during': 0.0%\n",
      "  ' from': 0.0%\n",
      "  ' was': 0.0%\n",
      "  ' of': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' '\n",
      "Log Probability: -0.0004\n",
      "Probability: 99.96%\n",
      "Top alternatives:\n",
      "  ' the': 0.04%\n",
      "  ' year': 0.0%\n",
      "  ' January': 0.0%\n",
      "  ' in': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: '201'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  '202': 0.0%\n",
      "  ' ': 0.0%\n",
      "  '200': 0.0%\n",
      "  '2': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: '0'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  '1': 0.0%\n",
      "  '2': 0.0%\n",
      "  '9': 0.0%\n",
      "  '6': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' was'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' is': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ',': 0.0%\n",
      "  ' were': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' Barack'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' President': 0.0%\n",
      "  ' Barr': 0.0%\n",
      "  ' ': 0.0%\n",
      "  ' Bar': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: ' Obama'\n",
      "Log Probability: -0.0000\n",
      "Probability: 100.0%\n",
      "Top alternatives:\n",
      "  ' H': 0.0%\n",
      "  ' Ob': 0.0%\n",
      "  ' ': 0.0%\n",
      "  '\\xa0': 0.0%\n",
      "--------------------------------------------------\n",
      "Token: '.'\n",
      "Log Probability: -0.0006\n",
      "Probability: 99.94%\n",
      "Top alternatives:\n",
      "  ',': 0.06%\n",
      "  '.\\n': 0.0%\n",
      "  '.\\n\\n': 0.0%\n",
      "  ' who': 0.0%\n",
      "--------------------------------------------------\n",
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 93.58%\n",
      "Minimum confidence: 40.03%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: 'The', Confidence: 40.03%\n",
      "\n",
      "Question: Who was the president of the United States in 2010?\n",
      "Response: The president of the United States in 2010 was Barack Obama.\n",
      "Confidence Score: 77.51%\n"
     ]
    }
   ],
   "source": [
    "def get_response_with_confidence(question: str, model: str = \"gpt-3.5-turbo\", show_logprobs: bool = False) -> dict:\n",
    "    \"\"\"Get a model response with confidence analysis for a given question.\"\"\"\n",
    "    # Make the API call\n",
    "    message = client.chat.completions.create(\n",
    "        model=model,\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "\n",
    "    response_text = message.choices[0].message.content\n",
    "    logprobs = message.choices[0].logprobs\n",
    "    confidence_score = calculate_response_confidence(logprobs)\n",
    "\n",
    "    if show_logprobs:\n",
    "        analyze_logprobs(message)\n",
    "\n",
    "    result = {\n",
    "        \"response\": response_text,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"detailed_analysis\": {\n",
    "            \"confidence_analysis\": analyze_token_confidence(logprobs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Try the function with a simple question\n",
    "question = \"Who was the president of the United States in 2010?\"\n",
    "result = get_response_with_confidence(question, show_logprobs=True)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 84.54%\n",
      "Minimum confidence: 36.26%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: ' don', Confidence: 37.59%\n",
      "Token: ' in', Confidence: 36.26%\n",
      "Token: ' Let', Confidence: 41.53%\n",
      "\n",
      "Question: What was the exact time and temperature when Marie Curie made her first radium discovery? Include the barometric pressure in the lab.\n",
      "Response: I'm sorry, but I don't have access to real-time data or historical records of specific events like Marie Curie's first radium discovery. However, I can provide you with general information about her discovery and the conditions in which she conducted her experiments. Let me know if you would like to know more about that.\n",
      "Confidence Score: 70.06%\n"
     ]
    }
   ],
   "source": [
    "# Try the function with a question where the model hallucinates\n",
    "question = \"What was the exact time and temperature when Marie Curie made her first radium discovery? Include the barometric pressure in the lab.\"\n",
    "result = get_response_with_confidence(question, show_logprobs=False)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Analysis:\n",
      "Average confidence: 87.40%\n",
      "Minimum confidence: 32.15%\n",
      "Maximum confidence: 100.00%\n",
      "\n",
      "Tokens with unusually low confidence:\n",
      "Token: ',', Confidence: 34.12%\n",
      "Token: ' their', Confidence: 35.47%\n",
      "Token: 'Tesla', Confidence: 33.25%\n",
      "Token: ' developed', Confidence: 34.73%\n",
      "Token: ' this', Confidence: 32.15%\n",
      "\n",
      "Question: Describe the collaboration between Nikola Tesla and Thomas Edison on their joint patent for wireless energy transmission in 1891. What were the specific technical details?\n",
      "Response: I'm sorry, but there seems to be a misunderstanding. Nikola Tesla and Thomas Edison were actually rivals in the field of electrical engineering, and they did not collaborate on a joint patent for wireless energy transmission in 1891 or at any other time. In fact, their approaches to electrical technology were quite different, with Tesla focusing on alternating current (AC) systems and wireless transmission, while Edison was known for his work on direct current (DC) systems and the invention of the incandescent light bulb.\n",
      "\n",
      "Tesla did work on wireless energy transmission and developed the concept of the Tesla coil, which could transmit electricity wirelessly over short distances. However, this was not a collaboration with Edison.\n",
      "\n",
      "If you have any other questions or need more information on a different topic, feel free to ask!\n",
      "Confidence Score: 70.83%\n"
     ]
    }
   ],
   "source": [
    "# Try the function with a question where the model hallucinates\n",
    "question = \"Describe the collaboration between Nikola Tesla and Thomas Edison on their joint patent for wireless energy transmission in 1891. What were the specific technical details?\"\n",
    "result = get_response_with_confidence(question, show_logprobs=False)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Confidence Score: {result['confidence_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
