{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a56bb7-e753-4731-84ed-cdc1084f57e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With guardrails\n",
    "\n",
    "A search application with guardrails. Only the display code is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fcb201-d306-40ad-b430-1093f76e8bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --quiet llama-index llama-index-retrievers-bm25 llama-index-llms-anthropic anthropic llama-index-llms-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffaacc11-e131-4170-ad1b-42bdf2e01b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GEMINI=\"gemini-2.0-flash\"\n",
    "#OPENAI=\"gpt-4o-mini\"\n",
    "CLAUDE=\"claude-3-7-sonnet-latest\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"GEMINI_API_KEY\"][:2] == \"AI\",\\\n",
    "       \"Please specify the GEMINI_API_KEY access token in keys.env file\"\n",
    "assert os.environ[\"ANTHROPIC_API_KEY\"][:2] == \"sk\",\\\n",
    "       \"Please specify the ANTHROPIC_API_KEY access token in keys.env file\"\n",
    "#assert os.environ[\"OPENAI_API_KEY\"][:2] == \"sk\",\\\n",
    "#       \"Please specify the OPENAI_API_KEY access token in keys.env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49ab22-ad3e-48ed-995a-42863897af3d",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "Replace \"PII\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac843ac-3a51-4dc0-b446-77fb27deecd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guardrail_type': 'PII Removal',\n",
       " 'activated': True,\n",
       " 'should_stop': False,\n",
       " 'sanitized_output': 'The killer was a man'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Custom guardrail, to replace all names by something generic\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "def guardrail_replace_names(to_scan: str):\n",
    "    llm = GoogleGenAI(model=GEMINI,\n",
    "                      api_key=os.environ[\"GEMINI_API_KEY\"], \n",
    "                      temperature=0)\n",
    "    system_prompt=\"\"\"\n",
    "        I will give you a piece of text. In that piece of text, replace any personal names by a generic identifier.\n",
    "        \n",
    "        Example:\n",
    "          Input:\n",
    "            I met Sally in the store.\n",
    "          Output:\n",
    "            I met a woman in the store.\n",
    "        \n",
    "        Return only the modified text, with no preamble or special markers.\n",
    "    \"\"\"\n",
    "    sanitized_output = llm.complete(system_prompt + \"\\n\" + to_scan).text.strip()\n",
    "    no_change = (sanitized_output == to_scan)\n",
    "    \n",
    "    return {\n",
    "        \"guardrail_type\": \"PII Removal\",\n",
    "        \"activated\": not no_change,\n",
    "        \"should_stop\": False,\n",
    "        \"sanitized_output\": sanitized_output,\n",
    "    }\n",
    "\n",
    "guardrail_replace_names(\"The killer was John Doe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d06200-e3c3-47d0-be5c-ebb9065194d0",
   "metadata": {},
   "source": [
    "Banned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b572e3-7865-4e64-9864-292dc57f0578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guardrail_type': 'Banned Topic',\n",
       " 'activated': True,\n",
       " 'should_stop': True,\n",
       " 'sanitized_output': 'Are priests allowed to marry?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def guardrail_banned_topics(to_scan: str):\n",
    "    banned_topics = [\n",
    "        \"religion\", \"politics\", \"sexual innuendo\"\n",
    "    ]\n",
    "    llm = GoogleGenAI(model=GEMINI,\n",
    "                      api_key=os.environ[\"GEMINI_API_KEY\"], \n",
    "                      temperature=0)\n",
    "    system_prompt=f\"\"\"\n",
    "        I will give you a piece of text. Check whether the text touches on any of these topics.\n",
    "        \n",
    "        {banned_topics}\n",
    "        \n",
    "        Return True or False, with no preamble or special markers.\n",
    "        Text:\n",
    "    \"\"\"\n",
    "    response = llm.complete(system_prompt + \"\\n\" + to_scan).text.strip()\n",
    "    is_banned = (response == \"True\")\n",
    "   \n",
    "    return {\n",
    "        \"guardrail_type\": \"Banned Topic\",\n",
    "        \"activated\": is_banned,\n",
    "        \"should_stop\": is_banned,\n",
    "        \"sanitized_output\": to_scan,\n",
    "    }\n",
    "\n",
    "guardrail_banned_topics(\"Are priests allowed to marry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6efd14e-cac3-4507-b053-71be0f2c990a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'should_stop': True,\n",
       " 'triggered': [{'guardrail_type': 'Banned Topic',\n",
       "   'activated': True,\n",
       "   'should_stop': True,\n",
       "   'sanitized_output': 'Are parish priests expected to be role models?'}],\n",
       " 'sanitized': 'Are parish priests expected to be role models?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_guardrails(to_scan, scanners):\n",
    "    should_stop = False\n",
    "    triggered_scanners = []  # Store results from triggered scanners\n",
    "\n",
    "    sanitized_output = to_scan # start with the original string\n",
    "    for scanner in scanners:\n",
    "        result = scanner(sanitized_output)\n",
    "\n",
    "        if result[\n",
    "            \"activated\"\n",
    "        ]:  # Check if the scanner found a threat (activated=True)\n",
    "            should_stop = result[\"should_stop\"]  # Set detected to True if any scanner triggers\n",
    "            triggered_scanners.append(result)  # all activated scanners\n",
    "            sanitized_output = result[\"sanitized_output\"] # Update the query\n",
    "\n",
    "    result = {\n",
    "        \"should_stop\": should_stop,\n",
    "        \"triggered\": triggered_scanners,\n",
    "        \"sanitized\": sanitized_output\n",
    "    }\n",
    "    return result\n",
    "\n",
    "apply_guardrails(\"Are parish priests expected to be role models?\", [guardrail_replace_names, guardrail_banned_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9273aa27-e2d4-4c85-baf4-1c30426a01a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'should_stop': False,\n",
       " 'triggered': [{'guardrail_type': 'PII Removal',\n",
       "   'activated': True,\n",
       "   'should_stop': False,\n",
       "   'sanitized_output': 'Is a man a good role model?'}],\n",
       " 'sanitized': 'Is a man a good role model?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_guardrails(\"Is Mr. Darcy a good role model?\", [guardrail_replace_names, guardrail_banned_topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2ac21-a21e-4ce9-8c75-b31b30fa59a4",
   "metadata": {},
   "source": [
    "# Guardrails around RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee37143-aefe-41ac-af70-00da317966b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "class GuardedQueryEngine(RetrieverQueryEngine):\n",
    "    def __init__(self, query_engine: RetrieverQueryEngine):\n",
    "        self._query_engine = query_engine\n",
    "    \n",
    "    def query(self, query):\n",
    "        # apply guardrails to inputs\n",
    "        gd = apply_guardrails(query,\n",
    "                              [guardrail_replace_names, guardrail_banned_topics])\n",
    "        if not gd[\"should_stop\"]:\n",
    "            print(f\"Modified Query: {gd['sanitized']}\")\n",
    "            query_response = self._query_engine.query(gd[\"sanitized\"])     \n",
    "            gd = apply_guardrails(str(query_response), [guardrail_banned_topics])\n",
    "            if not gd[\"should_stop\"]:\n",
    "                return Response(gd[\"sanitized\"],\n",
    "                                source_nodes=query_response.source_nodes)\n",
    "        return Response(str(gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3782e9-d901-4255-acae-0201fdbb64f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic RAG application\n",
    "\n",
    "This is the application that we want to protect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f487cc95-d26a-4436-94b9-9949a1d0f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:26,049 - INFO - Indexer initialized\n",
      "2025-05-13 00:56:26,050 - INFO - Loading https://www.gutenberg.org/cache/epub/31100/pg31100.txt from cache\n",
      "2025-05-13 00:56:26,121 - INFO - Cleaned Gutenberg text: removed 887 chars from start, 18518 chars from end\n",
      "2025-05-13 00:56:26,122 - INFO - Successfully loaded text from https://www.gutenberg.org/cache/epub/31100/pg31100.txt.\n",
      "2025-05-13 00:56:51,954 - INFO - Successfully loaded text from b395ceb2-141e-41a8-a5bd-fb1b9f152690 -- 24434 nodes created.\n",
      "2025-05-13 00:56:53,735 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from basic_rag import build_query_engine, print_response_to_query\n",
    "query_engine = build_query_engine(CLAUDE, [\"https://www.gutenberg.org/cache/epub/31100/pg31100.txt\"], 100) # Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7386fdeb-bea0-47fc-a42e-41698325f384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wrap it in Guardrails\n",
    "gd_query_engine = GuardedQueryEngine(query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc71180-869f-4945-ae96-6d96b6f8e721",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Good query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c4a7a4-3e17-4fcc-9b6a-47b518a31a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:54,388 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:56:54,391 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:56:54,995 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:56:54,997 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 00:56:55,232 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:56:55,235 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:56:55,590 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:56:55,593 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Query: Can you give advice without being resented for it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:00,294 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:00,567 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:00,570 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:00,894 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:00,896 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it is possible to give advice without being resented for it, as shown in the example where Elizabeth thanked her aunt for \"the kindness of her hints\" and they parted in what was described as \"a wonderful instance of advice being given on such a point, without being resented.\"\n",
      "\n",
      "However, the manner in which advice is offered seems important. When advice is perceived as kind or when it respects the other person's autonomy, it appears to be better received. Conversely, there are instances where people may resist advice, particularly when they feel their independence is being challenged, as suggested by the reference to someone being \"wilful and perverse\" and deciding for themselves \"without any consideration or deference for those who have surely some right to guide you.\"\n",
      "\n",
      "In some situations, like when Elinor was asked for advice, she declined to give it directly, instead suggesting that the person's \"own judgment must direct you,\" which represents another approach to handling advice-giving situations respectfully.\n",
      "\n",
      "\n",
      "**Sources**:\n",
      "Node ID: b814dd62-4fa8-401c-b76d-12b9e0cc7b76\n",
      "Text: Her aunt assured her that she was, and Elizabeth having thanked\n",
      "her for the kindness of her hints, they parted; a wonderful instance\n",
      "of advice being given on such a point, without being resented.  Mr.\n",
      "Score:  7.238\n",
      "\n",
      "Node ID: 4c2cb9d7-ccb2-4be5-9e00-e5098b9680bc\n",
      "Text: But you have now shewn me that you can be wilful and perverse;\n",
      "that you can and will decide for yourself, without any consideration\n",
      "or deference for those who have surely some right to guide you,\n",
      "without even asking their advice.\n",
      "Score:  6.464\n",
      "\n",
      "Node ID: cebf9220-56dd-45f6-ba62-744e956a68ac\n",
      "Text: What would you do yourself?\"  \"Pardon me,\" replied Elinor,\n",
      "startled by the question; \"but I can give you no advice under such\n",
      "circumstances. Your own judgment must direct you.\"\n",
      "Score:  5.678\n",
      "\n",
      "Node ID: 606a4ed5-9339-4b4e-a041-7e68d9943728\n",
      "Text: If you can give me your assurance of having no design beyond\n",
      "enjoying the conversation of a clever woman for a short period, and of\n",
      "yielding admiration only to her beauty and abilities, without being\n",
      "blinded by them to her faults, you will restore me to happiness;\n",
      "Score:  5.196\n",
      "\n",
      "Node ID: 5ef1025e-fade-49ea-b3f9-c966d527acd2\n",
      "Text: I can have nothing to fear from you--and can chearfully conduct\n",
      "you to Mrs Cope's without a fear of your being seduced by her Example,\n",
      "or contaminated by her Follies.\n",
      "Score:  4.893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_response_to_query(gd_query_engine, \"Can you give advice without being resented for it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93756382-aee1-46bc-90c7-5427da0724ad",
   "metadata": {},
   "source": [
    "### Query that should be rejected\n",
    "\n",
    "Because it touches on religion which is (let's assume) a prohibited topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51febb8c-86d0-406a-910c-34e733078fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:01,134 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:01,137 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:01,622 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:01,624 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 00:57:01,856 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:01,861 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:02,145 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:02,149 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should_stop': True, 'triggered': [{'guardrail_type': 'Banned Topic', 'activated': True, 'should_stop': True, 'sanitized_output': 'Are parish priests expected to be role models?'}], 'sanitized': 'Are parish priests expected to be role models?'}\n",
      "\n",
      "\n",
      "**Sources**:\n"
     ]
    }
   ],
   "source": [
    "print_response_to_query(gd_query_engine, \"Are parish priests expected to be role models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578fdf3-33f8-4b29-a4ec-c613cedacc9c",
   "metadata": {},
   "source": [
    "### Query that should be modified.\n",
    "\n",
    "Let's say that queries that reference people by name should be made more generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd36fffb-7e2e-4ff7-afa2-0ad1361cbb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:02,398 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:02,403 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:02,915 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:02,920 - INFO - AFC remote call 1 is done.\n",
      "2025-05-13 00:57:03,178 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:03,183 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:03,557 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:03,561 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Query: Would a man be an appealing match if he were not wealthy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:08,444 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:08,678 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:08,681 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-13 00:57:09,082 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 00:57:09,084 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the information provided, wealth appears to be a significant factor in determining a desirable match in this society. There are multiple references to wealth in relation to marriages - such as a woman who \"married a very wealthy man\" and a family described as \"all rich together,\" with mention of \"fifty thousand pounds.\" Additionally, there's mention of the Allens being \"wealthy and childless\" as \"absolute facts\" that seem to factor into someone's consideration.\n",
      "\n",
      "However, there's also indication that other factors matter in relationships, as shown by the hope that a couple would be \"united by mutual affection\" and that \"their dispositions were as exactly fitted to make them blessed in each other.\" In one case, a match is described as \"quite good enough\" even though it was eclipsed by another option that appears to have been more financially advantageous.\n",
      "\n",
      "So while wealth seems to be a highly valued attribute in potential matches, there are suggestions that other qualities like mutual affection and compatible dispositions were also considered important.\n",
      "\n",
      "\n",
      "**Sources**:\n",
      "Node ID: 969d9396-6d1b-4fd2-8625-2b3ebbab8415\n",
      "Text: \"No--I have never seen Mr. Elton,\" she replied, starting on this\n",
      "appeal; \"is he--is he a tall man?\"  \"Who shall answer that question?\"\n",
      "cried Emma. \"My father would say 'yes,' Mr.\n",
      "Score:  5.596\n",
      "\n",
      "Node ID: 1743a906-f5a0-4ae5-8b30-1f3b49d5d3e0\n",
      "Text: I remember her aunt very well, Biddy Henshawe; she married a\n",
      "very wealthy man. But the family are all rich together. Fifty thousand\n",
      "pounds!\n",
      "Score:  4.712\n",
      "\n",
      "Node ID: 43f33a53-0fb0-4eb9-8d10-f5c58cfaaa02\n",
      "Text: he was most earnest in hoping, and sanguine in believing, that\n",
      "it would be a match at last, and that, united by mutual affection, it\n",
      "would appear that their dispositions were as exactly fitted to make\n",
      "them blessed in each other,\n",
      "Score:  4.643\n",
      "\n",
      "Node ID: 7b9bf642-744f-46bf-8fff-712b8d22d6e3\n",
      "Text: and his own views on another (circumstances of which he boasted\n",
      "with almost equal openness), seemed sufficient vouchers for his truth;\n",
      "and to these were added the absolute facts of the Allens being wealthy\n",
      "and childless,\n",
      "Score:  4.571\n",
      "\n",
      "Node ID: 42a42d85-4a51-400f-86a8-1bfa15509e73\n",
      "Text: Elizabeth was the least dear to her of all her children; and\n",
      "though the man and the match were quite good enough for _her_, the\n",
      "worth of each was eclipsed by Mr. Bingley and Netherfield.\n",
      "Score:  4.282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_response_to_query(gd_query_engine, \"Would Mr. Darcy be an appealing match if he were not wealthy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a74e9-a1ab-4774-85df-b255f9858056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
