{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f4b00e-59c2-4496-a0e5-cd10dff0f0eb",
   "metadata": {},
   "source": [
    "## A very basic RAG\n",
    "\n",
    "You would never build a RAG system this basic. But it helps illustrate the problems we are trying to solve with some of the more advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41664a72-3e67-4e57-9e6d-2a01ed7ed8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet llama-index llama-index-retrievers-bm25 llama-index-llms-anthropic anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b078983c-cda3-46d0-8438-d284bc017b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"claude-3-7-haiku-latest\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"ANTHROPIC_API_KEY\"][:2] == \"sk\",\\\n",
    "       \"Please specify the ANTHROPIC_API_KEY access token in keys.env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b39def9-ec4c-46e9-93f9-de6a5e739a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a2bf0-6974-4c6b-82a4-411fb87d7033",
   "metadata": {},
   "source": [
    "## Utility: cache urls to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ec0ef6-aa50-4731-86a2-5327dcde0d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Union, Tuple, Any\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class CacheManager:\n",
    "    \"\"\"\n",
    "    Manages the local cache for downloaded files.\n",
    "    \n",
    "    Attributes:\n",
    "        cache_dir (Path): Path to the cache directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"./.cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the cache manager.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir (str): Path to the cache directory. Defaults to \"./.cache\".\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self._ensure_cache_dir()\n",
    "    \n",
    "    def _ensure_cache_dir(self) -> None:\n",
    "        \"\"\"Create the cache directory if it doesn't exist.\"\"\"\n",
    "        if not self.cache_dir.exists():\n",
    "            self.cache_dir.mkdir(parents=True)\n",
    "            logger.info(f\"Created cache directory at {self.cache_dir}\")\n",
    "    \n",
    "    def _get_cache_filename(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a unique filename for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to generate a filename for.\n",
    "            \n",
    "        Returns:\n",
    "            str: A unique filename based on the URL.\n",
    "        \"\"\"\n",
    "        # Extract the filename from the URL if possible\n",
    "        parsed_url = urlparse(url)\n",
    "        path_parts = parsed_url.path.split('/')\n",
    "        original_filename = path_parts[-1] if path_parts[-1] else \"index\"\n",
    "        \n",
    "        # Create a hash of the URL to ensure uniqueness\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n",
    "        \n",
    "        # Combine original filename with hash\n",
    "        if '.' in original_filename:\n",
    "            name_parts = original_filename.split('.')\n",
    "            extension = name_parts[-1]\n",
    "            base_name = '.'.join(name_parts[:-1])\n",
    "            return f\"{base_name}_{url_hash}.{extension}\"\n",
    "        else:\n",
    "            return f\"{original_filename}_{url_hash}.txt\"\n",
    "    \n",
    "    def get_cache_path(self, url: str) -> Path:\n",
    "        \"\"\"\n",
    "        Get the cache path for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to get the cache path for.\n",
    "            \n",
    "        Returns:\n",
    "            Path: The path where the cached file would be stored.\n",
    "        \"\"\"\n",
    "        filename = self._get_cache_filename(url)\n",
    "        return self.cache_dir / filename\n",
    "    \n",
    "    def is_cached(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a URL is already cached.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to check.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the URL is cached, False otherwise.\n",
    "        \"\"\"\n",
    "        cache_path = self.get_cache_path(url)\n",
    "        return cache_path.exists()\n",
    "    \n",
    "    def get_cached_content(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get the cached content for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to get the cached content for.\n",
    "            \n",
    "        Returns:\n",
    "            Optional[str]: The cached content if available, None otherwise.\n",
    "        \"\"\"\n",
    "        if not self.is_cached(url):\n",
    "            return None\n",
    "        \n",
    "        cache_path = self.get_cache_path(url)\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error reading cached file for {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cache_content(self, url: str, content: str) -> bool:\n",
    "        \"\"\"\n",
    "        Cache content for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL the content was downloaded from.\n",
    "            content (str): The content to cache.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if caching was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        self._ensure_cache_dir()\n",
    "        cache_path = self.get_cache_path(url)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            logger.info(f\"Cached content for {url} at {cache_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error caching content for {url}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def clear_cache(self) -> bool:\n",
    "        \"\"\"\n",
    "        Clear all cached files.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if clearing was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.cache_dir.exists():\n",
    "                for file_path in self.cache_dir.iterdir():\n",
    "                    if file_path.is_file():\n",
    "                        file_path.unlink()\n",
    "                logger.info(\"Cache cleared successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clearing cache: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_cache_size(self) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Get the total size of the cache.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[int, str]: A tuple containing the size in bytes and a human-readable size.\n",
    "        \"\"\"\n",
    "        total_size = 0\n",
    "        \n",
    "        if self.cache_dir.exists():\n",
    "            for file_path in self.cache_dir.iterdir():\n",
    "                if file_path.is_file():\n",
    "                    total_size += file_path.stat().st_size\n",
    "        \n",
    "        # Convert to human-readable format\n",
    "        units = ['B', 'KB', 'MB', 'GB']\n",
    "        size_human = total_size\n",
    "        unit_index = 0\n",
    "        \n",
    "        while size_human > 1024 and unit_index < len(units) - 1:\n",
    "            size_human /= 1024\n",
    "            unit_index += 1\n",
    "        \n",
    "        human_readable = f\"{size_human:.2f} {units[unit_index]}\"\n",
    "        return total_size, human_readable\n",
    "    \n",
    "    def list_cached_files(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List all cached files with metadata.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries containing file information.\n",
    "        \"\"\"\n",
    "        files_info = []\n",
    "        \n",
    "        if self.cache_dir.exists():\n",
    "            for file_path in self.cache_dir.iterdir():\n",
    "                if file_path.is_file():\n",
    "                    stat = file_path.stat()\n",
    "                    files_info.append({\n",
    "                        'filename': file_path.name,\n",
    "                        'path': str(file_path),\n",
    "                        'size_bytes': stat.st_size,\n",
    "                        'last_modified': time.ctime(stat.st_mtime)\n",
    "                    })\n",
    "        \n",
    "        return files_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fc33c-0770-4948-bd03-22acdffabd8c",
   "metadata": {},
   "source": [
    "## utility: Get text from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40dad339-d55a-4440-8ab4-367f0cb0ffc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class GutenbergTextLoadError(Exception):\n",
    "    \"\"\"Exception raised for errors in loading Gutenberg text files.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DocumentSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load_from_url(self, url) -> Document:\n",
    "        pass\n",
    "\n",
    "class GutenbergSource(DocumentSource):\n",
    "    \"\"\"\n",
    "    A class to load text files from Project Gutenberg as a LlamaIndex Document.\n",
    "    \n",
    "    This class handles fetching text content from URLs, processing Gutenberg-specific\n",
    "    formatting, and creating a document store indexed by BM25.\n",
    "    \n",
    "    Attributes:\n",
    "        cache_manager (CacheManager): Manager for the local cache.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \"./.cache\",\n",
    "    ):\n",
    "        self.cache_manager = CacheManager(cache_dir)\n",
    "   \n",
    "    def _fetch_text_from_url(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch text content from a URL with caching\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to fetch text from.\n",
    "            \n",
    "        Returns:\n",
    "            str: Text content from the URL.\n",
    "            \n",
    "        Raises:\n",
    "            GutenbergTextLoadError: If there's an error fetching or processing the URL.\n",
    "        \"\"\"\n",
    "        if self.cache_manager.is_cached(url):\n",
    "            logger.info(f\"Loading {url} from cache\")\n",
    "            cached_content = self.cache_manager.get_cached_content(url)\n",
    "            if cached_content:\n",
    "                return cached_content\n",
    "            logger.warning(f\"Cached content for {url} could not be read, downloading again\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Fetching text from URL: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check if content is text\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/plain' not in content_type and 'text/html' not in content_type:\n",
    "                raise GutenbergTextLoadError(f\"URL does not contain text content: {content_type}\")\n",
    "            \n",
    "            # Detect encoding or use utf-8 as fallback\n",
    "            encoding = response.encoding or 'utf-8'\n",
    "            content = response.content.decode(encoding)\n",
    "        \n",
    "            # Cache the downloaded content\n",
    "            self.cache_manager.cache_content(url, content)\n",
    "            \n",
    "            return content\n",
    "        except requests.RequestException as e:\n",
    "            raise GutenbergTextLoadError(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            raise GutenbergTextLoadError(f\"Error decoding content from {url}: {str(e)}\")\n",
    "    \n",
    "    def _clean_gutenberg_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean Project Gutenberg text by removing headers, footers, and license information.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text from Project Gutenberg.\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text with Gutenberg-specific content removed.\n",
    "        \"\"\"\n",
    "        # Pattern to find the start of the actual content (after header)\n",
    "        start_markers = [\n",
    "            r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\* START OF THE PROJECT GUTENBERG .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\*START OF THE PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\"\n",
    "        ]\n",
    "        \n",
    "        # Pattern to find the end of the content (before footer)\n",
    "        end_markers = [\n",
    "            r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\* END OF THE PROJECT GUTENBERG .+? \\*\\*\\*\", \n",
    "            r\"\\*\\*\\*END OF THE PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\"\n",
    "        ]\n",
    "        \n",
    "        # Find start of content\n",
    "        start_pos = 0\n",
    "        for marker in start_markers:\n",
    "            match = re.search(marker, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                start_pos = match.end()\n",
    "                break\n",
    "        \n",
    "        # Find end of content\n",
    "        end_pos = len(text)\n",
    "        for marker in end_markers:\n",
    "            match = re.search(marker, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                end_pos = match.start()\n",
    "                break\n",
    "        \n",
    "        # Extract and clean the content\n",
    "        content = text[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "        \n",
    "        logger.info(f\"Cleaned Gutenberg text: removed {start_pos} chars from start, \"\n",
    "                   f\"{len(text) - end_pos} chars from end\")\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def load_from_url(self, url) -> Document:\n",
    "        \"\"\"\n",
    "        Load text from a URL and return a LlamaIndex Document.\n",
    "        \n",
    "        Args:\n",
    "            url (str, optional): URL to load text from. If None, uses the default URL.\n",
    "            \n",
    "        Returns:\n",
    "            Document\n",
    "            \n",
    "        Raises:\n",
    "            GutenbergTextLoadError: If there's an error loading or processing the text.\n",
    "        \"\"\"\n",
    "        url = url or self.default_url\n",
    "        \n",
    "        try:\n",
    "            # Fetch and clean the text\n",
    "            raw_text = self._fetch_text_from_url(url)\n",
    "            cleaned_text = self._clean_gutenberg_text(raw_text)\n",
    "            \n",
    "            # Create a document with metadata\n",
    "            parsed_url = urlparse(url)\n",
    "            filename = os.path.basename(parsed_url.path)\n",
    "            \n",
    "            document = Document(\n",
    "                text=cleaned_text,\n",
    "                metadata={\n",
    "                    \"source\": url,\n",
    "                    \"filename\": filename,\n",
    "                    \"date_loaded\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully loaded text from {url}.\")\n",
    "            \n",
    "            return document\n",
    " \n",
    "        except Exception as e:\n",
    "            raise GutenbergTextLoadError(f\"Error loading from URL {url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2e1d2-2682-4e82-8e55-7b5134ddf34a",
   "metadata": {},
   "source": [
    "Try reading Anabasis of Alexander https://www.gutenberg.org/cache/epub/46976/pg46976.txt\n",
    "a 2nd century historical account of Alexander the Great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3a0082-337e-438b-89c8-00b0145ee721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:17,601 - INFO - Loading https://www.gutenberg.org/cache/epub/46976/pg46976.txt from cache\n",
      "2025-03-12 01:10:17,637 - INFO - Cleaned Gutenberg text: removed 1033 chars from start, 18492 chars from end\n",
      "2025-03-12 01:10:17,639 - INFO - Successfully loaded text from https://www.gutenberg.org/cache/epub/46976/pg46976.txt.\n"
     ]
    }
   ],
   "source": [
    "gs = GutenbergSource()\n",
    "doc = gs.load_from_url(\"https://www.gutenberg.org/cache/epub/46976/pg46976.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670e7a61-6219-4e4c-8f28-430c9153c0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he calls himself so in _Cynegeticus_ (v.\\n6); and in _Periplus_ (xii. 5; xxv. 1), he distinguishes Xenophon by\\nthe addition _the elder_. Lucian (_Alexander_, 56) calls Arrian simply\\n_Xenophon_. During the stay of the emperor Hadrian at Athens, A.D. 126,\\nArrian gained his friendship. He accompanied his patron to Rome, where\\nhe received the Roman citizenship. In consequence of this, he assumed\\nthe name of Flavius.[2] In the same way the Jewish historian, Josephus,\\nhad been allowed by Vespasian and Titus to bear the imperial name\\nFlavius.[3]\\n\\nPhotius says, that Arrian had a distinguished career in Rome, being\\nentrusted with various political offices, and at last reaching the\\nsupreme dignity of consul under Antoninus Pius.[4] Previous to this\\nhe was appointed (A.D. 132) by Hadrian, Governor of Cappadocia, which\\nprovince was soon after invaded by the Alani, or Massagetae, whom he\\ndefeated and expelled.[5] When Marcus Aurelius came to the throne,\\nArrian withdrew into private life and returned'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text[21000:22000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c75fa8b-5c65-4eb5-a1b3-08ae28b8ff6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0851828-29e1-4d33-a050-d313040fc58e\n"
     ]
    }
   ],
   "source": [
    "print(doc.id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7ecb0-96e6-4671-a017-1bcd29526480",
   "metadata": {},
   "source": [
    "## Step 1: Index document\n",
    "\n",
    "We will break up the document into chunks, and index it using BM25\n",
    "See: https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5237a6dc-5eb2-4bc6-86c1-4541105cb2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "class Indexer:\n",
    "    \"\"\"\n",
    "    A class to load documents into LlamaIndex using BM25.\n",
    "    \n",
    "    Attributes:\n",
    "        chunk_size (int): Size of text chunks for processing.\n",
    "        chunk_overlap (int): Overlap between text chunks.\n",
    "        docstore (SimpleDocumentStore): Document store for storing processed documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \"./.cache\",\n",
    "        chunk_size: int = 1024,\n",
    "        chunk_overlap: int = 20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Indexer.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Size of text chunks for processing. Defaults to 1024.\n",
    "            chunk_overlap (int): Overlap between text chunks. Defaults to 20.\n",
    "        \"\"\"        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize a simple document store\n",
    "        self.docstore = SimpleDocumentStore()\n",
    "        \n",
    "        self.node_parser = SentenceSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Indexer initialized\")\n",
    "    \n",
    "\n",
    "    def add_document_to_index(self, document: Document):\n",
    "        # Parse the document into nodes\n",
    "        nodes = self.node_parser.get_nodes_from_documents([document])\n",
    "\n",
    "        # Add nodes to the document store\n",
    "        self.docstore.add_documents(nodes)\n",
    "\n",
    "        logger.info(f\"Successfully loaded text from {document.id_} -- {len(nodes)} nodes created.\")\n",
    "            \n",
    "    def get_docstore(self) -> SimpleDocumentStore:\n",
    "        return self.docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aff170f-4d87-4fb7-ad75-b9b33e88bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:17,987 - INFO - Indexer initialized\n",
      "2025-03-12 01:10:23,982 - INFO - Successfully loaded text from b0851828-29e1-4d33-a050-d313040fc58e -- 6104 nodes created.\n"
     ]
    }
   ],
   "source": [
    "index = Indexer(chunk_size=100, chunk_overlap=20)\n",
    "index.add_document_to_index(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d50e0-9881-48a6-b5ff-c8a78ef3de06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Retrieve nodes that match query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da943d0-1ca9-4d23-a9f2-5629b7ad43e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:25,975 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.get_docstore(),\n",
    "    similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1d391c3-796f-413a-bb35-35e07ec4452e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ee1ef41e-3e31-4e07-9949-5e585a50651c<br>**Similarity:** 4.2463765144348145<br>**Text:** But Diogenes said that he\n",
       "wanted nothing else, except that he and his attendants would stand out\n",
       "of the sunlight. Alexander is said to have expressed his admiration\n",
       "of Diogenes’s conduct.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 31bab814-51cd-47cb-ab1a-eb7ab51bcdfc<br>**Similarity:** 4.118840217590332<br>**Text:** 100 stades; and most of it is the mean between\n",
       "these breadths.[642] This river Indus Alexander crossed at daybreak\n",
       "with his army into the country of the Indians; concerning whom, in\n",
       "this history I have described neither what laws they enjoy,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 005e2f98-94da-4b58-9486-5e7186584eb3<br>**Similarity:** 3.639586925506592<br>**Text:** 32). Alexander said: “If I were\n",
       "not Alexander, I should like to be Diogenes.” Cf. _Arrian_, i. 1;\n",
       "Plutarch (_de Fortit. Alex._, p. 331).<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9946e837-31de-45bb-88ef-26132edd6f20<br>**Similarity:** 3.4104578495025635<br>**Text:** Alexander is said to have expressed his admiration\n",
       "of Diogenes’s conduct.[832] Thus it is evident that Alexander was\n",
       "not entirely destitute of better feelings; but he was the slave of\n",
       "his insatiable ambition.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9c5c9882-9eb2-440b-ae1b-89759b623a6e<br>**Similarity:** 3.2550690174102783<br>**Text:** He also ascertained that for\n",
       "the present Bessus held the supreme command, both on account of his\n",
       "relationship to Darius and because the war was being carried on in his\n",
       "viceregal province. Hearing this,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "retrieved_nodes = retriever.retrieve(\"Describe the relationship between Alexander and Diogenes\")\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7166a1d-ea03-471e-9302-03435d64db66",
   "metadata": {},
   "source": [
    "## Step 3: Generate using these nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e9570e-6879-4278-900e-ffbb8a07068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63aebc5-fd19-4154-8782-50bae18e1052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:30,501 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Based on the text, Alexander and Diogenes had a brief but notable interaction. When Alexander met Diogenes, Diogenes simply requested that Alexander and his attendants \"stand out of the sunlight\" rather than asking for any favors or gifts. Alexander is said to have expressed admiration for Diogenes's conduct, showing respect for the philosopher's simple and independent nature. \n",
      "\n",
      "The text also quotes Alexander as saying, \"If I were not Alexander, I should like to be Diogenes,\" suggesting that Alexander respected Diogenes's philosophical approach to life and perhaps even envied his freedom from worldly concerns. The passage notes that this interaction shows that \"Alexander was not entirely destitute of better feelings,\" though he remained \"the slave of his insatiable ambition.\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"Use the following text to answer the given question.\"\n",
    "    )\n",
    "]\n",
    "messages += [\n",
    "    ChatMessage(role=\"system\", content=node.text) for node in retrieved_nodes\n",
    "]\n",
    "messages += [\n",
    "    ChatMessage(role=\"user\", content=\"Describe the relationship between Alexander and Diogenes.\")\n",
    "]\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ded63b-ff8a-46e5-8c99-40063d3c6989",
   "metadata": {},
   "source": [
    "## Llama Query engine to simplify Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50285303-18c8-4b9d-a2f8-8a7d6ea3967c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:34,846 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relationship between Alexander and Diogenes was marked by a notable encounter where Diogenes requested only that Alexander and his attendants stand out of his sunlight. Rather than being offended by this unusual request from someone addressing such a powerful figure, Alexander expressed admiration for Diogenes's conduct. \n",
      "\n",
      "This interaction reveals something about both men's characters. Alexander, despite his immense power and ambition, showed appreciation for Diogenes's simple and independent nature. In fact, Alexander is quoted as saying, \"If I were not Alexander, I should like to be Diogenes,\" suggesting a certain respect for the philosopher's way of life.\n",
      "\n",
      "While Alexander was described as \"not entirely destitute of better feelings,\" he was nonetheless characterized as \"the slave of his insatiable ambition,\" which contrasts with Diogenes's apparent contentment with merely having access to sunlight.\n"
     ]
    }
   ],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, llm=llm\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Describe the relationship between Alexander and Diogenes.\")\n",
    "response = {\n",
    "    \"answer\": str(response),\n",
    "    \"source_nodes\": response.source_nodes\n",
    "}\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16aaab66-54fc-4035-b6c3-75ad8c951bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: ee1ef41e-3e31-4e07-9949-5e585a50651c\n",
      "Text: But Diogenes said that he wanted nothing else, except that he\n",
      "and his attendants would stand out of the sunlight. Alexander is said\n",
      "to have expressed his admiration of Diogenes’s conduct.\n",
      "Score:  4.246\n",
      "\n",
      "Node ID: 31bab814-51cd-47cb-ab1a-eb7ab51bcdfc\n",
      "Text: 100 stades; and most of it is the mean between these\n",
      "breadths.[642] This river Indus Alexander crossed at daybreak with his\n",
      "army into the country of the Indians; concerning whom, in this history\n",
      "I have described neither what laws they enjoy,\n",
      "Score:  4.119\n",
      "\n",
      "Node ID: 005e2f98-94da-4b58-9486-5e7186584eb3\n",
      "Text: 32). Alexander said: “If I were not Alexander, I should like to\n",
      "be Diogenes.” Cf. _Arrian_, i. 1; Plutarch (_de Fortit. Alex._, p.\n",
      "331).\n",
      "Score:  3.640\n",
      "\n",
      "Node ID: 9946e837-31de-45bb-88ef-26132edd6f20\n",
      "Text: Alexander is said to have expressed his admiration of Diogenes’s\n",
      "conduct.[832] Thus it is evident that Alexander was not entirely\n",
      "destitute of better feelings; but he was the slave of his insatiable\n",
      "ambition.\n",
      "Score:  3.410\n",
      "\n",
      "Node ID: 9c5c9882-9eb2-440b-ae1b-89759b623a6e\n",
      "Text: He also ascertained that for the present Bessus held the supreme\n",
      "command, both on account of his relationship to Darius and because the\n",
      "war was being carried on in his viceregal province. Hearing this,\n",
      "Score:  3.255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in response['source_nodes']:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb82af5-5585-4685-a397-d76e3c9a8809",
   "metadata": {
    "tags": []
   },
   "source": [
    "## End to end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92e4c80-e2c6-47ae-a2de-37d0e8c614bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_query_engine(urls: [str], chunk_size: int) -> RetrieverQueryEngine:\n",
    "    gs = GutenbergSource()\n",
    "    index = Indexer(chunk_size=chunk_size, chunk_overlap=chunk_size//10)\n",
    "    \n",
    "    for url in urls:\n",
    "        doc = gs.load_from_url(url)\n",
    "        index.add_document_to_index(doc)\n",
    "    \n",
    "    retriever = BM25Retriever.from_defaults(\n",
    "        docstore=index.get_docstore(),\n",
    "        similarity_top_k=5)\n",
    "    \n",
    "    llm = Anthropic(\n",
    "        model=\"claude-3-7-sonnet-latest\",\n",
    "        api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever, llm=llm\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def print_response_to_query(query_engine: RetrieverQueryEngine, query: str):\n",
    "    response = query_engine.query(query)\n",
    "    response = {\n",
    "        \"answer\": str(response),\n",
    "        \"source_nodes\": response.source_nodes\n",
    "    }\n",
    "    print(response['answer'])\n",
    "    print(\"\\n\\n**Sources**:\")\n",
    "    for node in response['source_nodes']:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9184278-88c7-4174-82c4-7ee6bbdc511d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:34,899 - INFO - Indexer initialized\n",
      "2025-03-12 01:10:34,901 - INFO - Loading https://www.gutenberg.org/files/53669/53669-0.txt from cache\n",
      "2025-03-12 01:10:34,912 - INFO - Cleaned Gutenberg text: removed 50 chars from start, 49 chars from end\n",
      "2025-03-12 01:10:34,914 - INFO - Successfully loaded text from https://www.gutenberg.org/files/53669/53669-0.txt.\n",
      "2025-03-12 01:10:35,498 - INFO - Successfully loaded text from 9d9e10b3-bc9c-4c2d-a645-d95d0ff16755 -- 1208 nodes created.\n",
      "2025-03-12 01:10:35,995 - DEBUG - Building index from IDs objects\n",
      "2025-03-12 01:10:38,387 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the diaphragm is ruptured, you should replace the safety head with an unbroken head. Additionally, if you notice any tears, separation, or leaks occurring at the diaphragm, you should replace the entire valve-diaphragm assembly.\n",
      "\n",
      "When handling the diaphragm components, remember to unscrew the diaphragm cap by hand (not with a wrench) and be careful not to disturb the position of the yoke block by turning the needle, as this would affect the valve-needle adjustment.\n",
      "\n",
      "\n",
      "**Sources**:\n",
      "Node ID: 6afc9709-b53b-4fc9-8f7e-b1bbf198f9b0\n",
      "Text: Inspect to see if diaphragm is intact. If diaphragm is ruptured,\n",
      "replace the safety head with an unbroken head.\n",
      "Score:  4.869\n",
      "\n",
      "Node ID: b84ca5bf-c79d-4040-8766-7c528e693559\n",
      "Text: (3) Unscrew diaphragm cap and pull out washer, support, and\n",
      "valve-diaphragm assembly. To prevent loss of valve-needle adjustment\n",
      "(Fig 54), do not disturb position of yoke block by turning the needle.\n",
      "Score:  3.282\n",
      "\n",
      "Node ID: 1b07ee25-c1a2-412a-808c-2f5944ca3c99\n",
      "Text: (Fig 52) Screw on the diaphragm cap by hand. Do not use a\n",
      "wrench.   Install valve grip. (Par 74 _c_)    (4) Place valve spring\n",
      "over end of needle and install spring   retainer.\n",
      "Score:  2.676\n",
      "\n",
      "Node ID: 2484efb2-315b-48d6-9594-8718a980ae43\n",
      "Text: If the diaphragm   shows evidence of tears or separation, or if\n",
      "leaks occur at the   diaphragm, replace the valve-diaphragm assembly.\n",
      "Score:  2.472\n",
      "\n",
      "Node ID: 212bf098-9a76-458e-98e9-18c30c70df90\n",
      "Text: 75    Diaphragm cap,\n",
      "75    Diaphragm support,                                           75\n",
      "Diaphragm, valve, assembly,  10_b_, 48_b_, 56_e_, 58_a_, 74,\n",
      "Score:  2.421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine = build_query_engine([\"https://www.gutenberg.org/files/53669/53669-0.txt\"], 100) # Portable Flame Thrower\n",
    "print_response_to_query(query_engine, \"What should I do if the diaphragm is ruptured?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1e8f7-908c-49c1-8c66-8228c0b62465",
   "metadata": {},
   "source": [
    "## Limitation 1: Semantic Understanding\n",
    "\n",
    "Even though \"ruptured\" is the same as \"broken\", the returned nodes are very different because the search for \"broken\" doesn't return the sentences explaining what to do when it's ruptured (or vice-versa).\n",
    "As a result, the generated answer misses the key point about replacing the safety head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac057694-07a1-4ff8-af88-18d357f61e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:41,528 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the diaphragm is broken, you would need to replace the valve-diaphragm assembly. The proper procedure would involve unscrewing the diaphragm cap and removing the washer, support, and valve-diaphragm assembly. When doing this, it's important not to disturb the position of the yoke block by turning the needle, as this would affect the valve-needle adjustment. After replacing the broken components, you would need to reassemble by placing the valve spring over the end of the needle, installing the spring retainer, and then screwing the diaphragm cap back on by hand without using a wrench.\n",
      "\n",
      "\n",
      "**Sources**:\n",
      "Node ID: b84ca5bf-c79d-4040-8766-7c528e693559\n",
      "Text: (3) Unscrew diaphragm cap and pull out washer, support, and\n",
      "valve-diaphragm assembly. To prevent loss of valve-needle adjustment\n",
      "(Fig 54), do not disturb position of yoke block by turning the needle.\n",
      "Score:  3.282\n",
      "\n",
      "Node ID: 209034d7-2038-4eeb-a374-f00e84b9a575\n",
      "Text: (Par 49)    (2) _Spring-case assembly._ If outer case rotates\n",
      "and inner case does   not, and no spring action occurs, spring is\n",
      "broken and spring case   should be replaced as a unit.\n",
      "Score:  2.703\n",
      "\n",
      "Node ID: 1b07ee25-c1a2-412a-808c-2f5944ca3c99\n",
      "Text: (Fig 52) Screw on the diaphragm cap by hand. Do not use a\n",
      "wrench.   Install valve grip. (Par 74 _c_)    (4) Place valve spring\n",
      "over end of needle and install spring   retainer.\n",
      "Score:  2.676\n",
      "\n",
      "Node ID: 2126981f-ee90-437b-acfc-ea7061d688ea\n",
      "Text: If end of trigger rod is worn, replace   rod. Lug on the\n",
      "ignition-head body should be approximately 7/32 inch   high. If lug is\n",
      "worn or broken, replace ignition head body.  [Illustration: Fig 57.\n",
      "Score:  2.582\n",
      "\n",
      "Node ID: 9b7483ec-246c-4a9b-81e8-8944c15c495a\n",
      "Text: _c. Fuel._ Burn.  _d. Mixing apparatus._ Containers and filling\n",
      "lines may be rendered useless by ax or sledge blows, or by small-arms\n",
      "fire.  _e. Thickener._ Cans of thickener should be broken open.\n",
      "Score:  2.582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_response_to_query(query_engine, \"What should I do if the diaphragm is broken?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e5fe2-712a-48e9-852d-b9ba0e089a5c",
   "metadata": {},
   "source": [
    "## Limitation 2: Chunk size\n",
    "\n",
    "The results vary quite dramatically depending on the size of the chunks. It's unclear what size of chunk is best for a given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84cbe28-d0f7-4700-974e-67aea0376574",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:41,544 - INFO - Indexer initialized\n",
      "2025-03-12 01:10:41,546 - INFO - Loading https://www.gutenberg.org/files/53669/53669-0.txt from cache\n",
      "2025-03-12 01:10:41,554 - INFO - Cleaned Gutenberg text: removed 50 chars from start, 49 chars from end\n",
      "2025-03-12 01:10:41,556 - INFO - Successfully loaded text from https://www.gutenberg.org/files/53669/53669-0.txt.\n",
      "2025-03-12 01:10:42,118 - INFO - Successfully loaded text from 079005b9-1d47-42e8-8683-b6f994119c36 -- 1208 nodes created.\n",
      "2025-03-12 01:10:42,269 - DEBUG - Building index from IDs objects\n",
      "2025-03-12 01:10:44,814 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the diaphragm is ruptured, you should replace the safety head with an unbroken head. Additionally, if you notice any tears, separation, or leaks occurring at the diaphragm, you should replace the entire valve-diaphragm assembly.\n",
      "\n",
      "When handling the diaphragm components during maintenance, remember to unscrew the diaphragm cap by hand (not using a wrench) and be careful not to disturb the position of the yoke block by turning the needle, as this would affect the valve-needle adjustment.\n"
     ]
    }
   ],
   "source": [
    "def print_response(chunk_size: int) -> str:\n",
    "    query_engine = build_query_engine([\"https://www.gutenberg.org/files/53669/53669-0.txt\"],\n",
    "                                     chunk_size=chunk_size)\n",
    "    response = query_engine.query(\"What should I do if the diaphragm is ruptured?\")\n",
    "    print(response)\n",
    "\n",
    "print_response(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b09d9398-d5d1-4474-9637-7a6de03e2b70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:44,829 - INFO - Indexer initialized\n",
      "2025-03-12 01:10:44,833 - INFO - Loading https://www.gutenberg.org/files/53669/53669-0.txt from cache\n",
      "2025-03-12 01:10:44,845 - INFO - Cleaned Gutenberg text: removed 50 chars from start, 49 chars from end\n",
      "2025-03-12 01:10:44,847 - INFO - Successfully loaded text from https://www.gutenberg.org/files/53669/53669-0.txt.\n",
      "2025-03-12 01:10:45,141 - INFO - Successfully loaded text from 8ce56344-68a5-41b1-8390-f8833bd84443 -- 376 nodes created.\n",
      "2025-03-12 01:10:45,210 - DEBUG - Building index from IDs objects\n",
      "2025-03-12 01:10:47,360 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the diaphragm is ruptured, you should replace the safety head with an unbroken head. When inspecting the tank, you'll need to remove the deflector tube from the head (using your hand, not a wrench) to check if the diaphragm is intact. After replacing the head, you'll need to reassemble the plug, head, and deflector tube in the left fuel tank.\n"
     ]
    }
   ],
   "source": [
    "print_response(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb661796-a91d-454a-8b75-19a65f836e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:10:47,375 - INFO - Indexer initialized\n",
      "2025-03-12 01:10:47,376 - INFO - Loading https://www.gutenberg.org/files/53669/53669-0.txt from cache\n",
      "2025-03-12 01:10:47,387 - INFO - Cleaned Gutenberg text: removed 50 chars from start, 49 chars from end\n",
      "2025-03-12 01:10:47,389 - INFO - Successfully loaded text from https://www.gutenberg.org/files/53669/53669-0.txt.\n",
      "2025-03-12 01:10:47,611 - INFO - Successfully loaded text from 16fe36cd-d55f-4147-9000-a7d2c7a7d71c -- 124 nodes created.\n",
      "2025-03-12 01:10:47,656 - DEBUG - Building index from IDs objects\n",
      "2025-03-12 01:10:51,259 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you find that the diaphragm is ruptured, you should replace the safety head with an unbroken head. After replacement, you'll need to reassemble the plug, head, and deflector tube in the left fuel tank. When reinstalling, the deflector tube should face to the rear at a 45-degree angle to the operator's left shoulder. Remember to screw in the deflector tube by hand only (do not use a wrench on it), and then tighten the lock nut with a wrench.\n"
     ]
    }
   ],
   "source": [
    "print_response(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f832d-4ad9-4ddb-95db-cf717018094e",
   "metadata": {},
   "source": [
    "## Exploring tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "376a8db8-9c19-49e6-82fb-45805c0e94e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 01:18:19,536 - INFO - Indexer initialized\n",
      "2025-03-12 01:18:19,544 - INFO - Loading https://www.gutenberg.org/cache/epub/46976/pg46976.txt from cache\n",
      "2025-03-12 01:18:19,609 - INFO - Cleaned Gutenberg text: removed 1033 chars from start, 18492 chars from end\n",
      "2025-03-12 01:18:19,615 - INFO - Successfully loaded text from https://www.gutenberg.org/cache/epub/46976/pg46976.txt.\n",
      "2025-03-12 01:18:20,353 - INFO - Successfully loaded text from 29288a92-024f-4523-b578-8836461d3c6f -- 25 nodes created.\n",
      "2025-03-12 01:18:20,355 - INFO - Loading https://www.gutenberg.org/cache/epub/6400/pg6400.txt from cache\n",
      "2025-03-12 01:18:20,398 - INFO - Cleaned Gutenberg text: removed 917 chars from start, 18508 chars from end\n",
      "2025-03-12 01:18:20,399 - INFO - Successfully loaded text from https://www.gutenberg.org/cache/epub/6400/pg6400.txt.\n",
      "2025-03-12 01:18:21,259 - INFO - Successfully loaded text from 19703ae0-6c83-4faa-b337-2a384bcfb200 -- 35 nodes created.\n",
      "2025-03-12 01:18:21,261 - INFO - Loading https://www.gutenberg.org/cache/epub/3296/pg3296.txt from cache\n",
      "2025-03-12 01:18:21,280 - INFO - Cleaned Gutenberg text: removed 838 chars from start, 18499 chars from end\n",
      "2025-03-12 01:18:21,282 - INFO - Successfully loaded text from https://www.gutenberg.org/cache/epub/3296/pg3296.txt.\n",
      "2025-03-12 01:18:21,610 - INFO - Successfully loaded text from b0fc63cf-e9b3-457a-9b78-0975a73b5113 -- 16 nodes created.\n"
     ]
    }
   ],
   "source": [
    "gs = GutenbergSource()\n",
    "index = Indexer(chunk_size=10000, chunk_overlap=0)\n",
    "for url in [\n",
    "    \"https://www.gutenberg.org/cache/epub/46976/pg46976.txt\", # Alexander\n",
    "    \"https://www.gutenberg.org/cache/epub/6400/pg6400.txt\", # Twelve Caesars\n",
    "    \"https://www.gutenberg.org/cache/epub/3296/pg3296.txt\", # Augustine\n",
    "]:\n",
    "    doc = gs.load_from_url(url)\n",
    "    index.add_document_to_index(doc)\n",
    "docstore = index.get_docstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e51f654c-54cb-424d-821d-be7eafbaec19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "corpus = [str(value.text) for key, value in docstore.docs.items()]\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0512c53e-9af6-4603-a9c6-fb57e96f0947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astonishment'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.columns[3050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "312a47a4-edac-418f-b439-d648e2777ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "astonishment    0.074523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[['astonishment']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83c9b36e-256d-406a-aec1-ce11ba7d0fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: \"['describe', 'the', 'between', 'and'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tfidf_df[\"Describe the relationship between Alexander and Diogenes\".lower().split()].sum()\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebe05323-9773-41e4-8dd6-9bddeeb00865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationship    0.044188\n",
       "alexander       7.415023\n",
       "diogenes        0.113121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[\"relationship Alexander Diogenes\".lower().split()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db8aba00-d32f-4ad4-b89f-6fdde5e1f6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationship    0.044188\n",
       "wisdom          0.489559\n",
       "heaven          1.168821\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[\"relationship wisdom heaven\".lower().split()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5862064-5650-4548-a66d-1739060e1946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rome         2.315080\n",
       "macedonia    0.516891\n",
       "persia       0.199282\n",
       "india        0.580187\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[\"Rome Macedonia Persia India\".lower().split()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ad8bf-c1f6-436c-82b1-7bc65b2ef353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
