{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f4b00e-59c2-4496-a0e5-cd10dff0f0eb",
   "metadata": {},
   "source": [
    "## A very basic RAG\n",
    "\n",
    "You would never build a RAG system this basic. But it helps illustrate the problems we are trying to solve with some of the more advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41664a72-3e67-4e57-9e6d-2a01ed7ed8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet llama-index llama-index-retrievers-bm25 llama-index-llms-anthropic anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b078983c-cda3-46d0-8438-d284bc017b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"claude-3-7-haiku-latest\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "assert os.environ[\"ANTHROPIC_API_KEY\"][:2] == \"sk\",\\\n",
    "       \"Please specify the ANTHROPIC_API_KEY access token in keys.env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b39def9-ec4c-46e9-93f9-de6a5e739a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a2bf0-6974-4c6b-82a4-411fb87d7033",
   "metadata": {},
   "source": [
    "## Utility: cache urls to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ec0ef6-aa50-4731-86a2-5327dcde0d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Union, Tuple, Any\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class CacheManager:\n",
    "    \"\"\"\n",
    "    Manages the local cache for downloaded files.\n",
    "    \n",
    "    Attributes:\n",
    "        cache_dir (Path): Path to the cache directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"./.cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the cache manager.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir (str): Path to the cache directory. Defaults to \"./.cache\".\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self._ensure_cache_dir()\n",
    "    \n",
    "    def _ensure_cache_dir(self) -> None:\n",
    "        \"\"\"Create the cache directory if it doesn't exist.\"\"\"\n",
    "        if not self.cache_dir.exists():\n",
    "            self.cache_dir.mkdir(parents=True)\n",
    "            logger.info(f\"Created cache directory at {self.cache_dir}\")\n",
    "    \n",
    "    def _get_cache_filename(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a unique filename for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to generate a filename for.\n",
    "            \n",
    "        Returns:\n",
    "            str: A unique filename based on the URL.\n",
    "        \"\"\"\n",
    "        # Extract the filename from the URL if possible\n",
    "        parsed_url = urlparse(url)\n",
    "        path_parts = parsed_url.path.split('/')\n",
    "        original_filename = path_parts[-1] if path_parts[-1] else \"index\"\n",
    "        \n",
    "        # Create a hash of the URL to ensure uniqueness\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n",
    "        \n",
    "        # Combine original filename with hash\n",
    "        if '.' in original_filename:\n",
    "            name_parts = original_filename.split('.')\n",
    "            extension = name_parts[-1]\n",
    "            base_name = '.'.join(name_parts[:-1])\n",
    "            return f\"{base_name}_{url_hash}.{extension}\"\n",
    "        else:\n",
    "            return f\"{original_filename}_{url_hash}.txt\"\n",
    "    \n",
    "    def get_cache_path(self, url: str) -> Path:\n",
    "        \"\"\"\n",
    "        Get the cache path for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to get the cache path for.\n",
    "            \n",
    "        Returns:\n",
    "            Path: The path where the cached file would be stored.\n",
    "        \"\"\"\n",
    "        filename = self._get_cache_filename(url)\n",
    "        return self.cache_dir / filename\n",
    "    \n",
    "    def is_cached(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a URL is already cached.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to check.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the URL is cached, False otherwise.\n",
    "        \"\"\"\n",
    "        cache_path = self.get_cache_path(url)\n",
    "        return cache_path.exists()\n",
    "    \n",
    "    def get_cached_content(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get the cached content for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to get the cached content for.\n",
    "            \n",
    "        Returns:\n",
    "            Optional[str]: The cached content if available, None otherwise.\n",
    "        \"\"\"\n",
    "        if not self.is_cached(url):\n",
    "            return None\n",
    "        \n",
    "        cache_path = self.get_cache_path(url)\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error reading cached file for {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cache_content(self, url: str, content: str) -> bool:\n",
    "        \"\"\"\n",
    "        Cache content for a URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL the content was downloaded from.\n",
    "            content (str): The content to cache.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if caching was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        self._ensure_cache_dir()\n",
    "        cache_path = self.get_cache_path(url)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            logger.info(f\"Cached content for {url} at {cache_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error caching content for {url}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def clear_cache(self) -> bool:\n",
    "        \"\"\"\n",
    "        Clear all cached files.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if clearing was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.cache_dir.exists():\n",
    "                for file_path in self.cache_dir.iterdir():\n",
    "                    if file_path.is_file():\n",
    "                        file_path.unlink()\n",
    "                logger.info(\"Cache cleared successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clearing cache: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_cache_size(self) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Get the total size of the cache.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[int, str]: A tuple containing the size in bytes and a human-readable size.\n",
    "        \"\"\"\n",
    "        total_size = 0\n",
    "        \n",
    "        if self.cache_dir.exists():\n",
    "            for file_path in self.cache_dir.iterdir():\n",
    "                if file_path.is_file():\n",
    "                    total_size += file_path.stat().st_size\n",
    "        \n",
    "        # Convert to human-readable format\n",
    "        units = ['B', 'KB', 'MB', 'GB']\n",
    "        size_human = total_size\n",
    "        unit_index = 0\n",
    "        \n",
    "        while size_human > 1024 and unit_index < len(units) - 1:\n",
    "            size_human /= 1024\n",
    "            unit_index += 1\n",
    "        \n",
    "        human_readable = f\"{size_human:.2f} {units[unit_index]}\"\n",
    "        return total_size, human_readable\n",
    "    \n",
    "    def list_cached_files(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List all cached files with metadata.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries containing file information.\n",
    "        \"\"\"\n",
    "        files_info = []\n",
    "        \n",
    "        if self.cache_dir.exists():\n",
    "            for file_path in self.cache_dir.iterdir():\n",
    "                if file_path.is_file():\n",
    "                    stat = file_path.stat()\n",
    "                    files_info.append({\n",
    "                        'filename': file_path.name,\n",
    "                        'path': str(file_path),\n",
    "                        'size_bytes': stat.st_size,\n",
    "                        'last_modified': time.ctime(stat.st_mtime)\n",
    "                    })\n",
    "        \n",
    "        return files_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7ecb0-96e6-4671-a017-1bcd29526480",
   "metadata": {},
   "source": [
    "## Step 1: Index document\n",
    "\n",
    "We will break up the document as sentences, and index it using BM25\n",
    "See: https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5237a6dc-5eb2-4bc6-86c1-4541105cb2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Document, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.indices.document_summary import DocumentSummaryIndex\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "class GutenbergTextLoadError(Exception):\n",
    "    \"\"\"Exception raised for errors in loading Gutenberg text files.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GutenbergToBM25LlamaIndex:\n",
    "    \"\"\"\n",
    "    A class to load text files from Project Gutenberg into LlamaIndex using BM25.\n",
    "    \n",
    "    This class handles fetching text content from URLs, processing Gutenberg-specific\n",
    "    formatting, and creating a document store indexed by BM25.\n",
    "    \n",
    "    Attributes:\n",
    "        cache_manager (CacheManager): Manager for the local cache.\n",
    "        chunk_size (int): Size of text chunks for processing.\n",
    "        chunk_overlap (int): Overlap between text chunks.\n",
    "        docstore (SimpleDocumentStore): Document store for storing processed documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \"./.cache\",\n",
    "        chunk_size: int = 1024,\n",
    "        chunk_overlap: int = 20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GutenbergToBM25LlamaIndex loader.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Size of text chunks for processing. Defaults to 1024.\n",
    "            chunk_overlap (int): Overlap between text chunks. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.cache_manager = CacheManager(cache_dir)\n",
    "        self.anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.default_url = \"https://www.gutenberg.org/files/53669/53669-0.txt\"\n",
    "        \n",
    "        # Initialize a simple document store\n",
    "        self.docstore = SimpleDocumentStore()\n",
    "        \n",
    "        Settings.node_parser = SentenceSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        \n",
    "        logger.info(\"GutenbergToBM25LlamaIndex initialized with Anthropic Haiku model\")\n",
    "    \n",
    "    def fetch_text_from_url(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch text content from a URL with caching\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to fetch text from.\n",
    "            \n",
    "        Returns:\n",
    "            str: Text content from the URL.\n",
    "            \n",
    "        Raises:\n",
    "            GutenbergTextLoadError: If there's an error fetching or processing the URL.\n",
    "        \"\"\"\n",
    "        if self.cache_manager.is_cached(url):\n",
    "            logger.info(f\"Loading {url} from cache\")\n",
    "            cached_content = self.cache_manager.get_cached_content(url)\n",
    "            if cached_content:\n",
    "                return cached_content\n",
    "            logger.warning(f\"Cached content for {url} could not be read, downloading again\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Fetching text from URL: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check if content is text\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/plain' not in content_type and 'text/html' not in content_type:\n",
    "                raise GutenbergTextLoadError(f\"URL does not contain text content: {content_type}\")\n",
    "            \n",
    "            # Detect encoding or use utf-8 as fallback\n",
    "            encoding = response.encoding or 'utf-8'\n",
    "            content = response.content.decode(encoding)\n",
    "        \n",
    "            # Cache the downloaded content\n",
    "            self.cache_manager.cache_content(url, content)\n",
    "            \n",
    "            return content\n",
    "        except requests.RequestException as e:\n",
    "            raise GutenbergTextLoadError(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            raise GutenbergTextLoadError(f\"Error decoding content from {url}: {str(e)}\")\n",
    "    \n",
    "    def clean_gutenberg_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean Project Gutenberg text by removing headers, footers, and license information.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text from Project Gutenberg.\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text with Gutenberg-specific content removed.\n",
    "        \"\"\"\n",
    "        # Pattern to find the start of the actual content (after header)\n",
    "        start_markers = [\n",
    "            r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\* START OF THE PROJECT GUTENBERG .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\*START OF THE PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\"\n",
    "        ]\n",
    "        \n",
    "        # Pattern to find the end of the content (before footer)\n",
    "        end_markers = [\n",
    "            r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"\\*\\*\\* END OF THE PROJECT GUTENBERG .+? \\*\\*\\*\", \n",
    "            r\"\\*\\*\\*END OF THE PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\",\n",
    "            r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\"\n",
    "        ]\n",
    "        \n",
    "        # Find start of content\n",
    "        start_pos = 0\n",
    "        for marker in start_markers:\n",
    "            match = re.search(marker, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                start_pos = match.end()\n",
    "                break\n",
    "        \n",
    "        # Find end of content\n",
    "        end_pos = len(text)\n",
    "        for marker in end_markers:\n",
    "            match = re.search(marker, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                end_pos = match.start()\n",
    "                break\n",
    "        \n",
    "        # Extract and clean the content\n",
    "        content = text[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "        \n",
    "        logger.info(f\"Cleaned Gutenberg text: removed {start_pos} chars from start, \"\n",
    "                   f\"{len(text) - end_pos} chars from end\")\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def load_from_url(self, url: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Load text from a URL and create a BM25 retriever.\n",
    "        \n",
    "        Args:\n",
    "            url (str, optional): URL to load text from. If None, uses the default URL.\n",
    "            \n",
    "        Returns:\n",
    "            BM25Retriever: A BM25 retriever configured with the loaded documents.\n",
    "            \n",
    "        Raises:\n",
    "            GutenbergTextLoadError: If there's an error loading or processing the text.\n",
    "        \"\"\"\n",
    "        url = url or self.default_url\n",
    "        \n",
    "        try:\n",
    "            # Fetch and clean the text\n",
    "            raw_text = self.fetch_text_from_url(url)\n",
    "            cleaned_text = self.clean_gutenberg_text(raw_text)\n",
    "            \n",
    "            # Create a document with metadata\n",
    "            parsed_url = urlparse(url)\n",
    "            filename = os.path.basename(parsed_url.path)\n",
    "            \n",
    "            document = Document(\n",
    "                text=cleaned_text,\n",
    "                metadata={\n",
    "                    \"source\": url,\n",
    "                    \"filename\": filename,\n",
    "                    \"date_loaded\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Parse the document into nodes\n",
    "            nodes = Settings.node_parser.get_nodes_from_documents([document])\n",
    "            \n",
    "            # Add nodes to the document store\n",
    "            self.docstore.add_documents(nodes)\n",
    " \n",
    "            logger.info(f\"Successfully loaded text from {url} -- {len(nodes)} nodes created.\")\n",
    " \n",
    "        except Exception as e:\n",
    "            raise GutenbergTextLoadError(f\"Error loading from URL {url}: {str(e)}\")\n",
    "            \n",
    "    def get_indexed_documents(self) -> SimpleDocumentStore:\n",
    "        return self.docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aff170f-4d87-4fb7-ad75-b9b33e88bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 00:07:10,918 - INFO - GutenbergToBM25LlamaIndex initialized with Anthropic Haiku model\n",
      "2025-03-11 00:07:10,920 - INFO - Loading https://www.gutenberg.org/files/53669/53669-0.txt from cache\n",
      "2025-03-11 00:07:10,930 - INFO - Cleaned Gutenberg text: removed 50 chars from start, 49 chars from end\n",
      "2025-03-11 00:07:11,525 - INFO - Successfully loaded text from https://www.gutenberg.org/files/53669/53669-0.txt -- 1369 nodes created.\n"
     ]
    }
   ],
   "source": [
    "index = GutenbergToBM25LlamaIndex(chunk_size=100, chunk_overlap=20)\n",
    "index.load_from_url(\"https://www.gutenberg.org/files/53669/53669-0.txt\") # Portable Flame Thrower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d50e0-9881-48a6-b5ff-c8a78ef3de06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Retrieve nodes that match query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da943d0-1ca9-4d23-a9f2-5629b7ad43e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 00:07:11,657 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.get_indexed_documents(),\n",
    "    similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9184278-88c7-4174-82c4-7ee6bbdc511d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** d6e235d3-6f40-4122-b5bb-96d532612423<br>**Similarity:** 4.394947528839111<br>**Text:** Remove deflector\n",
       "tube from head (using hand, not wrench). Inspect to see if diaphragm\n",
       "is intact. If diaphragm is ruptured, replace the safety head with an\n",
       "unbroken head.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** f110aa83-8e72-46ac-b87e-fd8d0347ac8b<br>**Similarity:** 3.875696897506714<br>**Text:** If diaphragm is ruptured, replace the safety head with an\n",
       "unbroken head. (Par 69 b, c) Reassemble plug, head, and deflector tube\n",
       "in left fuel tank.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 47042443-c65f-4a51-9e54-37b80ea77176<br>**Similarity:** 3.2352800369262695<br>**Text:** (3) Unscrew diaphragm cap and pull out washer, support, and\n",
       "  valve-diaphragm assembly. To prevent loss of valve-needle adjustment\n",
       "  (Fig 54), do not disturb position of yoke block by turning the needle.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c6183220-083f-492e-8ade-a044cf6508c2<br>**Similarity:** 2.645098924636841<br>**Text:** 46_f_\n",
       "    thickener,                                              46_e_\n",
       "\n",
       "  Diaphragm,                                                   75\n",
       "\n",
       "  Diaphragm cap,                                               75\n",
       "\n",
       "  Diaphragm support,                                           75\n",
       "\n",
       "  Diaphragm, valve, assembly,  10_b_,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 64705a2f-a69f-4418-b4ef-30ad8ff669ec<br>**Similarity:** 2.6266260147094727<br>**Text:** (Fig 52) Screw on the diaphragm cap by hand. Do not use a wrench.\n",
       "  Install valve grip. (Par 74 _c_)\n",
       "\n",
       "  (4) Place valve spring over end of needle and install spring\n",
       "  retainer.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "retrieved_nodes = retriever.retrieve(\"What should I do if the diaphragm is ruptured?\")\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7166a1d-ea03-471e-9302-03435d64db66",
   "metadata": {},
   "source": [
    "## Step 3: Generate using these nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e9570e-6879-4278-900e-ffbb8a07068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63aebc5-fd19-4154-8782-50bae18e1052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 00:07:14,009 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: If the diaphragm is ruptured, you should replace the entire safety head with an unbroken head. Do not attempt to replace just the diaphragm - the manual specifically instructs to replace the complete safety head assembly when a ruptured diaphragm is found.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a mechanic. Use the information from the manual to answer the given query.\"\n",
    "    )\n",
    "]\n",
    "messages += [\n",
    "    ChatMessage(role=\"system\", content=node.text) for node in retrieved_nodes\n",
    "]\n",
    "messages += [\n",
    "    ChatMessage(role=\"user\", content=\"What should I do if the diaphragm is ruptured?\")\n",
    "]\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ded63b-ff8a-46e5-8c99-40063d3c6989",
   "metadata": {},
   "source": [
    "## Llama Query engine to simplify Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50285303-18c8-4b9d-a2f8-8a7d6ea3967c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 00:07:15,626 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the diaphragm is ruptured, you should replace the safety head with an unbroken head. After replacement, you'll need to reassemble the plug, head, and deflector tube in the left fuel tank.\n"
     ]
    }
   ],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, llm=llm\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What should I do if the diaphragm is ruptured?\")\n",
    "response = {\n",
    "    \"answer\": str(response),\n",
    "    \"source_nodes\": response.source_nodes\n",
    "}\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16aaab66-54fc-4035-b6c3-75ad8c951bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: d6e235d3-6f40-4122-b5bb-96d532612423\n",
      "Text: Remove deflector tube from head (using hand, not wrench).\n",
      "Inspect to see if diaphragm is intact. If diaphragm is ruptured,\n",
      "replace the safety head with an unbroken head.\n",
      "Score:  4.395\n",
      "\n",
      "Node ID: f110aa83-8e72-46ac-b87e-fd8d0347ac8b\n",
      "Text: If diaphragm is ruptured, replace the safety head with an\n",
      "unbroken head. (Par 69 b, c) Reassemble plug, head, and deflector tube\n",
      "in left fuel tank.\n",
      "Score:  3.876\n",
      "\n",
      "Node ID: 47042443-c65f-4a51-9e54-37b80ea77176\n",
      "Text: (3) Unscrew diaphragm cap and pull out washer, support, and\n",
      "valve-diaphragm assembly. To prevent loss of valve-needle adjustment\n",
      "(Fig 54), do not disturb position of yoke block by turning the needle.\n",
      "Score:  3.235\n",
      "\n",
      "Node ID: c6183220-083f-492e-8ade-a044cf6508c2\n",
      "Text: 46_f_     thickener,\n",
      "46_e_    Diaphragm,\n",
      "75    Diaphragm cap,                                               75\n",
      "Diaphragm support,                                           75\n",
      "Diaphragm, valve, assembly,  10_b_,\n",
      "Score:  2.645\n",
      "\n",
      "Node ID: 64705a2f-a69f-4418-b4ef-30ad8ff669ec\n",
      "Text: (Fig 52) Screw on the diaphragm cap by hand. Do not use a\n",
      "wrench.   Install valve grip. (Par 74 _c_)    (4) Place valve spring\n",
      "over end of needle and install spring   retainer.\n",
      "Score:  2.627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in response['source_nodes']:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b3798-a31a-441c-8e2c-3349aeba15c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
