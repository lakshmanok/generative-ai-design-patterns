"""
Knowledge Distillation for Small Language Models with 4-bit Quantization

This script implements knowledge distillation to train a smaller "student" model
using guidance from a larger "teacher" model. The student model is fully fine-tuned
and then optionally quantized to 4-bit precision for maximum efficiency.

The training data is loaded from JSON files generated by generate_training_set.py.

Key Features:
- Loads pre-generated Python code examples from JSON
- Implements knowledge distillation using custom DistillationTrainer
- Fully fine-tunes the student model (no LoRA)
- Supports DDP for multi-GPU training (teacher model replicated, student model DDP)
- Automatically quantizes the distilled model to 4-bit precision
- Saves both full precision and 4-bit quantized models

Default Configuration:
- Teacher: google/gemma-3-12b-it (12B parameters)
- Student: google/gemma-3-1b-it (1B parameters)
- Uses BF16 precision if supported, otherwise FP32
- 4-bit quantization using bitsandbytes NF4 format

Memory Requirements:
- Training: ~26-28 GB VRAM (9B teacher + 2B student)
- 4-bit quantized model: ~0.5-0.7 GB VRAM

Prerequisites:
1. Run generate_training_set.py first to create training data
2. Ensure sufficient GPU memory (e.g., A100 40GB or RTX 4090)
3. Install bitsandbytes for 4-bit quantization: pip install bitsandbytes

Usage:
    python examples/24_small_language_models/knowledge_distillation.py

The script will:
1. Load training data from training_data/python_code_examples.json
2. Prepare teacher (9B) and student (2B) models
3. Fully fine-tune the student model using knowledge distillation
4. Save the distilled model to ./distilled_model/
5. Quantize the model to 4-bit and save to ./distilled_model_4bit/
6. Display size comparisons and usage instructions

Example workflow:
    # First, generate training data
    python generate_training_set.py

    # Then run knowledge distillation and quantization
    python examples/24_small_language_models/knowledge_distillation.py

    # The script produces two models:
    # - ./distilled_model/        (full precision distilled model)
    # - ./distilled_model_4bit/   (4-bit quantized model, ~75% smaller)
"""

import os
import sys

# Comprehensive XLA disabling - must be set before importing anything else
os.environ["USE_XLA"] = "0"
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Force single GPU to avoid XLA multi-device
# Disable JAX/XLA completely
os.environ["JAX_PLATFORMS"] = ""
os.environ["JAX_PLATFORM_NAME"] = "cpu"
# Additional XLA disabling
os.environ["PJRT_DEVICE"] = ""
os.environ["XRT_TPU_CONFIG"] = ""
os.environ["TPU_NAME"] = ""
os.environ["TPU_ZONE"] = ""
os.environ["USE_TORCH_XLA"] = "0"

# Check if torch_xla is installed and provide instructions to uninstall
try:
    import torch_xla
    print("⚠️  WARNING: torch_xla is installed in your environment!")
    print("   This can cause conflicts with CUDA operations.")
    print("   To fix this issue, please uninstall torch_xla:")
    print("   $ pip uninstall torch_xla torch_xla_cuda_plugin -y")
    print("   Then restart your Python kernel/session and run this script again.")
    sys.exit(1)
except ImportError:
    # Good, torch_xla is not installed
    pass

# Fix for transformers compatibility issue with Gemma-3 models
from transformers import modeling_utils
if not hasattr(modeling_utils, "ALL_PARALLEL_STYLES") or modeling_utils.ALL_PARALLEL_STYLES is None:
    modeling_utils.ALL_PARALLEL_STYLES = ["tp", "none", "colwise", "rowwise"]

import json
from typing import List, Dict, Any
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from transformers import BitsAndBytesConfig
import torch
import time

# Ensure we're using CUDA backend
if torch.cuda.is_available():
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True
    # Don't set default device - let the trainer handle device placement
    # This was causing tensors to be created on GPU by default

# Import functions from generate_training_set.py
from generate_training_set import load_training_examples, load_dataset_from_saved_examples, DEFAULT_TRAINING_DATA_PATH

# Add memory management settings
torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def validate_training_data(filepath: str = DEFAULT_TRAINING_DATA_PATH) -> bool:
    """
    Validate that training data exists and provide helpful instructions if not.

    Args:
        filepath: Path to the training data JSON file

    Returns:
        True if training data exists and is valid, False otherwise
    """
    if not os.path.exists(filepath):
        print(f"❌ Training data not found at: {filepath}")
        print("\n📋 To create training data, follow these steps:")
        print("1. Run: python generate_training_set.py")
        print("2. Wait for the script to generate Python code examples")
        print("3. Then run this script again")
        print(f"\nExpected file location: {os.path.abspath(filepath)}")
        return False

    try:
        with open(filepath, 'r') as f:
            data = json.load(f)

        examples = data.get("examples", [])
        if not examples:
            print(f"❌ No training examples found in {filepath}")
            return False

        print(f"✅ Found {len(examples)} training examples in {filepath}")
        return True

    except json.JSONDecodeError:
        print(f"❌ Invalid JSON format in {filepath}")
        return False
    except Exception as e:
        print(f"❌ Error reading training data: {str(e)}")
        return False

class DistillationTrainer(Trainer):
    def __init__(
        self,
        teacher_model: AutoModelForCausalLM,
        temperature: float = 2.0,
        alpha: float = 0.5,  # Weight for distillation loss
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model
        self.temperature = temperature
        self.alpha = alpha

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        # Ensure inputs are on the correct device (CUDA)
        device = next(model.parameters()).device

        # Move all inputs to the correct device if needed
        inputs_on_device = {}
        for k, v in inputs.items():
            if isinstance(v, torch.Tensor):
                if v.device != device:
                    inputs_on_device[k] = v.to(device)
                else:
                    inputs_on_device[k] = v
            else:
                inputs_on_device[k] = v

        # Get student model outputs
        student_outputs = model(**inputs_on_device)
        student_logits = student_outputs.logits

        # Get teacher model outputs (no gradient computation)
        # Ensure teacher model is on the same device
        teacher_device = next(self.teacher_model.parameters()).device
        if teacher_device != device:
            # This shouldn't happen, but just in case
            print(f"Warning: Teacher device {teacher_device} != Student device {device}")

        with torch.no_grad():
            # Use CUDA stream for teacher inference to avoid XLA
            if device.type == 'cuda':
                with torch.cuda.device(device):
                    teacher_outputs = self.teacher_model(**inputs_on_device)
            else:
                teacher_outputs = self.teacher_model(**inputs_on_device)
            teacher_logits = teacher_outputs.logits

        # Calculate task loss (language modeling loss)
        task_loss = student_outputs.loss

        # Calculate distillation loss (KL divergence)
        # Scale logits by temperature
        student_logits_scaled = student_logits / self.temperature
        teacher_logits_scaled = teacher_logits / self.temperature

        # Handle vocabulary size mismatch
        # Get the minimum vocabulary size between teacher and student
        student_vocab_size = student_logits_scaled.size(-1)
        teacher_vocab_size = teacher_logits_scaled.size(-1)

        if student_vocab_size != teacher_vocab_size:
            # Align vocabulary sizes by using only the common tokens
            min_vocab_size = min(student_vocab_size, teacher_vocab_size)
            student_logits_scaled = student_logits_scaled[..., :min_vocab_size]
            teacher_logits_scaled = teacher_logits_scaled[..., :min_vocab_size]

            # Log the vocabulary size mismatch (only once)
            if not hasattr(self, '_vocab_size_warning_logged'):
                print(f"⚠️  Vocabulary size mismatch detected:")
                print(f"   Student vocab size: {student_vocab_size}")
                print(f"   Teacher vocab size: {teacher_vocab_size}")
                print(f"   Using common vocabulary of size: {min_vocab_size}")
                self._vocab_size_warning_logged = True

        # Calculate KL divergence loss
        distillation_loss = torch.nn.functional.kl_div(
            torch.log_softmax(student_logits_scaled, dim=-1),
            torch.softmax(teacher_logits_scaled, dim=-1),
            reduction='batchmean'
        ) * (self.temperature ** 2)  # Scale back

        # Combine losses
        loss = (1 - self.alpha) * task_loss + self.alpha * distillation_loss

        return (loss, student_outputs) if return_outputs else loss

def prepare_models(
    teacher_model_name: str,
    student_model_name: str,
    current_process_device: torch.device
) -> tuple[AutoModelForCausalLM, AutoModelForCausalLM, AutoTokenizer]:
    """
    Load and prepare both teacher and student models for distillation.
    Both models will be placed on the same device.

    Args:
        teacher_model_name: Name of the teacher model.
        student_model_name: Name of the student model.
        current_process_device: The torch.device to use.

    Returns:
        Tuple of (teacher_model, student_model, tokenizer).
    """
    print(f"Loading models on device: {current_process_device}")

    # Ensure XLA dispatch is disabled
    if hasattr(torch, '_dispatch_keys'):
        # Remove XLA dispatch key if it exists
        xla_key = getattr(torch._C, 'DispatchKey', None)
        if xla_key and hasattr(xla_key, 'XLA'):
            torch._C._dispatch_tls_set_dispatch_key_excluded(xla_key.XLA, True)

    # Determine the dtype to use based on BF16 support
    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    model_dtype = torch.bfloat16 if use_bf16 else torch.float32
    print(f"Using dtype: {model_dtype}")

    # Load tokenizer
    print(f"Loading tokenizer for {student_model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(
        student_model_name,
        trust_remote_code=True,
        padding_side="right",
        truncation_side="right"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # TEACHER MODEL - Load directly on the target device
    print(f"Loading TEACHER model '{teacher_model_name}' on {current_process_device}...")

    # Load teacher model without device_map or quantization to avoid XLA
    teacher_model = AutoModelForCausalLM.from_pretrained(
        teacher_model_name,
        torch_dtype=model_dtype,  # Use appropriate dtype
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        attn_implementation="eager",  # Use eager attention for Gemma2 models
    )

    # Manually move to device and ensure it's in CUDA mode
    teacher_model = teacher_model.to(current_process_device)
    teacher_model.eval()

    # Ensure all parameters are on the correct device
    for param in teacher_model.parameters():
        param.data = param.data.to(current_process_device)

    # STUDENT MODEL - Load and place on the same device
    print(f"Loading STUDENT model '{student_model_name}' on {current_process_device}...")
    student_model = AutoModelForCausalLM.from_pretrained(
        student_model_name,
        torch_dtype=model_dtype,  # Use appropriate dtype
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        attn_implementation="eager",  # Use eager attention for Gemma2 models
    )

    # Move student model to the same device as teacher
    if current_process_device.type == "cuda":
        student_model = student_model.to(current_process_device)

    # Ensure all parameters are on the correct device
    for param in student_model.parameters():
        param.data = param.data.to(current_process_device)

    # Memory check
    if current_process_device.type == "cuda" and torch.cuda.is_available():
        print(f"\nGPU Memory Usage on {current_process_device}:")
        print(f"  Allocated: {torch.cuda.memory_allocated(current_process_device) / 1024**3:.2f} GB")
        print(f"  Reserved:  {torch.cuda.memory_reserved(current_process_device) / 1024**3:.2f} GB")

    return teacher_model, student_model, tokenizer

def main():
    """
    Main function to perform knowledge distillation using pre-generated training data.
    Both teacher and student models will be placed on the same GPU.
    """
    print("=" * 60)
    print("Knowledge Distillation for Small Language Models")
    print("=" * 60)

    # Setup device - use first available GPU or CPU
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(0)
        print(f"🚀 Using GPU: {device}")
    else:
        device = torch.device("cpu")
        print(f"🏃 Using CPU: {device}")

    # Model Configuration - Using Gemma-3 models on A100-80GB
    TEACHER_MODEL_NAME = "google/gemma-3-12b-it"     # 12B teacher model (~24GB in FP16)
    STUDENT_MODEL_NAME = "google/gemma-3-1b-it"      # 1B student model (~2GB in FP16)
    OUTPUT_DIR = "./distilled_model"

    # Validate that training data exists
    print("\n🔍 Validating training data...")
    if not validate_training_data(DEFAULT_TRAINING_DATA_PATH):
        return

    # Load dataset from pre-generated JSON file (each process loads the data)
    print("\n📚 Loading training dataset from saved examples...")
    dataset = load_dataset_from_saved_examples(DEFAULT_TRAINING_DATA_PATH)

    if dataset is None:
        print(f"❌ Failed to load dataset from {DEFAULT_TRAINING_DATA_PATH}")
        return

    print(f"✅ Successfully loaded dataset with {len(dataset['train'])} training examples")
    print(f"    and {len(dataset['test'])} validation examples")

    # Prepare models and tokenizer
    print("\n🤖 Preparing teacher and student models...")
    try:
        teacher_model, student_model, tokenizer = prepare_models(
            teacher_model_name=TEACHER_MODEL_NAME,
            student_model_name=STUDENT_MODEL_NAME,
            current_process_device=device
        )
        print("✅ Models prepared successfully on all processes.")
    except torch.cuda.OutOfMemoryError as e:
        print(f"\n❌ Out of GPU memory on {device}! Error: {e}")
        print("   Try using smaller models or GPUs with more memory.")
        return
    except Exception as e:
        print(f"\n❌ Error loading models on {device}: {str(e)}")
        return

    # Tokenize the dataset (after models and tokenizer are prepared)
    print("\n📜 Tokenizing dataset...")
    def tokenize_function(examples):
        # Don't return tensors here - let the data collator handle tensor creation
        # This avoids creating tensors on the wrong device
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=1024
        )
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names
    )
    print("✅ Dataset tokenized successfully.")

    # Set up training arguments (adjust output_dir if running multiple DDP jobs)
    # Check if BF16 is supported on the current GPU
    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()

    print(f"Mixed precision: {'BF16' if use_bf16 else 'FP32 (mixed precision disabled)'}")

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        weight_decay=0.01,
        warmup_ratio=0.1,
        logging_steps=10,
        disable_tqdm=False,
        save_steps=None,
        eval_steps=100,
        eval_strategy="steps",
        save_strategy="no",        # Disable automatic saves
        load_best_model_at_end=False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        bf16=use_bf16,  # Use BF16 if supported
        fp16=False,  # Disable FP16 to avoid gradient scaling issues
        report_to="tensorboard",
        dataloader_pin_memory=False,  # Disable pin memory to avoid conflicts with GPU tensors
    )

    # Initialize distillation trainer
    trainer = DistillationTrainer(
        teacher_model=teacher_model,
        model=student_model, # Pass the student model here, Trainer handles DDP wrapping
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        tokenizer=tokenizer # Pass tokenizer for saving
    )

    # Train the model
    print("\n🎓 Starting knowledge distillation training...")
    trainer.train()

    # Save the final model
    print("\n💾 Saving final model...")

    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Get the actual model (unwrap from DDP if needed)
    if hasattr(trainer.model, 'module'):
        # If wrapped in DDP, get the underlying model
        model_to_save = trainer.model.module
    else:
        model_to_save = trainer.model

    # Save model state dict directly (avoids DTensor import)
    torch.save(model_to_save.state_dict(), os.path.join(OUTPUT_DIR, "pytorch_model.bin"))

    # Save config
    model_to_save.config.save_pretrained(OUTPUT_DIR)

    # Save tokenizer
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"✅ Model weights saved to {OUTPUT_DIR}/pytorch_model.bin")
    print(f"✅ Config saved to {OUTPUT_DIR}/config.json")
    print(f"✅ Tokenizer saved to {OUTPUT_DIR}/")

    # Save training metrics
    metrics = trainer.evaluate(eval_dataset=tokenized_dataset["test"])
    with open(os.path.join(OUTPUT_DIR, "training_metrics.json"), "w") as f:
        json.dump(metrics, f, indent=2)

    print(f"\n🎉 Knowledge distillation completed successfully!")
    print(f"   The distilled model has been saved to '{OUTPUT_DIR}'")
    print(f"   Final evaluation metrics: {metrics}")

    # ============== 4-bit Quantization ==============
    print(f"\n📦 Quantizing the distilled student model to 4-bit...")

    # Clear GPU memory from training
    del trainer
    del teacher_model
    del student_model
    torch.cuda.empty_cache()

    # Define 4-bit quantization config
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,
        bnb_4bit_use_double_quant=True,  # Double quantization for even more compression
        bnb_4bit_quant_type="nf4",  # NormalFloat4 quantization
    )

    # Load the distilled model with 4-bit quantization
    print(f"Loading distilled model from '{OUTPUT_DIR}' with 4-bit quantization...")
    try:
        quantized_model = AutoModelForCausalLM.from_pretrained(
            OUTPUT_DIR,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="eager",
        )

        # Create output directory for quantized model
        QUANTIZED_OUTPUT_DIR = f"{OUTPUT_DIR}_4bit"
        os.makedirs(QUANTIZED_OUTPUT_DIR, exist_ok=True)

        # Save the quantized model
        print(f"Saving 4-bit quantized model to '{QUANTIZED_OUTPUT_DIR}'...")
        quantized_model.save_pretrained(QUANTIZED_OUTPUT_DIR)
        tokenizer.save_pretrained(QUANTIZED_OUTPUT_DIR)

        # Save quantization config
        with open(os.path.join(QUANTIZED_OUTPUT_DIR, "quantization_config.json"), "w") as f:
            json.dump({
                "quantization_method": "bitsandbytes",
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": str(quantization_config.bnb_4bit_compute_dtype),
                "bnb_4bit_use_double_quant": True,
                "bnb_4bit_quant_type": "nf4"
            }, f, indent=2)

        # Calculate and display model sizes
        print(f"\n📊 Model Size Comparison:")

        # Get size of original model files
        original_size = 0
        for filename in os.listdir(OUTPUT_DIR):
            filepath = os.path.join(OUTPUT_DIR, filename)
            if os.path.isfile(filepath):
                original_size += os.path.getsize(filepath)

        # Get size of quantized model files
        quantized_size = 0
        for filename in os.listdir(QUANTIZED_OUTPUT_DIR):
            filepath = os.path.join(QUANTIZED_OUTPUT_DIR, filename)
            if os.path.isfile(filepath):
                quantized_size += os.path.getsize(filepath)

        print(f"   Original distilled model size: {original_size / (1024**3):.2f} GB")
        print(f"   4-bit quantized model size: {quantized_size / (1024**3):.2f} GB")
        print(f"   Size reduction: {(1 - quantized_size/original_size) * 100:.1f}%")

        # Memory usage
        if device.type == "cuda":
            print(f"\n   GPU Memory Usage (4-bit model):")
            print(f"   Allocated: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB")
            print(f"   Reserved:  {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB")

        print(f"\n✅ 4-bit quantization completed successfully!")
        print(f"   The quantized model has been saved to '{QUANTIZED_OUTPUT_DIR}'")

        print(f"\n📝 To load the 4-bit quantized model later:")
        print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig")
        print(f"   ")
        print(f"   quantization_config = BitsAndBytesConfig(")
        print(f"       load_in_4bit=True,")
        print(f"       bnb_4bit_compute_dtype=torch.bfloat16,")
        print(f"       bnb_4bit_use_double_quant=True,")
        print(f"       bnb_4bit_quant_type='nf4'")
        print(f"   )")
        print(f"   ")
        print(f"   model = AutoModelForCausalLM.from_pretrained(")
        print(f"       '{QUANTIZED_OUTPUT_DIR}',")
        print(f"       quantization_config=quantization_config,")
        print(f"       device_map='auto'")
        print(f"   )")
        print(f"   tokenizer = AutoTokenizer.from_pretrained('{QUANTIZED_OUTPUT_DIR}')")

    except ImportError:
        print(f"\n⚠️  WARNING: bitsandbytes library not found!")
        print(f"   To use 4-bit quantization, install it with:")
        print(f"   pip install bitsandbytes")
        print(f"   Then run the quantization separately.")
    except Exception as e:
        print(f"\n❌ Error during 4-bit quantization: {str(e)}")
        print(f"   The full precision model is still saved in '{OUTPUT_DIR}'")

    print(f"\nTo load the original (non-quantized) model:")
    print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f"   model = AutoModelForCausalLM.from_pretrained('{OUTPUT_DIR}')")
    print(f"   tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')")

if __name__ == "__main__":
    main()
